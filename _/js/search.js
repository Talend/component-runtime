var search = new JsSearch.Search('title');
search.indexStrategy = new JsSearch.AllSubstringsIndexStrategy();
search.addIndex('title');
search.addIndex('content');
search.addDocuments([{"title":"Talend Component Javadocs","content":"* link:{deploymentRoot}/apidocs/api/index.html[API Documentation^] * link:{deploymentRoot}/apidocs/junit/index.html[JUnit API Documentation^] * link:{deploymentRoot}/apidocs/junit-http/index.html[HTTP JUnit API Documentation^]","link":"apidocs.html"},{"title":"Talend Component Appendix","content":"The entry point of the API is the ContainerManager, it will enable you to define what is the Shared classloader and to create children: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- try (final ContainerManager manager = new ContainerManager( <1>     ContainerManager.DependenciesResolutionConfiguration.builder() <2>         .resolver(new MvnDependencyListLocalRepositoryResolver(\"META-INF/talend/dependencies.list\"))         .rootRepositoryLocation(new File(System.getProperty(\"user.home\", \".m2/repository\"))         .create(),     ContainerManager.ClassLoaderConfiguration.builder() <3>         .parent(getClass().getClassLoader())         .classesFilter(name -> true)         .parentClassesFilter(name -> true)         .create())) {     // create plugins } ---- <1> the ContainerManager is an AutoCloseable so you can use it in a try/finally block if desired. NOTE: it is recommanded to keep it running if you can reuse plugins to avoid to recreate classloaders and to mutualize them. This manager has two main configuration entries: how to resolve dependencies for plugins from the plugin file/location and how to configure the classloaders (what is the parent classloader, how to handle the parent first/last delegation etc...). <2> the DependenciesResolutionConfiguration enables to pass a custom Resolver which will be used to build the plugin classloaders. For now the library only provides MvnDependencyListLocalRepositoryResolver which will read the output of mvn dependencies:list put in the plugin jar and will resolve from a *local* maven repository the dependencies. Note that SNAPSHOT are only resolved based on their name and not from metadata (only useful in development). To continue the comparison to a Servlet server, you can easily implement an unpacked war resolver if you want. <3> the ClassLoaderConfiguration is configuring how the whole container/plugin pair will behave: what is the shared classloader?, which classes are loaded from the shared loader first (intended to be used for API which shouldn't be loaded from the plugin loader), which classes are loaded from the parent classloader (useful to exclude to load a \"common\" library from the parent classloader for instance, can be neat for guava, commons-lang3 etc...). Once you have a manager you can create plugins: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- final Container plugin1 = manager.create( <1>     \"plugin-id\", <2>     new File(\"/plugin/myplugin1.jar\")); <3> ---- <1> to create a plugin Container just use the create method of the manager <2> you can give an explicit id to the plugin (or if you bypass it, the manager will use the jar name) <3> you specify the plugin root jar To create the plugin container, the Resolver will resolve the dependencies needed for the plugin, then the manager will create the plugin classloader and register the plugin Container. It is common to need to do some actions when a plugin is registered/unregistered. For that purpose ContainerListener can be used: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- public class MyListener implements ContainerListener {     @Override     public void onCreate(final Container container) {         System.out.println(\"Container #\" + container.getId() + \" started.\");     }     @Override     public void onClose(final Container container) {         System.out.println(\"Container #\" + container.getId() + \" stopped.\");     } } ---- They are registered on the manager directly: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- final ContainerManager manager = getContainerManager(); final ContainerListener myListener = new MyListener(); manager.registerListener(myListener); <1> // do something manager.unregisterListener(myListener); <2> ---- <1> registerListener is used to add the listener from now on, it will not get any event for already created containers. <2> you can remove a listener with unregisterListener at any time.","link":"appendix.html"},{"title":"Talend Component Best Practices","content":"Few recommandations apply to the way a component packages are organized: 1. ensure to create a package-info.java with the component family/categories at the root of your component package: [source,java] ---- @Components(family = \"jdbc\", categories = \"Database\") package org.talend.sdk.component.jdbc; import org.talend.sdk.component.api.component.Components; ---- [start=2] 2. create a package for the configuration 3. create a package for the actions 4. create a package for the component and one subpackage by type of component (input, output, processors, ...) It is recommanded to ensure your configuration is serializable since it is likely you will pass it through your components which can be serialized. The first step to build a component is to identify the way it must be configured. It is generally split into two main big concepts: 1. the _DataStore_ which is the way you can access the backend 2. the _DataSet_ which is the way you interact with the backend Here are some examples to let you get an idea of what you put in each categories: [options=\"header,autowidth\"] |==== | Example description | DataStore | DataSet | Accessing a relational database like MySQL | the JDBC driver, url, username and password | the query to execute, row mapper, ... | Access a file system | the file pattern (or directory + file extension/prefix/...) | the file format, potentially the buffer size, ... |==== It is common to make the dataset including the datastore since both are required to work. However it is recommanded to replace this pattern by composing both in a higher level configuration model: [source,java] ---- @DataSet public class MyDataSet {     // ... } @DataStore public class MyDataStore {     // ... } public class MyComponentConfiguration {     @Option     private MyDataSet dataset;     @Option     private MyDataStore datastore; } ---- Processor configuration is simpler than I/O configuration since it is specific each time. For instance a mapper will take the mapping between the input and output model: [source,java] ---- public class MappingConfiguration {     @Option     private Map<String, String> fieldsMapping;     @Option     private boolean ignoreCase;     //... } ---- I/O are particular because they can be linked to a set of actions. It is recommanded to wire all the ones you can apply to ensure the consumers of your component can provide a rich experience to their users. Here are the most common ones: [cols=\"1,1,2,6,6\"] |==== | Type | Action | Description | Configuration example | Action example | DataStore | @Checkable | Expose a way to ensure the datastore/connection works a| [source,java] ---- @DataStore @Checkable public class JdbcDataStore   implements Serializable {   @Option   private String driver;   @Option   private String url;   @Option   private String username;   @Option   private String password; } ---- a| [source,java] ---- @HealthCheck public HealthCheckStatus healthCheck(@Option(\"datastore\") JdbcDataStore datastore) {     if (!doTest(dataStore)) {         // often add an exception message mapping or equivalent         return new HealthCheckStatus(Status.KO, \"Test failed\");     }     return new HealthCheckStatus(Status.KO, e.getMessage()); } ---- |==== Until the studio integration is complete, it is recommanded to limit processors to 1 input. It is also recommanded to provide as much information as possible to let the UI work with the data during its edition. The light validations are all the validations you can execute on the client side. They are listed in the <<documentation.adoc#documentation-ui-hints, UI hint>> part. This is the ones to use first before going with custom validations since they will be more efficient. These ones will enforce custom code to be executed, they are more heavy so try to avoid to use them for simple validations you can do with the previous part. Here you define an action taking some parameters needed for the validation and you link the option you want to validate to this action. Here is an example to validate a dataset. For example for our JDBC driver we could have: [source,java] ---- // ... public class JdbcDataStore   implements Serializable {   @Option   @Validable(\"driver\")   private String driver;   // ... } @AsyncValidation(\"driver\") public ValidationResult validateDriver(@Option(\"value\") String driver) {   if (findDriver(driver) != null) {     return new ValidationResult(Status.OK, \"Driver found\");   }   return new ValidationResult(Status.KO, \"Driver not found\"); } ---- Note that you can also make a class validable and you can use it to validate a form if you put it on your whole configuration: [source,java] ---- // note: some part of the API were removed for brievity public class MyConfiguration {   // a lot of @Options } public MyComponent {     public MyComponent(@Validable(\"configuration\") MyConfiguration config) {         // ...     }     //... } @AsyncValidation(\"configuration\") public ValidationResult validateDriver(@Option(\"value\") MyConfiguration configuration) {   if (isValid(configuration)) {     return new ValidationResult(Status.OK, \"Configuration valid\");   }   return new ValidationResult(Status.KO, \"Driver not valid ${because ...}\"); } ---- IMPORTANT: the parameter binding of the validation method uses the same logic than the component configuration injection. Therefore the @Option specifies the prefix to use to reference a parameter. It is recommanded to use @Option(\"value\") until you know exactly why you don't use it. This way the consumer can match the configuration model and just prefix it with value. to send the instance to validate. It can be neat and user friendly to provide completion on some fields. Here an example for the available drivers: [source,java] ---- // ... public class JdbcDataStore   implements Serializable {   @Option   @Completable(\"driver\")   private String driver;   // ... } @Completion(\"driver\") public CompletionList findDrivers() {     return new CompletionList(findDriverList()); } ---- Each component must have its own icon: [source,java] ---- @Icon(Icon.IconType.DB_INPUT) @PartitionMapper(family = \"jdbc\", name = \"input\") public class JdbcPartitionMapper     implements Serializable { } ---- TIP: you can use http://talend.surge.sh/icons/ to identify the one you want to use. Not mandatory for the first version but recommanded: enforce the version of your component. [source,java] ---- @Version(1) @PartitionMapper(family = \"jdbc\", name = \"input\") public class JdbcPartitionMapper     implements Serializable { } ---- If you break a configuration entry in a later version ensure to: 1. upgrade the version 2. support a migration of the configuration [source,java] ---- @Version(value = 2, migrationHandler = JdbcPartitionMapper.Migrations.class) @PartitionMapper(family = \"jdbc\", name = \"input\") public class JdbcPartitionMapper     implements Serializable {     public static class Migrations implements MigrationHandler {         // implement your migration     } } ---- Testing the components is crucial, you can use unit tests and simple standalone JUnit but it is highly recommanded to have a few Beam tests to ensure your component works in Big Data world. Don't hesitate to send your feedback on writing component and best practices you can encounter.","link":"best-practices.html"},{"title":"Gradle Plugin","content":"gradle-talend-component intends to help you to write components validating components match best practices. It is inspired from the Maven plugin and adds the ability to generate automatically the dependencies.txt file the SDK uses to build the component classpath. For more information on the configuration you can check out the maven properties matching the attributes. Here is how to use it: [source,groovy] ---- buildscript {   repositories {     mavenLocal()     mavenCentral()   }   dependencies {     classpath \"org.talend.sdk.component:gradle-talend-component:${talendComponentVersion}\"   } } apply plugin: 'org.talend.sdk.component' apply plugin: 'java' // optional customization talendComponentKit {     // dependencies.txt generation, replaces maven-dependency-plugin     dependenciesLocation = \"TALEND-INF/dependencies.txt\"     boolean skipDependenciesFile = false;     // classpath for validation utilities     sdkVersion = \"${talendComponentVersion}\"     apiVersion = \"${talendComponentApiVersion}\"     // documentation     skipDocumentation = false     documentationOutput = new File(....)     documentationLevel = 2 // first level will be == in the generated adoc     documentationTitle = 'My Component Family' // default to project name     documentationFormats = [:] // adoc attributes     documentationFormats = [:] // renderings to do     // validation     skipValidation = false     validateFamily = true     validateSerializable = true     validateInternationalization = true     validateModel = true     validateMetadata = true     validateComponent = true     validateDataStore = true     validateDataSet = true     validateActions = true     // web     serverArguments = []     serverPort = 8080     // car     carOutput = new File(....)     carMetadata = [:] // custom meta (string key-value pairs) } ----","link":"build-tools-gradle.html"},{"title":"Components Definition","content":"Talend Component framework relies on several primitive components. They can all use @PostConstruct and @PreDestroy to initialize/release some underlying resource at the beginning/end of the processing. IMPORTANT: in distributed environments class' constructor will be called on cluster manager node, methods annotated with @PostConstruct and @PreDestroy annotations will be called on worker nodes. Thus, partition plan computation and pipeline task will be performed on different nodes. //// [ditaa, generated-deployment-diagram, png] ....                  /-------------------------\\                  |       Create and        |                  |Submit task to cluster(1)|                  \\-------------------------/                              |                              V                 +---------------------------+                 |     Cluster manager       |                 |---------------------------|                 |     Partition plan        |                 |     computation(2)        |                 |                           |                 +---------------------------+                              ^                              |                           Serialized                           instances                              |                              V                     +-----------------+                     |   Worker node   |                     |-----------------|                     |Flow Execution(3)|                     +-----------------+ .... //// image:deployment-diagram.png[] 1. Created task consists of Jar file, containing class, which describes pipeline(flow) which should be processed in cluster. 2. During partition plan computation step pipeline is analyzed and split into stages. Cluster Manager node instantiates mappers/processors gets estimated data size using mappers, splits created mappers according to the estimated data size. All instances are serialized and sent to Worker nodes afterwards. 3. Serialized instances are received and deserialized, methods annotated with @PostConstruct annotation are called. After that, pipeline execution is started. Processor's @BeforeGroup annotated method is called before processing first element in chunk. After processing number of records estimated as chunk size, Processor's @AfterGroup annotated method called. Chunk size is calculated depending on environment the pipeline is processed by. After pipeline is processed, methods annotated with @PreDestroy annotation are called. //// [ditaa, generated-driver-processing-workflow, png] .... Partition plan computation(2)     +----------------+     | Create Mappers |     +----------------+             |             V +-------------------------+ |Compute partition plan(2)| +-------------------------+             |             V   +----------------------+   |  Serialize splitted  |   |mappers and processors|   +----------------------+ .... //// image:driver-processing-workflow.png[] //// [ditaa, generated-worker-processing-workflow, png] .... Flow Execution(3) +------------------+ |  @PostConstruct  | |     methods      | +------------------+          |          V +------------------+ |  @BeforeGroup    | |     methods      | +------------------+          |          V +------------------+ |   Perform task   | |   described in   | |     pipeline     | +------------------+          |          V +------------------+ |   @AfterGroup    | |     methods      | +------------------+          |          V +------------------+ |   @PreDestroy    | |     methods      | +------------------+ .... //// image:worker-processing-workflow.png[] IMPORTANT: all framework managed methods MUST be public too. Private methods are ignored. NOTE: in term of design the framework tries to be as declarative as possible but also to stay extensible not using fixed interfaces or method signatures. This will allow to add incrementally new features of the underlying implementations. ____ A PartitionMapper is a component able to split itself to make the execution more efficient. ____ This concept is borrowed to big data world and useful only in this context (BEAM executions). Overall idea is to divide the work before executing it to try to reduce the overall execution time. The process is the following: 1. Estimate the size of the data you will work on. This part is often heuristic and not very precise. 2. From that size the execution engine (_runner_ for beam) will request the mapper to split _itself_ in _N_ mappers with a subset of the overall work. 3. The _leaf_ (final) mappers will be used as a Producer (actual reader) factory. IMPORTANT: this kind of component MUST be Serializable to be distributable. A partition mapper requires 3 methods marked with specific annotations: 1. @Assessor for the evaluating method 2. @Split for the dividing method 3. @Emitter for the Producer factory The assessor method will return the estimated size of the data related to the component (depending its configuration). It MUST return a Number and MUST not take any parameter. Here is an example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Assessor public long estimateDataSetByteSize() {     return ....; } ---- The split method will return a collection of partition mappers and can take optionally a @PartitionSize long value which is the requested size of the dataset per sub partition mapper. Here is an example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Split public List<MyMapper> split(@PartitionSize final long desiredSize) {     return ....; } ---- The emitter method MUST not have any parameter and MUST return a producer. It generally uses the partition mapper configuration to instantiate/configure the producer. Here is an example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Emitter public MyProducer create() {     return ....; } ---- ____ A\u00a0Producer is the component interacting with a physical source. It produces input data for the processing flow. ____ A producer is a very simple component which MUST have a @Producer method without any parameter and returning any data: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Producer public MyData produces() {     return ...; } ---- ____ A Processor is a component responsible to convert an incoming data to another model. ____ A processor MUST have a method decorated with @ElementListener taking an incoming data and returning the processed data: [source,java] ---- @ElementListener public MyNewData map(final MyData data) {     return ...; } ---- IMPORTANT: this kind of component MUST be Serializable since it is distributed. IMPORTANT: if you don't care much of the type of the parameter and need to access data on a \"map like\" based rule set, then you can use JsonObject as parameter type and Talend Component will just wrap the data to enable you to access it as a map. The parameter type is not enforced, i.e. if you know you will get a SuperCustomDto then you can use that as parameter type but for generic component reusable in any chain it is more than highly encouraged to use JsonObject until you have your an evaluation language based processor (which has its own way to access component). Here is an example: [source,java] ---- @ElementListener public MyNewData map(final JsonObject incomingData) {     String name = incomingData.getString(\"name\");     int name = incomingData.getInt(\"age\");     return ...; } // equivalent to (using POJO subclassing) public class Person {     private String age;     private int age;     // getters/setters } @ElementListener public MyNewData map(final Person person) {     String name = person.getName();     int name = person.getAge();     return ...; } ---- A processor also supports @BeforeGroup and @AfterGroup which MUST be methods without parameters and returning void (result would be ignored). This is used by the runtime to mark a chunk of the data in a way which is estimated _good_ for the execution flow size. IMPORTANT: this is estimated so you don't have any guarantee on the size of a _group_. You can literally have groups of size 1. The common usage is to batch records for performance reasons: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @BeforeGroup public void initBatch() {     // ... } @AfterGroup public void endBatch() {     // ... } ---- IMPORTANT: it is a good practise to support a maxBatchSize here and potentially commit before the end of the group in case of a computed size which is way too big for your backend. In some case you may want to split the output of a processor in two. A common example is \"main\" and \"reject\" branches where part of the incoming data are put in a specific bucket to be processed later. This can be done using @Output. This can be used as a replacement of the returned value: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void map(final MyData data, @Output final OutputEmitter<MyNewData> output) {     output.emit(createNewData(data)); } ---- Or you can pass it a string which will represent the new branch: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void map(final MyData data,                 @Output final OutputEmitter<MyNewData> main,                 @Output(\"rejected\") final OutputEmitter<MyNewDataWithError> rejected) {     if (isRejected(data)) {         rejected.emit(createNewData(data));     } else {         main.emit(createNewData(data));     } } // or simply @ElementListener public MyNewData map(final MyData data,                     @Output(\"rejected\") final OutputEmitter<MyNewDataWithError> rejected) {     if (isSuspicious(data)) {         rejected.emit(createNewData(data));         return createNewData(data); // in this case we continue the processing anyway but notified another channel     }     return createNewData(data); } ---- Having multiple inputs is closeto the output case excep it doesn't require a wrapper OutputEmitter: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public MyNewData map(@Input final MyData data, @Input(\"input2\") final MyData2 data2) {     return createNewData(data1, data2); } ---- @Input takes the input name as parameter, if not set it uses the main (default) input branch. IMPORTANT: due to the work required to not use the default branch it is recommanded to use it when possible and not name its branches depending on the component semantic. ____ An Output is a Processor returning no data. ____ Conceptually an output is a listener of data. It perfectly matches the concept of processor. Being the last of the execution chain or returning no data will make your processor an output: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void store(final MyData data) {     // ... } ---- For now Talend Component doesn't enable you to define a Combiner. It would be the symmetric part of the partition mapper and allow to aggregate results in a single one.","link":"component-definition.html"},{"title":"Internationalization","content":"In the simplest case you should store messages using ResourceBundle properties file in your component module to use internationalization. The location of the properties file should be in the same package as the related component(s) and is named Messages (ex: org.talend.demo.MyComponent will use org.talend.demo.Messages[locale].properties). Out of the box components are internationalized using the same location logic for the resource bundle and here is the list of supported keys: [options=\"header,autowidth\"] |==== |Name Pattern|Description |${family}._displayName|the display name of the family |${family}.${configurationType}.${name}._displayName|the display name of a configuration type (dataStore or dataSet) |${family}.${component_name}._displayName|the display name of the component (used by the GUIs) |${property_path}._displayName|the display name of the option. |${simple_class_name}.${property_name}._displayName|the display name of the option using it class name. |${property_path}._placeholder|the placeholder of the option. |==== Example of configuration for a component named list belonging to the family memory (@Emitter(family = \"memory\", name = \"list\")): [source] ---- memory.list._displayName = Memory List ---- Configuration class are also translatable using the simple class name in the messages properties file. This useful when you have some common configuration shared within multiple components. If you have a configuration class like : [source,java] ---- public class MyConfig {     @Option     private String host;     @Option     private int port; } ---- You can give it a translatable display name by adding ${simple_class_name}.${property_name}._displayName to Messages.properties under the same package as the config class. [source] ---- MyConfig.host._displayName = Server Host Name MyConfig.host._placeholder = Enter Server Host Name... MyConfig.port._displayName = Server Port MyConfig.port._placeholder = Enter Server Port... ---- IMPORTANT: If you have a display name using the property path, it will override the display name defined using the simple class name. this rule apply also to placeholders","link":"component-internationalization.html"},{"title":"Component Loading","content":"Talend Component scanning is based on a plugin concept. To ensure plugins can be developped in parallel and avoid conflicts it requires to isolate plugins (components or component grouped in a single jar/plugin). Here we have multiple options which are (high level): - flat classpath: listed for completeness but rejected _by design_ because it doesn't match at all this requirement. - tree classloading: a shared classloader inherited by plugin classloaders but plugin classloader classes are not seen by the shared classloader nor by other plugins. - graph classloading: this one allows you to link the plugins and dependencies together dynamically in any direction. If you want to map it to concrete common examples, the tree classloading is commonly used by Servlet containers where plugins are web applications and the graph classloading can be illustrated by OSGi containers. In the spirit of avoiding a lot of complexity added by this layer, Talend Component relies on a tree classloading. The advantage is you don't need to define the relationship with other plugins/dependencies (it is built-in). Here is a representation of this solution: //// [ditaa, generated-classloader-layout, png] ....                  /--------\\      +---------->| Shared |<---------+      |           \\--------/          |      |               ^               |      |               |               | /----+-----\\    /----+-----\\    /----+-----\\ | Plugin 1 |    | Plugin 2 |    | Plugin N | \\----------/    \\----------/    \\----------/ .... //// image:classloader-layout.png[] The interesting part is the _shared_ area will contain Talend Component API which is the only (by default) shared classes accross the whole plugins. Then each plugins will be loaded in their own classloader with their dependencies. NOTE: this part explains the overall way to handle dependecnies but the Talend Maven plugin provides a shortcut for that. A plugin is just a jar which was enriched with the list of its dependencies. By default Talend Component runtime is able to read the output of maven-dependency-plugin in TALEND-INF/dependencies.txt location so you just need to ensure your component defines the following plugin: [source,xml] ---- <plugin>   <groupId>org.apache.maven.plugins</groupId>   <artifactId>maven-dependency-plugin</artifactId>   <version>3.0.2</version>   <executions>     <execution>       <id>create-TALEND-INF/dependencies.txt</id>       <phase>process-resources</phase>       <goals>         <goal>list</goal>       </goals>       <configuration>         <outputFile>${project.build.outputDirectory}/TALEND-INF/dependencies.txt</outputFile>       </configuration>     </execution>   </executions> </plugin> ---- If you check your jar once built you will see that the file contains something like: [source,bash] ---- $ unzip -p target/mycomponent-1.0.0-SNAPSHOT.jar TALEND-INF/dependencies.txt The following files have been resolved:    org.talend.sdk.component:component-api:jar:1.0.0-SNAPSHOT:provided    org.apache.geronimo.specs:geronimo-annotation_1.3_spec:jar:1.0:provided    org.superbiz:awesome-project:jar:1.2.3:compile    junit:junit:jar:4.12:test    org.hamcrest:hamcrest-core:jar:1.3:test ---- What is important to see is the scope associated to the artifacts: - the API (component-api and geronimo-annotation_1.3_spec) are provided because you can consider them to be there when executing (it comes with the framework) - your specific dependencies (awesome-project) is compile: it will be included as a needed dependency by the framework (note that using runtime works too). - the other dependencies will be ignored (test dependencies) Even if a flat classpath deployment is possible, it is not recommanded because it would then reduce the capabilities of the components. The way the framework resolves dependencies is based on a local maven repository layout. As a quick reminder it looks like: [source] ---- . \u251c\u2500\u2500 groupId1 \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifactId1 \u2502\u00a0\u00a0     \u251c\u2500\u2500 version1 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifactId1-version1.jar \u2502\u00a0\u00a0     \u2514\u2500\u2500 version2 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 artifactId1-version2.jar \u2514\u2500\u2500 groupId2  \u00a0\u00a0 \u2514\u2500\u2500 artifactId2  \u00a0\u00a0     \u2514\u2500\u2500 version1  \u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 artifactId2-version1.jar ---- This is all the layout the framework will use. Concretely the logic will convert the t-uple {groupId, artifactId, version, type (jar)} to the path in the repository. Talend Component runtime has two ways to find an artifact: - from the file system based on a configure maven 2 repository. - from a fatjar (uber jar) with a nested maven repository under MAVEN-INF/repository. The first option will use either - by default - ${user.home}/.m2/repository or a specific path configured when creating a ComponentManager. The nested repository option will need some configuration during the packaging to ensure the repository is well created. To create the nested MAVEN-INF/repository repository you can use nested-maven-repository extension: [source,xml,indent=0,subs=\"verbatim,quotes,attributes\"] ---- <plugin>   <groupId>org.apache.maven.plugins</groupId>   <artifactId>maven-shade-plugin</artifactId>   <version>3.0.0</version>   <executions>     <execution>       <phase>package</phase>       <goals>         <goal>shade</goal>       </goals>       <configuration>         <transformers>           <transformer implementation=\"org.talend.sdk.component.container.maven.shade.ContainerDependenciesTransformer\">             <session>${session}</project>           </transformer>         </transformers>       </configuration>     </execution>   </executions>   <dependencies>     <dependency>       <groupId>org.talend.sdk.component</groupId>       <artifactId>nested-maven-repository</artifactId>       <version>${the.plugin.version}</version>     </dependency>   </dependencies> </plugin> ---- Plugin are programmatically registered in general but if you want to make some of them automatically available you need to generate a TALEND-INF/plugins.properties which will map a plugin name to coordinates found with the maven mecanism we just talked about. Here again we can enrich maven-shade-plugin to do it: [source,xml,indent=0,subs=\"verbatim,quotes,attributes\"] ---- <plugin>   <groupId>org.apache.maven.plugins</groupId>   <artifactId>maven-shade-plugin</artifactId>   <version>3.0.0</version>   <executions>     <execution>       <phase>package</phase>       <goals>         <goal>shade</goal>       </goals>       <configuration>         <transformers>           <transformer implementation=\"org.talend.sdk.component.container.maven.shade.PluginTransformer\">             <session>${session}</project>           </transformer>         </transformers>       </configuration>     </execution>   </executions>   <dependencies>     <dependency>       <groupId>org.talend.sdk.component</groupId>       <artifactId>nested-maven-repository</artifactId>       <version>${the.plugin.version}</version>     </dependency>   </dependencies> </plugin> ---- Here is a final job/application bundle based on maven shade plugin: [source,xml,indent=0,subs=\"verbatim,quotes,attributes\"] ---- <plugin>   <groupId>org.apache.maven.plugins</groupId>   <artifactId>maven-shade-plugin</artifactId>   <version>3.0.0</version>   <configuration>     <createDependencyReducedPom>false</createDependencyReducedPom>     <filters>       <filter>         <artifact>*:*</artifact>         <excludes>           <exclude>META-INF/*.SF</exclude>           <exclude>META-INF/*.DSA</exclude>           <exclude>META-INF/*.RSA</exclude>         </excludes>       </filter>     </filters>   </configuration>   <executions>     <execution>       <phase>package</phase>       <goals>         <goal>shade</goal>       </goals>       <configuration>         <shadedClassifierName>shaded</shadedClassifierName>         <transformers>           <transformer               implementation=\"org.talend.sdk.component.container.maven.shade.ContainerDependenciesTransformer\">             <session>${session}</session>             <userArtifacts>               <artifact>                 <groupId>org.talend.sdk.component</groupId>                 <artifactId>sample-component</artifactId>                 <version>1.0</version>                 <type>jar</type>               </artifact>             </userArtifacts>           </transformer>           <transformer implementation=\"org.talend.sdk.component.container.maven.shade.PluginTransformer\">             <session>${session}</session>             <userArtifacts>               <artifact>                 <groupId>org.talend.sdk.component</groupId>                 <artifactId>sample-component</artifactId>                 <version>1.0</version>                 <type>jar</type>               </artifact>             </userArtifacts>           </transformer>         </transformers>       </configuration>     </execution>   </executions>   <dependencies>     <dependency>       <groupId>org.talend.sdk.component</groupId>       <artifactId>nested-maven-repository-maven-plugin</artifactId>       <version>${the.version}</version>     </dependency>   </dependencies> </plugin> ---- NOTE: the configuration unrelated to transformers can depend your application. ContainerDependenciesTransformer is the one to embed a maven repository and PluginTransformer to create a file listing (one per line) a list of artifacts (representing plugins). Both transformers share most of their configuration: - session: must be set to ${session}. This is used to retrieve dependencies. - scope: a comma separated list of scope to include in the artifact filtering (note that the default will rely on provided but you can replace it by compile, runtime, runtime+compile, runtime+system, test). - include: a comma separated list of artifact to include in the artifact filtering. - exclude: a comma separated list of artifact to exclude in the artifact filtering. - userArtifacts: a list of artifacts (groupId, artifactId, version, type - optional, file - optional for plugin transformer, scope - optional) which can be forced inline - mainly useful for\u00a0PluginTransformer. - includeTransitiveDependencies: should transitive dependencies of the components be included, true by default. - includeProjectComponentDependencies: should project component dependencies be included, false by default (normally a job project uses isolation for components so this is not needed). - userArtifacts: set of component artifacts to include. IMPORTANT: to use with the component tooling, it is recommended to keep default locations. Also if you feel you need to use project dependencies, you can need to refactor your project structure to ensure you keep component isolation. Talend component let you handle that part but the recommended practise is to use userArtifacts for the components and not the project <dependencies>. ContainerDependenciesTransformer specific configuration is the following one: - repositoryBase: base repository location (default to MAVEN-INF/repository). - ignoredPaths: a comma separated list of folder to not create in the output jar, this is common for the ones already created by other transformers/build parts. ContainerDependenciesTransformer specific configuration is the following one: - pluginListResource: base repository location (default to TALEND-INF/plugins.properties). Example: if you want to list only the plugins you use you can configure this transformer like that: [source,xml,indent=0,subs=\"verbatim,quotes,attributes\"] ---- <transformer implementation=\"org.talend.sdk.component.container.maven.shade.PluginTransformer\">   <session>${session}</session>   <include>org.talend.sdk.component:component-x,org.talend.sdk.component:component-y,org.talend.sdk.component:component-z</include> </transformer> ----","link":"component-loading.html"},{"title":"Registering components","content":"As seen in the <<getting-started.adoc#getting-started-first-quick-start, Getting Started>>, you need an annotation to register your component through family method. Multiple components can use the same family value but the pair family+name MUST be unique for the system. If you desire (recommended) to share the same component family name instead of repeating yourself in all family methods, you can use @Components annotation on the root package of you component, it will enable you to define the component family and the categories the component belongs to (default is Misc if not set). Here is a sample package-info.java: [source,java] ---- @Components(name = \"my_component_family\", categories = \"My Category\") package org.talend.sdk.component.sample; import org.talend.sdk.component.api.component.Components; ---- For an existing component it can look like: [source,java] ---- @Components(name = \"Salesforce\", categories = {\"Business\", \"Cloud\"}) package org.talend.sdk.component.sample; import org.talend.sdk.component.api.component.Components; ---- Components can require a few metadata to be integrated in Talend Studio or Cloud platform. Here is how to provide these information. These metadata are set on the component class and belongs to org.talend.sdk.component.api.component package. [options=\"header,autowidth\"] |==== | API | Description | @Icon | Set an icon key used to represent the component. Note you can use a custom key with custom() method but it is not guaranteed the icon will be rendered properly. | @Version | Set the component version, default to 1. |==== Example: [source,java] ---- @Icon(FILE_XML_O) @PartitionMapper(name = \"jaxbInput\") public class JaxbPartitionMapper implements Serializable {     // ... } ---- If some impacting changes happen on the configuration they can be manage through a migration handler at *component* level (to enable to support trans-model migration). The @Version annotation supports a migrationHandler method which will take the implementation migrating the incoming configuration to the current model. For instance if filepath configuration entry from v1 changed to location in v2 you can remap the value to the right key in your MigrationHandler implementation. TIP: it is recommanded to not manage all migrations in the handler but rather split it in services you inject in the migration handler (through constructor): [source,java] ---- // full component code structure skipped for brievity, kept only migration part @Version(value = 3, migrationHandler = MyComponent.Migrations.class) public class MyComponent {     // the component code...     private interface VersionConfigurationHandler {         Map<String, String> migrate(Map<String, String> incomingData);     }     public static class Migrations {         private final List<VersionConfigurationHandler> handlers;         // VersionConfigurationHandler implementations are decorated with @Service         public Migrations(final List<VersionConfigurationHandler> migrations) {             this.handlers = migrations;             this.handlers.sort(/*some custom logic*/);         }         @Override         public Map<String, String> migrate(int incomingVersion, Map<String, String> incomingData) {             Map<String, String> out = incomingData;             for (MigrationHandler handler : handlers) {                 out = handler.migrate(out);             }         }     } } ---- What is important in this snippet is not much the way the code is organized but rather the fact you organize your migrations the way which fits the best your component. If migrations are not conflicting no need of something fancy, just apply them all but if you need to apply them in order you need to ensure they are sorted. Said otherwise: don't see this API as a migration API but as a migration callback and adjust the migration code structure you need behind the MigrationHandler based on your component requirements. The service injection enables you to do so. @PartitionMapper will obviously mark a partition mapper: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @PartitionMapper(family = \"demo\", name = \"my_mapper\") public class MyMapper { } ---- @Emitter is a shortcut for @PartitionMapper when you don't support distribution. Said otherwise it will enforce an implicit partition mapper execution with an assessor size of 1 and a split returning itself. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Emitter(family = \"demo\", name = \"my_input\") public class MyInput { } ---- A method decorated with @Processor will be considered as a producer factory: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Processor(family = \"demo\", name = \"my_processor\") public class MyProcessor { } ----","link":"component-registering.html"},{"title":"Talend Component Design Choices","content":"The component API has multiple strong choices: . it is declarative (through annotations) to ensure it is .. evolutive (it can get new fancy featrues without breaking old code) .. static as much as possible Being fully declarative, any new API can be added iteratively without requiring any changes to existing components. Example (projection on beam potential evolution): [source,java] ---- @ElementListener public MyOutput onElement(MyInput data) {     return ...; } ---- wouldn't be affected by the addition of the new Timer API which can be used like: [source,java] ---- @ElementListener public MyOutput onElement(MyInput data,                           @Timer(\"my-timer\") Timer timer) {     return ...; } ---- Intent of the framework is to be able to fit java UI as well as web UI. It must be understood as colocalized and remote UI. The direct impact of that choice is to try to move as much as possible the logic to the UI side for UI related actions. Typically we want to validate a pattern, a size, ... on the client side and not on the server side. Being static encourages this practise. The other goal to be really static in its definition is to ensure the model will not be mutated at runtime and all the auditing and modelling can be done before, in the design phase. Being static also ensures the development can be validated as much as possible through build tools. This doesn't replace the requirement to test the components but helps the developer to maintain its components with automated tools. The processor API supports JsonObject as well as any custom model. Intent is to support generic component development which need to access configured \"object paths\" and specific components which rely on a well defined path from the input. A generic component would look like: [source,java] ---- @ElementListener public MyOutput onElement(JsonObject input) {     return ...; } ---- A specific component would look like (with MyInput a POJO): [source,java] ---- @ElementListener public MyOutput onElement(MyInput input) {     return ...; } ---- By design the framework must run in DI (plain standalone java program) but also in Beam pipelines. It is also out of scope of the framework to handle the way the runtime serializes - if needed - the data. For that reason it is primordial to not import serialization constraint in the stack. This is why JsonObject is not an IndexedRecord from avro for instance, to not impose any implementation. Any actual serialization concern - implementation - should either be hidden in the framework runtime (= outside component developer scope) or in the runtime integration with the framework (beam integration for instance). In this context, JSON-P is a good compromise because it brings a very powerful API with very few constraints. The components must be able to execute even if they have conflicting libraries. For that purpose it requires to isolate their classloaders. For that purpose a component will define its dependencies based on a maven format and will always be bound to its own classloader. The definition payload is as flat as possible and _strongly_ typed to ensure it can be manipulated by consumers. This way the consumers can add/remove fields with just some mapping rules and don't require any abstract tree handling. The execution (runtime) configuration is the concatenation of a few framework metadata (only the version actually) and a key/value model of the instance of the configuration based on the definition properties paths for the keys. This enables the consumers to maintain and work with the keys/values up to their need. The framework not being responsible for any persistence it is crucial to ensure consumers can handle it from end to end which includes the ability to search for values (update a machine, update a port etc...) and keys (new encryption rule on key certificate for instance). Talend component is a metamodel provider (to build forms) and runtime *execution* platform (take a configuration instance and use it volatively to execute a component logic). This implies it can't own the data more than defining the contract it has for these two endpoints and must let the consumers handle the data lifecycle (creation, encryption, deletion, ....). A new mime type called talend/stream is introduced to define a streaming format. It basically matches a JSON object per line: [source,javascript] ---- {\"key1\":\"value1\"} {\"key2\":\"value2\"} {\"key1\":\"value11\"} {\"key1\":\"value111\"} {\"key2\":\"value2\"} ---- Icons (@Icon) are based on a fixed set. Even if a custom icon is usable this is without any guarantee. This comes from the fact components can be used in any environment and require a kind of uniform look which can't be guaranteed outside the UI itself so defining only keys is the best way to communicate this information. TIP: when you exactly know how you will deploy your component (ie in the Studio) then you can use @Icon(value = CUSTOM, custom = \"...\") to use a custom icon file.","link":"design.html"},{"title":"Talend Component Documentation Overview","content":"Talend Component framework is under the responsability of Mike Hirt team. If you know nothing about Talend Components, the getting started is the place to start with. * *From scratch:*   <<getting-started.adoc#getting-started-introducing-talend-component, Overview>> |   <<getting-started.adoc#getting-started-system-requirements, Requirements>> * *Tutorial:*   <<getting-started.adoc#getting-started-quick-start, Code>> |   <<getting-started.adoc#getting-started-quick-start-run, Run>> * *Core features:*   <<documentation.adoc#_components_definitions, Overview>> * *Advanced:*   <<documentation-testing.adoc#documentation-testing-start, Testing>>","link":"documentation-overview.html"},{"title":"Gallery","content":"[cols=\"1,3a,4a,4a\",role=\"table gallery\",options=\"header,autowidth\"] |=== | Name | Code | Studio Rendering | Web Rendering | Input/Text |[source,java] ---- @Option String config; ---- |image::gallery/widgets/studio/input.png[Studio Input,window=\"_blank\",link=\"images/gallery/widgets/studio/input.png\"] |image::gallery/widgets/web/input.png[Web Input,window=\"_blank\",link=\"images/gallery/widgets/web/input.png\"] | Password |[source,java] ---- @Option @Credential String config; ---- |image::gallery/widgets/studio/password.png[Studio Password,window=\"_blank\",link=\"images/gallery/widgets/studio/password.png\"] |image::gallery/widgets/web/password.png[Web Password,window=\"_blank\",link=\"images/gallery/widgets/web/password.png\"] | Textarea |[source,java] ---- @Option @Textarea String config; ---- |image::gallery/widgets/studio/textarea.png[Studio Textarea,window=\"_blank\",link=\"images/gallery/widgets/studio/textarea.png\"] |image::gallery/widgets/web/textarea.png[Web Textarea,window=\"_blank\",link=\"images/gallery/widgets/web/textarea.png\"] | Checkbox |[source,java] ---- @Option Boolean config; ---- |image::gallery/widgets/studio/checkbox.png[Studio Checkbox,window=\"_blank\",link=\"images/gallery/widgets/studio/checkbox.png\"] |image::gallery/widgets/web/checkbox.png[Web Checkbox,window=\"_blank\",link=\"images/gallery/widgets/web/checkbox.png\"] | List |[source,java] ---- @Option List<String> config; ---- |image::gallery/widgets/studio/list.png[Studio List,window=\"_blank\",link=\"images/gallery/widgets/studio/list.png\"] |image::gallery/widgets/web/list.png[Web List,window=\"_blank\",link=\"images/gallery/widgets/web/list.png\"] | Table |[source,java] ---- @Option Object config; ---- |image::gallery/widgets/studio/table.png[Studio Table,window=\"_blank\",link=\"images/gallery/widgets/studio/table.png\"] |image::gallery/widgets/web/table.png[Web Table,window=\"_blank\",link=\"images/gallery/widgets/web/table.png\"] | Code |[source,java] ---- @Code(\"java\") @Option String config; ---- |image::gallery/widgets/studio/javaCode.png[Studio Code,window=\"_blank\",link=\"images/gallery/widgets/studio/javaCode.png\"] |image::gallery/widgets/web/javaCode.png[Web Code,window=\"_blank\",link=\"images/gallery/widgets/web/javaCode.png\"] | Schema |[source,java] ---- @Option @Structure List<String> config; ---- |image::gallery/widgets/studio/schema.png[Studio Schema,window=\"_blank\",link=\"images/gallery/widgets/studio/schema.png\"] |image::gallery/widgets/web/schema.png[Web Schema,window=\"_blank\",link=\"images/gallery/widgets/web/schema.png\"] |=== [cols=\"1,3a,4a,4a\",role=\"table gallery\",options=\"header,autowidth\"] |=== | Name | Code | Studio Rendering | Web Rendering | Property validation |[source,java] ---- /** configuration class */ @Option @Validable(\"url\") String config; /** service class */ @AsyncValidation(\"url\") ValidationResult doValidate(String url) { //validate the property } ---- |image::gallery/widgets/studio/validation_property.png[Studio Code,window=\"_blank\",link=\"images/gallery/widgets/studio/prop_validation.png\"] |image::gallery/widgets/web/validation_property.png[Web Code,window=\"_blank\",link=\"images/gallery/widgets/web/prop_validation.png\"] | Data store validation |[source,java] ---- @Datastore @Checkable public class config { /** config ...*/ } /** service class */ @HealthCheck public HealthCheckStatus testConnection(){ //validate the connection } ---- |image::gallery/widgets/studio/validation_datastore.png[Studio Code,window=\"_blank\",link=\"images/gallery/widgets/studio/prop_validation.png\"] |image::gallery/widgets/web/validation_datastore.png[Web Code,window=\"_blank\",link=\"images/gallery/widgets/web/prop_validation.png\"] |===","link":"gallery.html"},{"title":"Talend Component Kit Developer Reference Guide","content":"ifeval::[\"{backend}\" == \"html5\"] ifeval::[\"{docbranch}\" == \"master\"] IMPORTANT: this is a version under development which has not yet been deployed. You can however use it using the -SNAPSHOT version and Sonatype snapshot https://oss.sonatype.org/content/repositories/snapshots/[repository]. TIP: if you want a PDF version of that page you can find it in our snapshots: https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.talend.sdk.component&a=documentation&v={project_version}-SNAPSHOT&e=pdf&c=all-in-one[PDF]. endif::[] ifeval::[\"{docbranch}\" != \"master\"] NOTE: if you want a PDF version of that page just click on this http://repo.apache.maven.org/maven2/org/talend/sdk/component/documentation/{project_version}/documentation-{project_version}-all-in-one.pdf[link]. endif::[] endif::[] NOTE: if you prefer you can use the <<all-in-one.adoc#, single page>> documentation. * <<documentation-overview.adoc#, Overview>> * <<getting-started.adoc#, Getting Started>> * <<documentation.adoc#, Reference Guide>> * <<documentation-testing.adoc#, Testing>> * <<best-practices.adoc#, Best Practices>> * <<design.adoc#, Design choices>> * <<wrapping-a-beam-io.adoc#, How to wrap a Beam I/O>> * <<documentation-rest.adoc#, Web>> * <<studio.adoc#, Experimental Talend Studio Integration>> * <<changelog.adoc#, Changelog>> * <<contributors.adoc#, Wall Of Fame>> * <<apidocs.adoc#, API Documentation>> * <<appendix.adoc#, Appendix>>","link":"index.html"},{"title":"Talend Component Kit Release Process","content":"Version: {project_version} This page gives some hints about how to release the repository. Before configuring Maven you need to have a GPG key. Once you installed GPG, you can either import an existing key or generate one using gpg --gen-key. Then a few entries into your maven settings.xml are needed to provide the needed credentials for the release. Here is the overall template: [source,xml] ---- <settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"           xsi:schemaLocation=\"             http://maven.apache.org/SETTINGS/1.0.0             https://maven.apache.org/xsd/settings-1.0.0.xsd\">   <servers>     <server>       <id>ossrh</id>       <username>${env.OSSRH_USER}</username>       <password>${env.OSSRH_PASS}</password>     </server>     <server>       <id>github</id>       <username>${env.TLND_GITHUB_USER}</username>       <password>${env.TLND_GITHUB_PASS}</password>     </server>     <server>       <id>jira</id>       <username>${env.TLND_JIRA_USER}</username>       <password>${env.TLND_JIRA_PASS}</password>     </server>     <server>       <id>blackduck</id>       <username>${env.TLND_BLACKDUCK_USER}</username>       <password>${env.TLND_BLACKDUCK_PASS}</password>     </server>   </servers>   <profiles>     <profile>       <id>ossrh</id>       <activation>         <activeByDefault>true</activeByDefault>       </activation>       <properties>         <additionalparam>-Xdoclint:none</additionalparam>         <gpg.executable>gpg</gpg.executable>         <gpg.defaultKeyring>false</gpg.defaultKeyring>         <gpg.keyname>${env.GPG_KEYNAME}</gpg.keyname>         <gpg.passphrase>${env.GPG_PASSPHRASE}</gpg.passphrase>         <gpg.publicKeyring>${env.HOME}/.gpg/talend.pub.bin</gpg.publicKeyring>         <gpg.secretKeyring>${env.HOME}/.gpg/talend.priv.bin</gpg.secretKeyring>       </properties>     </profile>   </profiles> </settings> ---- NOTE: all the environment variables should be either set in your environment or hardcoded inline. Note that it is more than highly recommanded to use maven encryption: https://maven.apache.org/guides/mini/guide-encryption.html. 1. The OSSH variables are your Sonatype OSS repository credential with the permissions to deploy on org.talend. If you don't have it already you can ask for one at https://issues.sonatype.org/projects/OSSRH using your talend address. 2. The GITHUB variables are your Github account credentials. It is mainly used to update the documentation. 3. The JIRA variables are your Talend account credentials with read permissions on https://jira.talendforge.org/projects/TCOMP/. 4. The Blackduck configuration is used for security scans. 5. The GPG variables reference the key you created in previous part. The release contacts JIRA to create the release notes. It uses all TCOMP issues which have the label changelog. Before any release don't forget to go through all issues of the version you will release and add/remove the label depending the issue you want to appear into the release note. IMPORTANT: at that stage we assume previous steps have been *done*. Then, the release uses a standard Maven process, you should be able to do it in two steps: [source,sh] ---- mvn release:prepare mvn release:perform ---- Once these commands passed, you need to do/ensure: 1. The git tag corresponding to the release was pushed upstream (on github.com/Talend/component-runtime), if not, execute git push --follow-tags? 2. Close and release the staging repository on Sonatype OSS (log in on https://oss.sonatype.org/, then hit Staging Repositories in the left pane, select your repository and hit Close then Release on the top buttons). After a moment - it can take a few hours - the binaries will be available on central (http://repo.apache.maven.org/maven2/). Don't forget to check it to ensure there was no issue during the release process. You can also validate that the release deployed the new version (into the version menu) for the website.","link":"release-process.html"},{"title":"Search","content":"","link":"search.html"},{"title":"Built in services","content":"The framework provides some built-in services you can inject by type in components and actions out of the box. Here is the list: [options=\"header,autowidth\"] |=== | Type | Description a| org.talend.sdk.component.api.service.cache.LocalCache | Provides a small abstraction to cache data which don't need to be recomputed very often. Commonly used by actions for the UI interactions. a| org.talend.sdk.component.api.service.dependency.Resolver a| Allows to resolve a dependency from its Maven coordinates. a| javax.json.spi.JsonProvider a| A JSON-P instance. Prefer other JSON-P instances if you don't exactly know why you use this one. a| javax.json.JsonBuilderFactory a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations. a| javax.json.JsonWriterFactory a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations. a| javax.json.JsonReaderFactory a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations. a| javax.json.stream.JsonParserFactory a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations. a| javax.json.stream.JsonGeneratorFactory a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations. IMPORTANT: it assumes the dependency is locally available to the execution instance which is not guaranteed yet by the framework. a| org.talend.sdk.component.api.service.configuration.LocalConfiguration a| Represents the local configuration which can be used during the design. WARNING: it is not recommanded to use it for the runtime since the local configuration is generally different and the instances are distincts. TIP: you can also use the local cache as an interceptor with @Cached a| Every interface that extends HttpClient and that contains methods annotated with @Request a| This let you define an http client in a declarative manner using an annotated interface. TIP: See the <<_httpclient_usage>> for details. |=== Let assume that we have a REST API defined like below, and that it requires a basic authentication header. |=== | GET     /api/records/{id} | - | POST    /api/records      | with a json playload to be created {\"id\":\"some id\", \"data\":\"some data\"} |=== To create an http client able to consume this REST API, we will define an interface that extends HttpClient, The HttpClient interface lets you set the base for the http address that our client will hit. The base is the part of the address that we will need to add to the request path to hit the api. Every method annotated with @Request of our interface will define an http request. Also every request can have @Codec that let us encode/decode the request/response playloads. TIP: if your payload(s) is(are) String or Void you can ignore the coder/decoder. [source,java] ---- public interface APIClient extends HttpClient {     @Request(path = \"api/records/{id}\", method = \"GET\")     @Codec(decoder = RecordDecoder.class) //decoder =  decode returned data to Record class     Record getRecord(@Header(\"Authorization\") String basicAuth, @Path(\"id\") int id);     @Request(path = \"api/records\", method = \"POST\")     @Codec(encoder = RecordEncoder.class, decoder = RecordDecoder.class) //encoder = encode record to fit request format (json in this example)     Record createRecord(@Header(\"Authorization\") String basicAuth, Record record); } ---- IMPORTANT: The interface should extends HttpClient. In the codec classes (class that implement Encoder/Decoder) you can inject any of your services annotated with @Service or @Internationalized into the constructor. The i18n services can be useful to have i18n messages for errors handling for example. This interface can be injected into our Components classes or Services to consume the defined api. [source,java] ---- @Service public class MyService {     private APIClient client;     public MyService(...,APIClient client){         //...         this.client = client;         client.base(\"http://localhost:8080\");// init the base of the api, ofen in a PostConstruct or init method     }     //...     // Our get request     Record rec =  client.getRecord(\"Basic MLFKG?VKFJ\", 100);     //...     // Our post request     Record newRecord = client.createRecord(\"Basic MLFKG?VKFJ\", new Record()); } ---- Note: by default */*+json are mapped to JSON-P and */*+xml to JAX-B if the model has a @XmlRootElement annotation. For advanced cases you can customize the Connection directly using @UseConfigurer on the method. It will call your custom instance of Configurer. Note that you can use some @ConfigurerOption in the method signature to pass some configurer configuration. For instance if you have this configurer: [source,java] ---- public class BasicConfigurer implements Configurer {     @Override     public void configure(final Connection connection, final ConfigurerConfiguration configuration) {         final String user = configuration.get(\"username\", String.class);         final String pwd = configuration.get(\"password\", String.class);         connection.withHeader(             \"Authorization\",             Base64.getEncoder().encodeToString((user + ':' + pwd).getBytes(StandardCharsets.UTF_8)));     } } ---- You can then set it on a method to automatically add the basic header with this kind of API usage: [source,java] ---- public interface APIClient extends HttpClient {     @Request(path = \"...\")     @UseConfigurer(BasicConfigurer.class)     Record findRecord(@ConfigurerOption(\"username\") String user, @ConfigurerOption(\"password\") String pwd); } ----","link":"services-built-in.html"},{"title":"Advanced: define a custom API","content":"It is possible to extend the Component API for custom front features. What is important here is to keep in mind you should do it only if it targets not portable components (only used by the Studio or Beam). In term of organization it is recommanded to create a custom xxxx-component-api module with the new set of annotations. To extend the UI just add an annotation which can be put on @Option fields which is decorated with @Ui. All its members will be put in the metadata of the parameter. Example: [source,java] ---- @Ui @Target(TYPE) @Retention(RUNTIME) public @interface MyLayout { } ----","link":"services-custom-api.html"},{"title":"Services and interceptors","content":"For common concerns like caching, auditing etc, it can be fancy to use interceptor like API. It is enabled by the framework on services. An interceptor defines an annotation marked with @Intercepts which defines the implementation of the interceptor (an InterceptorHandler). Here is an example: [source,java] ---- @Intercepts(LoggingHandler.class) @Target({ TYPE, METHOD }) @Retention(RUNTIME) public @interface Logged {     String value(); } ---- Then handler is created from its constructor and can take service injections (by type). The first parameter, however, can be a BiFunction<Method, Object[], Object> which representes the invocation chain if your interceptor can be used with others. IMPORTANT: if you do a generic interceptor it is important to pass the invoker as first parameter. If you don't do so you can't combine interceptors at all. Here is an interceptor implementation for our @Logged API: [source,java] ---- public class LoggingHandler implements InterceptorHandler {     // injected     private final BiFunction<Method, Object[], Object> invoker;     private final SomeService service;     // internal     private final ConcurrentMap<Method, String> loggerNames = new ConcurrentHashMap<>();     public CacheHandler(final BiFunction<Method, Object[], Object> invoker, final SomeService service) {         this.invoker = invoker;         this.service = service;     }     @Override     public Object invoke(final Method method, final Object[] args) {         final String name = loggerNames.computeIfAbsent(method, m -> findAnnotation(m, Logged.class).get().value());         service.getLogger(name).info(\"Invoking {}\", method.getName());         return invoker.apply(method, args);     } } ---- This implementation is compatible with interceptor chains since it takes the invoker as first constructor parameter and it also takes a service injection. Then the implementation just does what is needed - logging the invoked method here. NOTE: the findAnnotation annotation - inherited from InterceptorHandler is an utility method to find an annotation on a method or class (in this order).","link":"services-interceptors.html"},{"title":"Internationalization","content":"Recommanded practise for internationalization are: * store messages using ResourceBundle properties file in your component module * the location of the properties are in the same package than the related component(s) and is named Messages (ex: org.talend.demo.MyComponent will use org.talend.demo.Messages[locale].properties) * for your own messages use the internationalization API Overal idea is to design its messages as methods returning String values and back the template by a ResourceBundle located in the same package than the interface defining these methods and named Messages. IMPORTANT: this is the mecanism to use to internationalize your own messages in your own components. To ensure you internationalization API is identified you need to mark it with @Internationalized: [source,java] ---- @Internationalized <1> public interface Translator {     String message();     String templatizedMessage(String arg0, int arg1); <2>     String localized(String arg0, @Language Locale locale); <3> } ---- <1> @Internationalized allows to mark a class as a i18n service <2> you can pass parameters and the message will use MessageFormat syntax to be resolved based on the ResourceBundle template <3> you can use @Language on a Locale parameter to specify manually the locale to use, note that a single value will be used (the first parameter tagged as such).","link":"services-internationalization.html"},{"title":"Creating a job pipeline","content":"The Job builder let you create a job pipeline programmatically using Talend components (xref:component-definition.html[Producers and Processors]). The job pipeline is an acyclic graph, so you can built complex pipelines. Let's take a simple use case where we will have 2 data source (employee and salary) that we will format to csv and write the result to a file. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- Job.components()   <1>         .component(\"employee\",\"db://input\")         .component(\"salary\", \"db://input\")         .component(\"concat\", \"transform://concat?separator=;\")         .component(\"csv\", \"file://out?__version=2\")     .connections()  <2>         .from(\"employee\").to(\"concat\", \"string1\")         .from(\"salary\").to(\"concat\", \"string2\")         .from(\"concat\").to(\"csv\")     .build()    <3>     .run(); <4> ---- <1> We define all the components that will be used in the job pipeline. Every component is defined by an unique id and an URI that identify the component. The URI follow the form : [family]://[component][?version][&configuration] * *family*: the name of the component family * *component*: the name of the component * *version* : the version of the component, it's represented in a key=value format. where the key is __version and the value is a number. * *configuration*: here you can provide the component configuration as key=value tuple. where the key is the path of the configuration and the value is the configuration value in string format.  EXAMPLE: \"job://csvFileGen?__version=1&path=/temp/result.csv&encoding=utf-8\" IMPORTANT: configuration parameters must be URI/URL encoded. <2> Then, we define the connections between the components to construct the job pipeline. the links from -> to use the component id and the default input/output branches. You can also connect a specific branch of a component if it has multiple or named inputs/outputs branches using the methods from(id, branchName) -> to(id, branchName). In the example above, the concat component have to inputs (string1 and string2). <3> In this step, we validate the job pipeline by asserting that :  * It has some starting components (component that don't have a from connection and that need to be of type producer).  * There is no cyclic connections. as the job pipeline need to be an acyclic graph.  * All the components used in connections are already declared.  * The connection is used only once. you can't connect a component input/output branch twice. <4> We run the job pipeline. IMPORTANT: In this version, the execution of the job is linear. the component are not executed in parallel even if some steps may be independents. The job builder let you provide some job properties : The job builder let you set a key provider to join your data when a component has multiple inputs. The key provider can be set contextually to a component or globally to the job [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- Job.components()         .component(\"employee\",\"db://input\")             .property(GroupKeyProvider.class.getName(),                  (GroupKeyProvider) context -> context.getData().getString(\"id\")) <1>         .component(\"salary\", \"db://input\")         .component(\"concat\", \"transform://concat?separator=;\")     .connections()         .from(\"employee\").to(\"concat\", \"string1\")         .from(\"salary\").to(\"concat\", \"string2\")     .build()     .property(GroupKeyProvider.class.getName(), <2>                  (GroupKeyProvider) context -> context.getData().getString(\"employee_id\"))     .run(); ---- <1> Here we have defined a key provider for the data produced by the component employee <2> Here we have defined a key provider for all the data manipulated in this job. If the incoming data has different ids you can provide a complex global key provider relaying on the context that give you the component id and the branch Name. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- GroupKeyProvider keyProvider = (GroupKeyProvider) context -> {             if (\"employee\".equals(context.getComponentId())) {                 return context.getData().getString(\"id\");             }             return context.getData().getString(\"employee_id\");           }; ---- For link:https://beam.apache.org/[beam] case, you need to rely on beam pipeline definition and use component-runtime-beam dependency which provides Beam bridges. org.talend.sdk.component.runtime.beam.TalendIO provides a way to convert a partition mapper or a processor to an input  or processor using the read or write methods. [source,java] ---- public class Main {     public static void main(final String[] args) {         final ComponentManager manager = ComponentManager.instance()         Pipeline pipeline = Pipeline.create();         //Create beam input from mapper and apply input to pipeline         pipeline.apply(TalendIO.read(manager.findMapper(manager.findMapper(\"sample\", \"reader\", 1, new HashMap<String, String>() {{                     put(\"fileprefix\", \"input\");                 }}).get()))                 .apply(new ViewsMappingTransform(emptyMap(), \"sample\")) // prepare it for the output record format (see next part)         //Create beam processor from talend processor and apply to pipeline                 .apply(TalendIO.write(manager.findProcessor(\"test\", \"writer\", 1, new HashMap<String, String>() {{                     put(\"fileprefix\", \"output\");                 }}).get(), emptyMap()));         //... run pipeline     } } ---- org.talend.sdk.component.runtime.beam.TalendFn provides the way to wrap a processor in a Beam PTransform and integrate  it in the pipeline. [source,java] ---- public class Main {     public static void main(final String[] args) {         //Component manager and pipeline initialization...         //Create beam PTransform from processor and apply input to pipeline         pipeline.apply(TalendFn.asFn(manager.findProcessor(\"sample\", \"mapper\", 1, emptyMap())).get())), emptyMap());         //... run pipeline     } } ---- The multiple inputs/outputs are represented by a Map element in beam case to avoid to use multiple inputs/outputs. TIP: you can use ViewsMappingTransform or CoGroupByKeyResultMappingTransform to adapt the input/output format to the record format representing the multiple inputs/output, so a kind of Map<String, List<?>>, but materialized as a JsonObject. Input data must be of type JsonObject in this case. IMPORTANT: Beam serializing components it is crucial to add component-runtime-standalone dependency to the project. It will take care of providing an implicit and lazy ComponentManager managing the component in a fatjar case. For simple I/O you can get automatic conversion of the Beam.io to a component I/O transparently if you decorated your PTransform with @PartitionMapper or @Processor. The limitation are: - Inputs must implement PTransform<PBegin, PCollection<?>> and must be a BoundedSource. - Outputs must implement PTransform<PCollection<?>, PDone> and just register on the input PCollection a DoFn. More information on that topic on <<wrapping-a-beam-io.adoc#, How to wrap a Beam I/O>> page.","link":"services-pipeline.html"},{"title":"Talend Component Studio Integration","content":"The studio integration relies on Component Server (see <<documentation-rest.adoc#, Web>> for more details). When the plugin is deployed it starts a local server the Studio uses to gather data about the components. Copy org.talend.sdk.component.studio-integration.jar in the $STUDIO_HOME/plugins directory and restart the studio. Also ensure the component-server dependencies  and commons-cli 1.4 are into the Studio maven repository. If you install any component, ensure its dependencies are in the repository as well. TIP: you can set in $STUDIO_HOME/configuration/config.ini the value maven.repository to global to reuse your local maven repository. The configuration goes into $STUDIO_HOME/configuration/config.ini. |=== | Name | Description | Default | component.debounce.timeout | Specifies timeout in milliseconds before calling listeners in components Text fields | 750 | component.kit.skip | If true the plugin is not active. Useful if you don't have any component develop with the framework. | false | component.java.arguments | Component server additional options | - | component.java.m2 | The maven repository the server uses to resolve components | default to global Studio configuration | component.java.coordinates | A list of comma separated GAV (groupId:artifactId:version) of components to register | - | component.java.registry | A properties file with values matching component GAV (groupId:artifactId:version) which are registered at startup | - | component.java.port | Set a port to use for the server | random | components.server.beam.active | Active, if set to true, beam support (_Experimental_). It requires beam sdk java core dependencies to be available. | false |=== IMPORTANT: to activate the plugin and be able to deploy your component, don't forget to set component.kit.skip to true. If you run multiple Studio instance automatically in parallel you can have some issues with the random port computation (can happen on a CI platform). For that purpose you can create the file $HOME/.talend/locks/org.talend.sdk.component.studio-integration.lock. When a server will start it will acquire a lock on that file and prevent another one to get a port until it is started. It ensures you can't get two concurrent processes getting the same allocated port. IMPORTANT: it is highly unlikely it happens on a desktop and forcing a different value through component.java.port in your config.ini is likely a better solution for local installations.","link":"studio.html"},{"title":"Beam testing","content":"If you want to ensure your component works in Beam the minimum to do is to try with the direct runner (if you don't want to use spark). Check https://beam.apache.org/contribute/testing/ out for more details.","link":"testing-beam.html"},{"title":"Best practises","content":"NOTE: this part is mainly around tools usable with JUnit. You can use most of these techniques with TestNG as well, check out the documentation if you need to use TestNG. This is a great solution to repeat the same test multiple times. Overall idea is to define a test scenario (I test function F) and to make the input/output data dynamic. Here is an example. Let's assume we have this test which validates the connection URI using ConnectionService: [source,java] ---- public class MyConnectionURITest {     @Test     public void checkMySQL() {         assertTrue(new ConnectionService().isValid(\"jdbc:mysql://localhost:3306/mysql\"));     }     @Test     public void checkOracle() {         assertTrue(new ConnectionService().isValid(\"jdbc:oracle:thin:@//myhost:1521/oracle\"));     } } ---- We clearly identify the test method is always the same except the value. It can therefore be rewritter using JUnit Parameterized runner like that: [source,java] ---- @RunWith(Parameterized.class) <1> public class MyConnectionURITest {     @Parameterized.Parameters(name = \"{0}\") <2>     public static Iterable<String> uris() { <3>         return asList(             \"jdbc:mysql://localhost:3306/mysql\",             \"jdbc:oracle:thin:@//myhost:1521/oracle\");     }     @Parameterized.Parameter <4>     public String uri;     @Test     public void isValid() { <5>         assertNotNull(uri);     } } ---- <1> Parameterized is the runner understanding @Parameters and how to use it. Note that you can generate random data here if desired. <2> by default the name of the executed test is the index of the data, here we customize it using the first parameter toString() value to have something more readable <3> the @Parameters method MUST be static and return an array or iterable of the data used by the tests <4> you can then inject the current data using @Parameter annotation, it can take a parameter if you use an array of array instead of an iterable of object in @Parameterized and you can select which item you want injected this way <5> the @Test method will be executed using the contextual data, in this sample we'll get executed twice with the 2 specified urls TIP: you don't have to define a single @Test method, if you define multiple, each of them will be executed with all the data (ie if we add a test in previous example you will get 4 tests execution - 2 per data, ie 2x2) JUnit 5 reworked this feature to make it way easier to use. The full documentation is available at http://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests. The main difference is you can also define inline on the test method that it is a parameterized test and which are the values: [source,java] ---- @ParameterizedTest @ValueSource(strings = { \"racecar\", \"radar\", \"able was I ere I saw elba\" }) void mytest(String currentValue) {     // do test } ---- However you can still use the previous behavior using a method binding configuration: [source,java] ---- @ParameterizedTest @MethodSource(\"stringProvider\") void mytest(String currentValue) {     // do test } static Stream<String> stringProvider() {     return Stream.of(\"foo\", \"bar\"); } ---- This last option allows you to inject any type of value - not only primitives - which is very common to define scenarii. IMPORTANT: don't forget to add junit-jupiter-params dependency to benefit from this feature.","link":"testing-best-practices.html"},{"title":"Generating data?","content":"Several data generator exists if you want to populate objects with a semantic a bit more evolved than a plain random string like commons-lang3: * https://github.com/Codearte/jfairy * https://github.com/DiUS/java-faker * https://github.com/andygibson/datafactory * ... A bit more advanced, these ones allow to bind directly generic data on a model - but data quality is not always there: * https://github.com/devopsfolks/podam * https://github.com/benas/random-beans * ... Note there are two main kind of implementations: * the one using a _pattern_ and random generated data * a set of precomputed data extrapolated to create new values Check against your use case to know which one is the best. NOTE: an interesting alternative to data generation is to import _real_ data and use Talend Studio to sanitize the data (remove sensitive information replacing them by generated data or anonymized data) and just inject that file into the system. If you are using JUnit 5, you can have a look to https://glytching.github.io/junit-extensions/randomBeans which is pretty good on that topic.","link":"testing-generating-data.html"},{"title":"component-runtime-http-junit","content":"The HTTP JUnit module allows you to mock REST API very easily. Here are its coordinates: [source,xml] ---- <dependency>   <groupId>org.talend.sdk.component</groupId>   <artifactId>component-runtime-junit</artifactId>   <version>${talend-component.version}</version>   <scope>test</scope> </dependency> ---- TIP: this module uses Apache Johnzon and Netty, if you have any conflict (in particular with netty) you can add the classifier shaded to the dependency and the two dependencies are shaded avoiding the conflicts with your component. It supports JUnit 4 and JUnit 5 as well but the overall concept is the exact same one: the extension/rule is able to serve precomputed responses saved in the classpath. You can plug your own ResponseLocator to map a request to a response but the default implementation - which should be sufficient in most cases - will look in talend/testing/http/<class name>_<method name>.json. Note that you can also put it in talend/testing/http/<request path>.json. JUnit 4 setup is done through two rules: JUnit4HttpApi which is responsible to start the server and JUnit4HttpApiPerMethodConfigurator which is responsible to configure the server per test and also handle the capture mode (see later). IMPORTANT: if you don't use the JUnit4HttpApiPerMethodConfigurator, the capture feature will be deactivated and the per test mocking will not be available. Most of the test will look like: [source,java] ---- public class MyRESTApiTest {     @ClassRule     public static final JUnit4HttpApi API = new JUnit4HttpApi();     @Rule     public final JUnit4HttpApiPerMethodConfigurator configurator = new JUnit4HttpApiPerMethodConfigurator(API);     @Test     public void direct() throws Exception {         // ... do your requests     } } ---- For tests using SSL based services, you will need to use activeSsl() on the JUnit4HttpApi rule. If you need to access the server ssl socket factory you can do it from the HttpApiHandler (the rule): [source,java] [subs=+quotes] ---- @ClassRule public static final JUnit4HttpApi API = new JUnit4HttpApi()*.activeSsl()*; @Test public void test() throws Exception {     final HttpsURLConnection connection = getHttpsConnection();     connection.setSSLSocketFactory(API.getSslContext().getSocketFactory());     // .... } ---- JUnit 5 uses a JUnit 5 extension based on the HttpApi annotation you can put on your test class. You can inject the test handler (which has some utilities for advanced cases) through @HttpApiInject: [source,java] ---- @HttpApi class JUnit5HttpApiTest {     @HttpApiInject     private HttpApiHandler<?> handler;     @Test     void getProxy() throws Exception {         // .... do your requests     } } ---- NOTE: the injection is optional and the @HttpApi allows you to configure several behaviors of the test. For tests using SSL based services, you will need to use @HttpApi(useSsl = true). You can access the client SSL socket factory through the api handler: [source,java] [subs=+quotes] ---- @HttpApi*(useSsl = true)* class MyHttpsApiTest {     @HttpApiInject     private HttpApiHandler<?> handler;     @Test     void test() throws Exception {         final HttpsURLConnection connection = getHttpsConnection();         connection.setSSLSocketFactory(handler.getSslContext().getSocketFactory());         // ....     } } ---- The strength of this implementation is to run a small proxy server and auto configure the JVM: http[s].proxyHost, http[s].proxyPort, HttpsURLConnection#defaultSSLSocketFactory and SSLContext#default are auto configured to work out of the box with the proxy. It allows you to keep in your tests the native and real URLs. For instance this test is perfectlt valid: [source,java] ---- public class GoogleTest {     @ClassRule     public static final JUnit4HttpApi API = new JUnit4HttpApi();     @Rule     public final JUnit4HttpApiPerMethodConfigurator configurator = new JUnit4HttpApiPerMethodConfigurator(API);     @Test     public void google() throws Exception {         assertEquals(HttpURLConnection.HTTP_OK, get(\"https://google.fr?q=Talend\"));     }     private int get(final String uri) throws Exception {         // do the GET request, skipped for brievity     } } ---- If you execute this test, it will fail with a HTTP 400 because the proxy doesn't find the mocked response. You can create it manually as seen in the introduction of the module but you can also set the property talend.junit.http.capture to the folder where to store the captures. It must be the root folder and not the folder where the json are (ie not prefixed by talend/testing/http by default). Generally you will want to use src/test/resources. If new File(\"src/test/resources\") resolves to the valid folder when executing your test (Maven default), then you can just set the system property to true, otherwise you need to adjust accordingly the system property value. Once you ran the tests with this system property, the testing framework will have created the correct mock response files and you can remove the system property. The test will still pass, using google.com...even if you disconnect your machine from the internet. The rule (extension) is doing all the work for you :). Setting talend.junit.http.passthrough system property to true, the server will just be a proxy and will execute each request to the actual server - like in capturing mode.","link":"testing-http.html"},{"title":"component-runtime-junit","content":"component-runtime-junit is a small test library allowing you to validate simple logic based on Talend Component tooling. To import it add to your project the following dependency: [source,xml] ---- <dependency>   <groupId>org.talend.sdk.component</groupId>   <artifactId>component-runtime-junit</artifactId>   <version>${talend-component.version}</version>   <scope>test</scope> </dependency> ---- This dependency also provide some mocked components that you can use with your own component to create tests. The mocked components are provided under the family test : * emitter     : a mock of an input component * collector   : a mock of an output component Then you can define a standard JUnit test and use the SimpleComponentRule rule: [source,java] ---- public class MyComponentTest {     @Rule <1>     public final SimpleComponentRule components = new SimpleComponentRule(\"org.talend.sdk.component.mycomponent.\");     @Test     public void produce() {         Job.components() <2>              .component(\"mycomponent\",\"yourcomponentfamily://yourcomponent?\"+createComponentConfig())              .component(\"collector\", \"test://collector\")            .connections()              .from(\"mycomponent\").to(\"collector\")            .build()            .run();         final List<MyRecord> records = components.getCollectedData(MyRecord.class); <3>         doAssertRecords(records); // depending your test     } } ---- <1> the rule will create a component manager and provide two mock components: an emitter and a collector. Don't forget to set the root package of your component to enable it. <2> you define any chain you want to test, it generally uses the mock as source or collector <3> you validate your component behavior, for a source you can assert the right records were emitted in the mock collect The JUnit 5 integration is mainly the same as for JUnit 4 except it uses the new JUnit 5 extension mecanism. The entry point is the @WithComponents annotation you put on your test class which takes the component package you want to test and you can use @Injected to inject in a test class field an instance of ComponentsHandler which exposes the same utilities than the JUnit 4 rule: [source,java] ---- @WithComponents(\"org.talend.sdk.component.junit.component\") <1> public class ComponentExtensionTest {     @Injected <2>     private ComponentsHandler handler;     @Test     public void manualMapper() {         final Mapper mapper = handler.createMapper(Source.class, new Source.Config() {             {                 values = asList(\"a\", \"b\");             }         });         assertFalse(mapper.isStream());         final Input input = mapper.create();         assertEquals(\"a\", input.next());         assertEquals(\"b\", input.next());         assertNull(input.next());     } } ---- <1> The annotation defines which components to register in the test context. <2> The field allows to get the handler to be able to orchestrate the tests. NOTE: if it is the first time you use JUnit 5, don't forget the imports changed and you must use org.junit.jupiter.api.Test instead of org.junit.Test. Some IDE versions and surefire versions can also need you to install either a plugin or a specific configuration. Using the component \"test\"/\"collector\" as in previous sample stores all records emitted by the chain (typically your source) in memory, you can then access them using theSimpleComponentRule.getCollectoedRecord(type). Note that this method filters by type, if you don't care of the type just use Object.class. The input mocking is symmetric to the output but here you provide the data you want to inject: [source,java] ---- public class MyComponentTest {     @Rule     public final SimpleComponentRule components = new SimpleComponentRule(\"org.talend.sdk.component.mycomponent.\");     @Test     public void produce() {         components.setInputData(asList(createData(), createData(), createData())); <1>         Job.components() <2>              .component(\"emitter\",\"test://emitter\")              .component(\"out\", \"yourcomponentfamily://myoutput?\"+createComponentConfig())            .connections()               .from(\"emitter\").to(\"out\")            .build            .run();         assertMyOutputProcessedTheInputData();     } } ---- <1> using setInputData you prepare the execution(s) to have a fake input when using \"test\"/\"emitter\" component. The component configuration is a POJO (using @Option on fields) and the runtime configuration (ExecutionChainBuilder) uses a Map<String, String>. To make the conversion easier, the JUnit integration provides a SimpleFactory.configurationByExample utility to get this map instance from a configuration instance. Example: [source,java] ---- final MyComponentConfig componentConfig = new MyComponentConfig(); componentConfig.setUser(\"....\"); // .. other inits final Map<String, String> configuration = configurationByExample(componentConfig); ---- The SimpleComponentRule also allows to test a mapper unitarly, you can get an instance from a configuration and you can execute this instance to collect the output. Here is a snippet doing that: [source,java] ---- public class MapperTest {     @ClassRule     public static final SimpleComponentRule COMPONENT_FACTORY = new SimpleComponentRule(             \"org.company.talend.component\");     @Test     public void mapper() {         final Mapper mapper = COMPONENT_FACTORY.createMapper(MyMapper.class, new Source.Config() {{             values = asList(\"a\", \"b\");         }});         assertEquals(asList(\"a\", \"b\"), COMPONENT_FACTORY.collectAsList(String.class, mapper));     } } ---- As for the mapper a processor is testable unitary. The case is a bit more complex since you can have multiple inputs and outputs: [source,java] ---- public class ProcessorTest {     @ClassRule     public static final SimpleComponentRule COMPONENT_FACTORY = new SimpleComponentRule(             \"org.company.talend.component\");     @Test     public void processor() {         final Processor processor = COMPONENT_FACTORY.createProcessor(Transform.class, null);         final SimpleComponentRule.Outputs outputs = COMPONENT_FACTORY.collect(processor,                         new JoinInputFactory().withInput(\"__default__\", asList(new Transform.Record(\"a\"), new Transform.Record(\"bb\")))                                               .withInput(\"second\", asList(new Transform.Record(\"1\"), new Transform.Record(\"2\")))                 );         assertEquals(2, outputs.size());         assertEquals(asList(2, 3), outputs.get(Integer.class, \"size\"));         assertEquals(asList(\"a1\", \"bb2\"), outputs.get(String.class, \"value\"));     } } ---- Here again the rule allows you to instantiate a Processor from your code and then to collect the output from the inputs you pass in. There are two convenient implementation of the input factory: 1. MainInputFactory for processors using only the default input. 2. JoinInputfactory for processors using multiple inputs have a method withInput(branch, data) The first arg is the branch name and the second arg is the data used by the branch. TIP: you can also implement your own input representation if needed implementing org.talend.sdk.component.junit.ControllableInputFactory.","link":"testing-junit.html"},{"title":"Secrets/Passwords and Maven","content":"If you desire you can reuse your Maven settings.xml servers - including the encrypted ones. org.talend.sdk.component.maven.MavenDecrypter will give you the ability to find a server username/password from a server identifier: [source,java] ---- final MavenDecrypter decrypter = new MavenDecrypter(); final Server decrypted = decrypter.find(\"my-test-server\"); // decrypted.getUsername(); // decrypted.getPassword(); ---- It is very useful to not store secrets and test on real systems on a continuous integration platform. TIP: even if you don't use maven on the platform you can generate the settings.xml and settings-security.xml files to use that feature. See https://maven.apache.org/guides/mini/guide-encryption.html for more details.","link":"testing-maven-passwords.html"},{"title":"component-runtime-testing-spark","content":"The folowing artifact will allow you to test against a spark cluster: [source,xml] ---- <dependency>   <groupId>org.talend.sdk.component</groupId>   <artifactId>component-runtime-testing-spark</artifactId>   <version>${talend-component.version}</version>   <scope>test</scope> </dependency> ---- The usage relies on a JUnit TestRule. It is recommanded to use it as a @ClassRule to ensure a single instance of a spark cluster is built but you can also use it as a simple @Rule which means it will be created per method instead of per test class. It takes as parameter the spark and scala version to use. It will then fork a master and N slaves. Finally it will give you submit* method allowing you to send jobs either from the test classpath or from a shade if you run it as an integration test. Here is a sample: [source,java] ---- public class SparkClusterRuleTest {     @ClassRule     public static final SparkClusterRule SPARK = new SparkClusterRule(\"2.10\", \"1.6.3\", 1);     @Test     public void classpathSubmit() throws IOException {         SPARK.submitClasspath(SubmittableMain.class, getMainArgs());         // do wait the test passed     } } ---- TIP: this is working with @Parameterized so you can submit a bunch of jobs with different args and even combine it with beam TestPipeline if you make it transient! The integration with JUnit 5 of that spark cluster logic uses @WithSpark marker for the extension and let you, optionally, inject through @SparkInject, the BaseSpark<?> handler to access te spark cluster meta information - like its host/port. Here is a basic test using it: [source,java] ---- @WithSpark class SparkExtensionTest {     @SparkInject     private BaseSpark<?> spark;     @Test     void classpathSubmit() throws IOException {         final File out = new File(jarLocation(SparkClusterRuleTest.class).getParentFile(), \"classpathSubmitJunit5.out\");         if (out.exists()) {             out.delete();         }         spark.submitClasspath(SparkClusterRuleTest.SubmittableMain.class, spark.getSparkMaster(), out.getAbsolutePath());         await().atMost(5, MINUTES).until(                 () -> out.exists() ? Files.readAllLines(out.toPath()).stream().collect(joining(\"\\n\")).trim() : null,                 equalTo(\"b -> 1\\na -> 1\"));     } } ---- In current state, SparkClusterRule doesn't allow to know a job execution is done - even if it exposes the webui url so you can poll it to check. The best at the moment is to ensure the output of your job exists and contains the right value. awaitability or equivalent library can help you to write such logic. Here are the coordinates of the artifact: [source,xml] ---- <dependency>   <groupId>org.awaitility</groupId>   <artifactId>awaitility</artifactId>   <version>3.0.0</version>   <scope>test</scope> </dependency> ---- And here is how to wait a file exists and its content (for instance) is the expected one: [source,java] ---- await()     .atMost(5, MINUTES)     .until(         () -> out.exists() ? Files.readAllLines(out.toPath()).stream().collect(joining(\"\\n\")).trim() : null,         equalTo(\"the expected content of the file\")); ----","link":"testing-spark.html"},{"title":"Test Tuto (to be replaced by real one)","content":"[role=\"time=1,15\"] This is cool - This - Too [role=\"time=16,20\"] This is not bad - Yes - No","link":"tutorial-tmp.html"},{"title":"Wrapping a Beam I/O","content":"[[wrapping-a-beam-io__start]] This part is limited to particular kinds of link:https://beam.apache.org/[Beam] PTransform: - the PTransform<PBegin, PCollection<?>> for the inputs - the PTransform<PCollection<?>, PDone> for the outputs. The outputs also must use a single (composite or not) DoFn in their apply method. Assume you want to wrap an input like this one (based on existing Beam ones): [source,java] ---- @AutoValue public abstract [static] class Read extends PTransform<PBegin, PCollection<String>> {   // config   @Override   public PCollection<String> expand(final PBegin input) {     return input.apply(         org.apache.beam.sdk.io.Read.from(new BoundedElasticsearchSource(this, null)));   }   // ... other transform methods } ---- To wrap the Read in a framework component you create a transform delegating to this one with a @PartitionMapper annotation at least (you likely want to follow the best practices as well adding @Icon and @Version) and using @Option constructor injections to configure the component: [source,java] ---- @PartitionMapper(family = \"myfamily\", name = \"myname\") public class WrapRead extends PTransform<PBegin, PCollection<String>> {   private PTransform<PBegin, PCollection<String>> delegate;   public WrapRead(@Option(\"dataset\") final WrapReadDataSet dataset) {     delegate = TheIO.read().withConfiguration(this.createConfigurationFrom(dataset));   }   @Override   public PCollection<String> expand(final PBegin input) {     return delegate.expand(input);   }   // ... other methods like the mapping with the native configuration (createConfigurationFrom) } ---- Assume you want to wrap an output like this one (based on existing Beam ones): [source,java] ---- @AutoValue public abstract [static] class Write extends PTransform<PCollection<String>, PDone> {     // configuration withXXX(...)     @Override     public PDone expand(final PCollection<String> input) {       input.apply(ParDo.of(new WriteFn(this)));       return PDone.in(input.getPipeline());     }     // other methods of the transform } ---- You can wrap this output exactly the same way than for the inputs but using @Processor this time: [source,java] ---- @PartitionMapper(family = \"myfamily\", name = \"myname\") public class WrapRead extends PTransform<PCollection<String>, PDone> {   private PTransform<PCollection<String>, PDone> delegate;   public WrapRead(@Option(\"dataset\") final WrapReadDataSet dataset) {     delegate = TheIO.write().withConfiguration(this.createConfigurationFrom(dataset));   }   @Override   public PDone expand(final PCollection<String> input) {     return delegate.expand(input);   }   // ... other methods like the mapping with the native configuration (createConfigurationFrom) } ---- Note that the class org.talend.sdk.component.runtime.beam.transform.DelegatingTransform fully delegates to another transform the \"expansion\". Therefore you can extend it and just implement the configuration mapping: [source,java] ---- @Processor(family = \"beam\", name = \"file\") public class BeamFileOutput extends DelegatingTransform<PCollection<String>, PDone> {     public BeamFileOutput(@Option(\"output\") final String output) {         super(TextIO.write()             .withSuffix(\"test\")             .to(FileBasedSink.convertToFileResourceIfPossible(output)));     } } ---- In terms of classloading, when you write an IO all the Beam SDK Java core stack is assumed in Talend Component Kit runtime as provided so never include it in compile scope - it would be ignored anyway. If you need a JSonCoder you can use org.talend.sdk.component.runtime.beam.factory.service.PluginCoderFactory service which gives you access the JSON-P and JSON-B coders. Here is a sample input based on beam Kafka: [source,java] ---- @Version @Icon(Icon.IconType.KAFKA) @Emitter(name = \"Input\") @AllArgsConstructor @Documentation(\"Kafka Input\") public class KafkaInput extends PTransform<PBegin, PCollection<JsonObject>> { <1>     private final InputConfiguration configuration;     private final JsonBuilderFactory builder;     private final PluginCoderFactory coderFactory;     private KafkaIO.Read<byte[], byte[]> delegate() {         final KafkaIO.Read<byte[], byte[]> read = KafkaIO.<byte[], byte[]> read()                 .withBootstrapServers(configuration.getBootstrapServers())                 .withTopics(configuration.getTopics().stream().map(InputConfiguration.Topic::getName).collect(toList()))                 .withKeyDeserializer(ByteArrayDeserializer.class).withValueDeserializer(ByteArrayDeserializer.class);         if (configuration.getMaxResults() > 0) {             return read.withMaxNumRecords(configuration.getMaxResults());         }         return read;     }     @Override <2>     public PCollection<JsonObject> expand(final PBegin pBegin) {         final PCollection<KafkaRecord<byte[], byte[]>> kafkaEntries = pBegin.getPipeline().apply(delegate());         return kafkaEntries.apply(ParDo.of(new RecordToJson(builder))).setCoder(coderFactory.jsonp()); <3>     }     @AllArgsConstructor     private static class RecordToJson extends DoFn<KafkaRecord<byte[], byte[]>, JsonObject> {         private final JsonBuilderFactory builder;         @ProcessElement         public void onElement(final ProcessContext context) {             context.output(toJson(context.element()));         }         // todo: we shouldnt be typed string/string so make it evolving         private JsonObject toJson(final KafkaRecord<byte[], byte[]> element) {             return builder.createObjectBuilder().add(\"key\", new String(element.getKV().getKey()))                     .add(\"value\", new String(element.getKV().getValue())).build();         }     } } ---- <1> the PTransform generics define it is an input (PBegin marker) <2> the expand method chains the native IO with a custom mapper (RecordToJson) <3> the mapper uses the JSON-P coder automatically created from the contextual component Since the Beam wrapper doesn't respect the standard Kit programming Model (no @Emitter for instance) you need to set <talend.validation.component>false</talend.validation.component> property in your pom.xml (or equivalent for Gradle) to skip the Kit component programming model validations.","link":"wrapping-a-beam-io.html"}]);

var searchInput = document.getElementById('searchInput');
var resultCounter = document.getElementById('resultCounter');
var resultTable = document.getElementById('resultTable');

searchInput.oninput = function () {
    var results = search.search(searchInput.value);

    resultCounter.innerText = results.length + ' results';

    resultTable.tBodies[0].innerHTML = '';
    if (!searchInput.value || !searchInput.value.length) {
      resultTable.classList.add('hidden');
      return;
    }
    for (var i = 0; i < results.length; i++) {
      var title = document.createElement('td');
      title.innerText = results[i].title;

      var content = document.createElement('td');
      content.innerText = results[i].content.length < 150 ? results[0].content : (results[0].content.substring(0, 147) + '...');

      var linkA = document.createElement('a');
      linkA.href = results[i].link;
      linkA.innerHTML = 'Open';
      var link = document.createElement('td');
      link.appendChild(linkA);

      var tableRow = document.createElement('tr');
      tableRow.appendChild(title);
      tableRow.appendChild(content);
      tableRow.appendChild(link);
      resultTable.tBodies[0].appendChild(tableRow);
    }
    resultTable.classList.remove('hidden');
};

if (!!window.location.search && window.location.search.indexOf('?q=') == 0) {
  searchInput.value = window.location.search.substring(3, window.location.search.length);
  searchInput.oninput();
}
