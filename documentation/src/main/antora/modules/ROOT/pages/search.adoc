= Search
:page-partial:
:page-talend_search: true

++++
[
  {
    "content":"In this tutorial we will see how to ensure the sensitive data of a component configuration is correctly handled. It is very common to define credentials in a component configuration. Most known use cases will be: 1. Passwords, 2. Secrets, 3. Potentially keys (it is also common to show them in plain text in a textarea), 4. Tokens To illustrate that we will use a REST client configuration which takes a username, password and token to connect to the REST API: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Data // or getters/setters if you don’t use lombok @GridLayout({ @GridLayout.Row({ \"username\", \"password\" }), @GridLayout.Row(\"token\") }) public class RestApiConfiguration implements Serializable { @Option private String username; @Option private String password; @Option private String token; } ---- This simple configuration defines three String without any specific widget so they will be represented as plain inputs. There are two major consequences you probably want to avoid: 1. The password and token will be clearly readable in all Talend user interfaces (Studio or Web), 2. The password and token will be potentially stored in clear. To solve that, Talend Component Kit provides you @Credential marker you can use on any @Option. This marker will have two effects: 1. Replace the default input widget by a password oriented one (See widgets gallery for screenshots), 2. Request the Studio or the Talend Cloud products to store the data as sensitive data (as encrypted values). To ensure our password and token are never stored in clear or shown in the code we migrate our previous model to the following one: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Data // or getters/setters if you don’t use lombok @GridLayout({ @GridLayout.Row({ \"username\", \"password\" }), @GridLayout.Row(\"token\") }) public class RestApiConfiguration implements Serializable { @Option private String username; @Option @Credential private String password; @Option @Credential private String token; } ---- And that it is! Now your password and token will not be accessible by error anymore :). Read more about the component configuration… Last updated 2018-04-30 17:18:07 CEST",
    "link":"tutorial-configuration-sensitive-data.html",
    "title":"Mask your configuration sensitive data"
  },
  {
    "content":"In this tutorial we will create a complete working input component for hazelcast. This will include : 1. The component family registration. 2. The component configuration and the UI layout 3. The partition mapper that let the input split it self to work in a distributed environment. 4. The source that is responsible for connecting and reading data from the data source. NOTE: Getter and Setter methods are omitted for simplicity in this tutorial We register the component family via a the package-info.java file in the package of the component. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Icon(value = Icon.IconType.CUSTOM, custom = \"hazelcast\") <1> @Components(family = \"Hazelcast\", categories = \"IMDG\") <2> package org.talend.hazelcast; ---- <1> This define the family icon. <2> In this line we define the component family and the component categories. Those information are used in the web and studio applications to group the components. The component configuration define the configurable part of the component in addition to the configuration type and the UI layout. The configuration is a simple POJO class decorated with annotations from the component framework. Here is the configuration of our component, that we will explain in details. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @GridLayout({ <1> @GridLayout.Row({ \"hazelcastXml\", \"mapName\" }), @GridLayout.Row({ \"executorService\" }), }) public class HazelcastConfiguration implements Serializable { @Option <2> private String hazelcastXml; <3> @Option private String mapName; <4> @Option private String executorService = \"default\"; <5> ClientConfig newConfig() throws IOException { <6> final ClientConfig newconfig = hazelcastXml == null ? new XmlClientConfigBuilder().build() : new XmlClientConfigBuilder(hazelcastXml).build(); newconfig.setInstanceName(getClass().getSimpleName() + \"_\" + UUID.randomUUID().toString()); newconfig.setClassLoader(Thread.currentThread().getContextClassLoader()); return newconfig; } } ---- <1> In this part we define the UI layout of the configuration. This layout will be used to show and organize the configuration in the web and Talend Studio applications. <2> All the attributes annotated by @Option are known as configuration and will be bind to a default widget according to there types, at least a specific widget is explicitly declared See widgets gallery for more details . <3> The hazelcast xml configuration file path. <4> The name of the map to be read. <5> The name of the executor service with a default name: default. <6> This only a simple utility method that convert our configuration to a hazelcast client configuration object Read more about the component configuration… As our component need to work first in distributed environments. Every input component has to define a partition mapper that will be responsible of calculating the number of sources to be created according to the hole dataset size and the requested bundle size by the targeted runner. Let’s first start examining the skeleton of our partition mapper. Then we will implement every method one by one. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Version(1) <1> @Icon(value = Icon.IconType.CUSTOM, custom = \"hazelcastInput\") <2> @PartitionMapper(name = \"Input\") <3> public class HazelcastMapper implements Serializable { private final HazelcastConfiguration configuration; private final JsonBuilderFactory jsonFactory; private final Jsonb jsonb; private final HazelcastService service; public HazelcastMapper(@Option(\"configuration\") final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service) {} <4> @PostConstruct public void init() throws IOException {} <5> @PreDestroy public void close() {} <6> @Assessor public long estimateSize() {} <7> @Split public List<HazelcastMapper> split(@PartitionSize final long bundleSize) {} <8> @Emitter public HazelcastSource createSource() {} <9> ---- <1> @Version annotation indicate the version of the component. it will be used to migrate the component configuration if needed. <2> @Icon annotation indicate the icon of the component. here we have defined a custom icon that need to be bundled in the component jar under resources/icons. <3> @PartitionMapper annotation indicate that this class is the partition mapper and give it’s name. <4> This constructor of the mapper is responsible of injecting the component configuration and services. Configuration parameter are annotated by @Option. and other parameters are considered as services and will be injected by the component framework. The service may be local services (class annotated with @Service) or some services provided by the component framework. <5> The method annotated with @PostConstruct is executed once on the driver node in a distributed environment and can be used to do some initialization. Here we will get the hazelcast instance according to the provided configuration. <6> The method annotated with @PreDestroy is used to clean resource at the end of the execution of the partition mapper. here we will shutdown the hazelcast instance loaded in the post Construct method. <7> The method annotated with @Assessor is responsible of calculating the dataset size. Here we will get the size of all the hazelcast members. <8> the method annotated with @Split is responsible of split of this mapper according to the requested bundles size by the runner and the hole dataset size. <9> The method annotated with @Emitter is responsible of creating the producer instance that will read the data from the data source (hazelcast in this case). Now that we know what we need to implement and why. Let’s start coding those methods one by one. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- private final Collection<String> members; <1> <2> public HazelcastMapper(@Option(\"configuration\") final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service) { this(configuration, jsonFactory, jsonb, service, emptyList()); } // internal <3> protected HazelcastMapper(final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service, final Collection<String> members) { this.configuration = configuration; this.jsonFactory = jsonFactory; this.jsonb = jsonb; this.service = service; this.members = members; } ---- <1> We will need the list of hazecast members later. So we add a collection attribute to the mapper <2> The component public constructor, responsible for injecting configuration and services. <3> An internal constructor that get a collection of members in addition to previous parameters. This will be useful later in this tutorial. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- private transient HazelcastInstance instance; <1> @PostConstruct public void init() throws IOException { instance = service.findInstance(configuration.newConfig()); <2> } ---- <1> We will need Hazelcast instance. we add this as an attribute to the mapper. <2> Here we create an instance of hazelcast according to the provided configuration. You can notice that we use the injected HazelcastService instance to perform that. This service is implemented in the project. Here is the HazelcastService implementation. Every class annotated with @Service can be injected to the component via it’s constructor. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- import org.talend.sdk.component.api.service.Service; @Service public class HazelcastService { public HazelcastInstance findInstance(final ClientConfig config) { return HazelcastClient.newHazelcastClient(config); <1> } } ---- <1> We create a new instance of hazelcast client. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- private transient IExecutorService executorService; <1> @PreDestroy public void close() { <2> instance.getLifecycleService().shutdown(); executorService = null; } ---- <1> This execution service will be used in our mapper. So we add it as an attribute. <2> Here we shutdown the instance that we have created in the PostConstruct. and we also free the executorService reference [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Assessor public long estimateSize() { return getSizeByMembers() <1> .values().stream() .mapToLong(this::getFutureValue) <2> .sum(); <3> } ---- <1> We get the size of all members by calling the method getSizeByMembers. This method submit a task to the cluster member that will calculate the member size locally and asynchronously. <2> We get the the size of the member from the callable task that we have submitted. <3> We sum the size of all the members Here is the implementation of the two methods used above [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- private Map<Member, Future<Long>> getSizeByMembers() { final IExecutorService executorService = getExecutorService(); final SerializableTask<Long> sizeComputation = new SerializableTask<Long>() { @Override public Long call() throws Exception { return localInstance.getMap(configuration.getMapName()).getLocalMapStats().getHeapCost(); } }; if (members.isEmpty()) { // == if no specific members defined, apply on all the cluster return executorService.submitToAllMembers(sizeComputation); } final Set<Member> members = instance.getCluster().getMembers().stream() .filter(m → this.members.contains(m.getUuid())) .collect(toSet()); return executorService.submitToMembers(sizeComputation, members); } private IExecutorService getExecutorService() { return executorService == null ? executorService = instance.getExecutorService(configuration.getExecutorService()) : executorService; } ---- [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Split public List<HazelcastMapper> split(@PartitionSize final long bundleSize) { <1> final List<HazelcastMapper> partitions = new ArrayList<>(); final Collection<Member> members = new ArrayList<>(); long current = 0; for (final Map.Entry<Member, Future<Long>> entries : getSizeByMembers().entrySet()) { final long memberSize = getFutureValue(entries.getValue()); if (members.isEmpty()) { members.add(entries.getKey()); current += memberSize; } else if (current + memberSize > bundleSize) { partitions.add( new HazelcastMapper(configuration, jsonFactory, jsonb, service, toIdentifiers(members))); // reset current iteration members.clear(); current = 0; } } if (!members.isEmpty()) { partitions.add(new HazelcastMapper(configuration, jsonFactory, jsonb, service, toIdentifiers(members))); } if (partitions.isEmpty()) { // just execute this if no plan (= no distribution) partitions.add(this); } return partitions; } ---- <1> This method create a collection of mapper according to the requested bundleSize and the dataset size. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Emitter public HazelcastSource createSource() { return new HazelcastSource(configuration, jsonFactory, jsonb, service, members); <1> } ---- <1> After we have split the mapper. now every mapper will create a producer that will read the records according to the provided configuration. Here is the full code source for the partition mapper to have a global view of it. Read more about partition mapper… [source,java,indent=0,subs=\"verbatim,quotes,attributes\",role=\"initial-block-closed\"] ---- @Version(1) <1> @Icon(Icon.IconType.DB_INPUT) <2> @PartitionMapper(name = \"Input\") <3> public class HazelcastMapper implements Serializable { private final HazelcastConfiguration configuration; private final JsonBuilderFactory jsonFactory; private final Jsonb jsonb; private final HazelcastService service; private final Collection<String> members; private transient HazelcastInstance instance; private transient IExecutorService executorService; // framework API public HazelcastMapper(@Option(\"configuration\") final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service) { this(configuration, jsonFactory, jsonb, service, emptyList()); } // internal protected HazelcastMapper(final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service, final Collection<String> members) { this.configuration = configuration; this.jsonFactory = jsonFactory; this.jsonb = jsonb; this.service = service; this.members = members; } @PostConstruct public void init() throws IOException { // Here we create an instance of hazelcast according to the provided configuration // Here you can notice that we use the injected HazelcastService instance to perform that. // This service is implemented in the project. See the implementation in <1> instance = service.findInstance(configuration.newConfig()); } @PreDestroy public void close() { // Here we shutdown the instance that we have created in the PostConstruct. and we free the executorService reference instance.getLifecycleService().shutdown(); executorService = null; } @Assessor public long estimateSize() { // Here we calculate the hole size of all memebers return getSizeByMembers().values().stream() .mapToLong(this::getFutureValue) .sum(); } // This method return a map of size by memeber of hazelcast cluster private Map<Member, Future<Long>> getSizeByMembers() { final IExecutorService executorService = getExecutorService(); final SerializableTask<Long> sizeComputation = new SerializableTask<Long>() { @Override public Long call() throws Exception { return localInstance.getMap(configuration.getMapName()).getLocalMapStats().getHeapCost(); } }; if (members.isEmpty()) { // == if no specific memebers defined, apply on all the cluster return executorService.submitToAllMembers(sizeComputation); } final Set<Member> members = instance.getCluster().getMembers().stream() .filter(m → this.members.contains(m.getUuid())) .collect(toSet()); return executorService.submitToMembers(sizeComputation, members); } // This method create a collection of mapper according to the requested bundleSize and the dataset size @Split public List<HazelcastMapper> split(@PartitionSize final long bundleSize) { final List<HazelcastMapper> partitions = new ArrayList<>(); final Collection<Member> members = new ArrayList<>(); long current = 0; for (final Map.Entry<Member, Future<Long>> entries : getSizeByMembers().entrySet()) { final long memberSize = getFutureValue(entries.getValue()); if (members.isEmpty()) { members.add(entries.getKey()); current += memberSize; } else if (current + memberSize > bundleSize) { partitions.add( new HazelcastMapper(configuration, jsonFactory, jsonb, service, toIdentifiers(members))); // reset current iteration members.clear(); current = 0; } } if (!members.isEmpty()) { partitions.add(new HazelcastMapper(configuration, jsonFactory, jsonb, service, toIdentifiers(members))); } if (partitions.isEmpty()) { // just execute this if no plan (= no distribution) partitions.add(this); } return partitions; } //After we have splited the mapper. now every mapper will create an emitter that // will read the records according to the provided configuration @Emitter public HazelcastSource createSource() { return new HazelcastSource(configuration, jsonFactory, jsonb, service, members); } private Set<String> toIdentifiers(final Collection<Member> members) { return members.stream().map(Member::getUuid).collect(toSet()); } private long getFutureValue(final Future<Long> future) { try { return future.get(configuration.getTimeout(), SECONDS); } catch (final InterruptedException e) { Thread.currentThread().interrupt(); throw new IllegalStateException(e); } catch (final ExecutionException | TimeoutException e) { throw new IllegalArgumentException(e); } } private IExecutorService getExecutorService() { return executorService == null ? executorService = instance.getExecutorService(configuration.getExecutorService()) : executorService; } } ---- Now that we have setup our component configuration and written our partition mapper that will create our producers. Let implement the source logic that will use the configuration provided by the mapper to read the records from the data source. To implement a source we need to implement the producer method that will produce a record every time it’s invoked. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- public class HazelcastSource implements Serializable { private final HazelcastConfiguration configuration; private final JsonBuilderFactory jsonFactory; private final Jsonb jsonb; private final HazelcastService service; private final Collection<String> members; private transient HazelcastInstance instance; private transient BufferizedProducerSupport<JsonObject> buffer; <1> // The constructor was omited to reduce the code @PostConstruct <2> public void createInstance() throws IOException { instance = service.findInstance(configuration.newConfig()); final Iterator<Member> memberIterators = instance.getCluster().getMembers().stream() .filter(m → members.isEmpty() || members.contains(m.getUuid())) .collect(toSet()) .iterator(); buffer = new BufferizedProducerSupport<>) → { if (!memberIterators.hasNext( { return null; } final Member member = memberIterators.next(); // note: this works if this jar is deployed on the hz cluster try { return instance.getExecutorService(configuration.getExecutorService()) .submitToMember(new SerializableTask<Map<String, String>>() { @Override public Map<String, String> call() throws Exception { final IMap<Object, Object> map = localInstance.getMap(configuration.getMapName()); final Set<?> keys = map.localKeySet(); return keys.stream().collect(toMap(jsonb::toJson, e → jsonb.toJson(map.get(e)))); } }, member).get(configuration.getTimeout(), SECONDS).entrySet().stream() .map(entry → { final JsonObjectBuilder builder = jsonFactory.createObjectBuilder(); if (entry.getKey().startsWith(\"{\")) { builder.add(\"key\", jsonb.fromJson(entry.getKey(), JsonObject.class)); } else { // plain string builder.add(\"key\", entry.getKey()); } if (entry.getValue().startsWith(\"{\")) { builder.add(\"value\", jsonb.fromJson(entry.getValue(), JsonObject.class)); } else { // plain string builder.add(\"value\", entry.getValue()); } return builder.build(); }) .collect(toList()) .iterator(); } catch (final InterruptedException e) { Thread.currentThread().interrupt(); throw new IllegalStateException(e); } catch (final ExecutionException | TimeoutException e) { throw new IllegalArgumentException(e); } }); } @Producer <3> public JsonObject next() { return buffer.next(); } @PreDestroy <4> public void destroyInstance() { //We shutdown the hazelcast instance instance.getLifecycleService().shutdown(); } } ---- <1> This BufferizedProducerSupport is a utility class that encapsulate the buffering logic so that you need only to provide how to load the data and note the logic to iterate on it. Here in this case the buffer will be created in the PostConstruct method and loaded once, then used to produce records one by one. <2> the method annotated with @PostConstruct is invoked once on the node. so here we can create some connection, do some initialisation of buffering. In our case we are creating a buffer of records in this method using the BufferizedProducerSupport class. <3> The method annotated with @Producer is responsible of producing record. this method return null when there is no more record to read <4> The method annotated with @PreDestroy is called before the Source destruction and it used to clean up all the resources used in the Source. In our case we are shutting down the hazelcast instance that we have created in the post construct method. Read more about source … We have seen how to create a complete working input in this tutorial. In the next one we will explain how to create some unit tests for it. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-create-an-input-component.html",
    "title":"Create an input component"
  },
  {
    "content":"The Component API is The component API has multiple strong choices: . it is declarative (through annotations) to ensure it is .. evolutive (it can get new fancy features without breaking old code) .. static as much as possible Being fully declarative, any new API can be added iteratively without requiring any changes to existing components. Example (projection on beam potential evolution): [source,java] ---- @ElementListener public MyOutput onElement(MyInput data) { return …; } ---- wouldn’t be affected by the addition of the new Timer API which can be used like: [source,java] ---- @ElementListener public MyOutput onElement(MyInput data, @Timer(\"my-timer\") Timer timer) { return …; } ---- Intent of the framework is to be able to fit java UI as well as web UI. It must be understood as colocalized and remote UI. The direct impact of that choice is to try to move as much as possible the logic to the UI side for UI related actions. Typically we want to validate a pattern, a size, … on the client side and not on the server side. Being static encourages this practise. The other goal to be really static in its definition is to ensure the model will not be mutated at runtime and all the auditing and modelling can be done before, in the design phase. Being static also ensures the development can be validated as much as possible through build tools. This doesn’t replace the requirement to test the components but helps the developer to maintain its components with automated tools. The processor API supports JsonObject as well as any custom model. Intent is to support generic component development which need to access configured \"object paths\" and specific components which rely on a well defined path from the input. A generic component would look like: [source,java] ---- @ElementListener public MyOutput onElement(JsonObject input) { return …; } ---- A specific component would look like (with MyInput a POJO): [source,java] ---- @ElementListener public MyOutput onElement(MyInput input) { return …; } ---- By design the framework must run in DI (plain standalone java program) but also in Beam pipelines. It is also out of scope of the framework to handle the way the runtime serializes - if needed - the data. For that reason it is primordial to not import serialization constraint in the stack. This is why JsonObject is not an IndexedRecord from avro for instance, to not impose any implementation. Any actual serialization concern - implementation - should either be hidden in the framework runtime (= outside component developer scope) or in the runtime integration with the framework (beam integration for instance). In this context, JSON-P is a good compromise because it brings a very powerful API with very few constraints. The components must be able to execute even if they have conflicting libraries. For that purpose it requires to isolate their classloaders. For that purpose a component will define its dependencies based on a maven format and will always be bound to its own classloader. The definition payload is as flat as possible and strongly typed to ensure it can be manipulated by consumers. This way the consumers can add/remove fields with just some mapping rules and don’t require any abstract tree handling. The execution (runtime) configuration is the concatenation of a few framework metadata (only the version actually) and a key/value model of the instance of the configuration based on the definition properties paths for the keys. This enables the consumers to maintain and work with the keys/values up to their need. The framework not being responsible for any persistence it is crucial to ensure consumers can handle it from end to end which includes the ability to search for values (update a machine, update a port etc…) and keys (new encryption rule on key certificate for instance). Talend component is a metamodel provider (to build forms) and runtime execution platform (take a configuration instance and use it volatively to execute a component logic). This implies it can’t own the data more than defining the contract it has for these two endpoints and must let the consumers handle the data lifecycle (creation, encryption, deletion, ….). A new mime type called talend/stream is introduced to define a streaming format. It basically matches a JSON object per line: [source,javascript] ---- {\"key1\":\"value1\"} {\"key2\":\"value2\"} {\"key1\":\"value11\"} {\"key1\":\"value111\"} {\"key2\":\"value2\"} ---- Icons (@Icon) are based on a fixed set. Even if a custom icon is usable this is without any guarantee. This comes from the fact components can be used in any environment and require a kind of uniform look which can’t be guaranteed outside the UI itself so defining only keys is the best way to communicate this information. TIP: when you exactly know how you will deploy your component (ie in the Studio) then you can use @Icon(value = CUSTOM, custom = \"…\") to use a custom icon file. Last updated 2018-04-30 17:18:08 CEST",
    "link":"design.html",
    "title":"Talend Component Design Choices"
  },
  {
    "content":"In this tutorial we will show how to create components that consume a REST API. As an example, we will develop an input component that will provide a search functionality for Zendesk using there Search API. NOTE: We use lambok. to get ride of getters, setters and constructors from our classes. TIP: You can generate a project using the components kit starter as described in this tutorial. As our input component will relay on Zendesk Search API. We will need an http client to consume it. Zendesk Search API takes the following query parameters on this endpoint /api/v2/search.json. * query : The search query. * sort_by : One of updated_at, created_at, priority, status, or ticket_type. Defaults to sorting by relevance. * sort_order: One of asc or desc. Defaults to desc. So let’s create our http client according to that. Talend component kit provides a built-in service to create an easy to use http client in a declarative manner using java annotations. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public interface SearchClient extends HttpClient { <1> @Request(path = \"api/v2/search.json\", method = \"GET\") <2> Response<JsonObject> search(@Header(\"Authorization\") String auth,<3> <4> @Header(\"Content-Type\") String contentType, <5> @Query(\"query\") String query, <6> @Query(\"sort_by\") String sortBy, @Query(\"sort_order\") String sortOrder, @Query(\"page\") Integer page ); } ---- <1> Our interface need to extend org.talend.sdk.component.api.service.http.HttpClient to be known as an http client by the component framework. This interface also provides void base(String base) method that will let us set the base uri for the http request. In our case, it will be the Zendesk instance url. <2> @Request annotation let us define two things. the http request path and method (GET, POST, PUT,…). <3> At this line we have two important things. The method return type and a header param. At this point we will explain the method return that is of type Response<JsonObject>. The Response object let us access to the http response status code, headers, error payload and the response body that will be of type JsonObject here. The response body will be decoded according to the content type returned by the API. The component framework provides codec for json content. If you want to consume specific content type, you will need to provide your personalized codec using the @Codec annotation. <4> We define the Authorization http request header that will let us provide the authorization token. <5> We define another http request header to provide the content type. <6> We define the query parameters using the @Query annotation that will provide the parameter name. And that all what we need to do to create our http client. No implementation is needed for the interface, as it will be provided by the component framework according to what we have defined. TIP: This http client can be injected into a mapper or a processor to perform http requests. NOTE: For the sake of simplicity, we will use the basic authentication supported by the API. Let’s start setting up the configuration for the basic authentication. To be able to consume the Search API, we will need to provide the Zendesk instance URL, the username and the password. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- @Data @DataStore <1> @GridLayout({ <2> @GridLayout.Row({ \"url\" }), @GridLayout.Row({ \"username\", \"password\" }) }) @Documentation(\"Basic authentication for Zendesk API\") public class BasicAuth { @Option @Documentation(\"Zendesk instance url\") private final String url; @Option @Documentation(\"Zendesk account username (e-mail).\") private final String username; @Option @Credential <3> @Documentation(\"Zendesk account password\") private final String password; public String getAuthorizationHeader() { <4> try { return \"Basic \" + Base64.getEncoder() .encodeToStringthis.getUsername() + \":\" + this.getPassword(.getBytes(\"UTF-8\")); } catch (UnsupportedEncodingException e) { throw new RuntimeException(e); } } } ---- <1> As This configuration class provide the authentication information. We can type it as Datastore, so that it can be validated using services (a kind of test connection feature) or used by Talend studio or web application metadata. <2> This is the UI layout of this configuration. <3> We mark the password as Credential to that it can be handled as sensitive data in Talend Studio and web application. Read more about sensitive data handling. <4> This method generate a basic authentication token using the username and the password. This token will be used to authenticate our http call to the Search API. Now that we have our data store configuration. that will provide us with the basic authentication token. We need to setup our data set configuration. i.e the search query that will define the records that our input component will provide. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- @Data @DataSet <1> @GridLayout({ <2> @GridLayout.Row({ \"dataStore\" }), @GridLayout.Row({ \"query\" }), @GridLayout.Row({ \"sortBy\", \"sortOrder\" }) }) @Documentation(\"Data set that define a search query for Zendesk Search API. See api reference developer.zendesk.com/rest_api/docs/core/search\") public class SearchQuery { @Option @Documentation(\"Authentication information.\") private final BasicAuth dataStore; @Option @TextArea <3> @Documentation(\"Search query.\") <4> private final String query; @Option @DefaultValue(\"relevance\") <5> @Documentation(\"One of updated_at, created_at, priority, status, or ticket_type. Defaults to sorting by relevance\") private final String sortBy; @Option @DefaultValue(\"desc\") @Documentation(\"One of asc or desc. Defaults to desc\") private final String sortOrder; } ---- <1> This mark this configuration class as a DataSet type. Read more about configuration type. <2> The UI layout of this configuration. <3> We bind a text area widget to the search query field. See all the available widgets. <4> Note the usage of @Documentation annotation. this annotation let us document our component (configuration in this scope). There is a Talend component maven plugin that can be used to generate the component documentation with all the configuration description and the default values. <5> Here we give the field a default value. That’s all for the configuration part. Let’s create the component logic. NOTE: We will not split the http calls on many workers. so our mappers will not implement the split part. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- @Version @Icon(value = Icon.IconType.CUSTOM, custom = \"zendesk\") @PartitionMapper(name = \"search\") @Documentation(\"Search component for zendesk query\") public class SearchMapper implements Serializable { private final SearchQuery configuration; <1> private final SearchClient searchClient; <2> public SearchMapper(@Option(\"configuration\") final SearchQuery configuration, final SearchClient searchClient) { this.configuration = configuration; this.searchClient = searchClient; } @PostConstruct public void init() { searchClient.base(configuration.getDataStore().getUrl()); <3> } @Assessor public long estimateSize() { return 1L; } @Split public List<SearchMapper> split(@PartitionSize final long bundles) { return Collections.singletonList(this); <4> } @Emitter public SearchSource createWorker() { return new SearchSource(configuration, searchClient); <5> } } ---- <1> The component configuration, that will be injected by the component framework <2> The http client that we have created above. it will also be injected by the framework via the mapper constructor. <3> We setup the base URL of our http client using the configuration url. <4> As we will not split the http requests we return this mapper in the split method. <5> We create a source that will perform the http request and return the search result. Now we create the source that will perform the http request to the search api and convert the result to JsonObject records. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public class SearchSource implements Serializable { private final SearchQuery config; <1> private final SearchClient searchClient; <2> private BufferizedProducerSupport<JsonValue> bufferedReader; <3> private transient int page = 0; private transient int previousPage = -1; public SearchSource(final SearchQuery configuration, final SearchClient searchClient) { this.config = configuration; this.searchClient = searchClient; } @PostConstruct public void init() { <4> bufferedReader = new BufferizedProducerSupport<>) → { JsonObject result = null; if (previousPage == -1) { result = search(config.getDataStore().getAuthorizationHeader(), config.getQuery(), config.getSortBy(), config.getSortBy() == null ? null : config.getSortOrder(), null); } else if (previousPage != page) { result = search(config.getDataStore().getAuthorizationHeader(), config.getQuery(), config.getSortBy(), config.getSortBy() == null ? null : config.getSortOrder(), page); } if (result == null) { return null; } previousPage = page; String nextPage = result.getString(\"next_page\", null); if (nextPage != null) { page++; } return result.getJsonArray(\"results\").iterator(); }); } @Producer public JsonObject next() { <5> final JsonValue next = bufferedReader.next(); return next == null ? null : next.asJsonObject(); } <6> private JsonObject search(String auth, String query, String sortBy, String sortOrder, Integer page) { final Response<JsonObject> response = searchClient.search(auth, \"application/json\", query, sortBy, sortOrder, page); if (response.status() == 200 && response.body().getInt(\"count\") != 0) { return response.body(); } final String mediaType = extractMediaType(response.headers(; if (mediaType != null && mediaType.contains(\"application/json\")) { final JsonObject error = response.error(JsonObject.class); throw new RuntimeException(error.getString(\"error\") + \"\\n\" + error.getString(\"description\")); } throw new RuntimeException(response.error(String.class)); } <7> private String extractMediaType(final Map<String, List<String>> headers) { final String contentType = headers == null || headers.isEmpty() || !headers.containsKey(HEADER_Content_Type) ? null : headers.get(HEADER_Content_Type).iterator().next(); if (contentType == null || contentType.isEmpty()) { return null; } // content-type contains charset and/or boundary return contentType.contains(\";\" ? contentType.split(\";\")[0] : contentType).toLowerCase(ROOT); } } ---- <1> The component configuration injected from the component mapper. <2> The http client injected from the component mapper. <3> A buffer utility that we will use to buffer search result and iterate on theme one by one <4> In the init method we initialize our record buffer by providing the logic to iterate on the search result. we get the first result page and convert the results to json records. The buffer will retrieve the next result page if needed. <5> This method return the next record from the buffer. when no more record is present the buffer return null. <6> In this method we use the http client to perform the http request to the search api. According to the http response status code we get get the results or we throw an error if needed. <7> This method let us extract the media type returned by the API. That all you will need to do to create a simple Talend component that consume a REST API. In a next tutorial, we will show how to test this kind of component and use the component framework API simulation tools to create unit tests. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-create-components-rest-api.html",
    "title":"Create components for REST API"
  },
  {
    "content":"Several data generator exists if you want to populate objects with a semantic a bit more evolved than a plain random string like commons-lang3: * github.com/Codearte/jfairy * github.com/DiUS/java-faker * github.com/andygibson/datafactory * … A bit more advanced, these ones allow to bind directly generic data on a model - but data quality is not always there: * github.com/devopsfolks/podam * github.com/benas/random-beans * … Note there are two main kind of implementations: * the one using a pattern and random generated data * a set of precomputed data extrapolated to create new values Check against your use case to know which one is the best. NOTE: an interesting alternative to data generation is to import real data and use Talend Studio to sanitize the data (remove sensitive information replacing them by generated data or anonymized data) and just inject that file into the system. If you are using JUnit 5, you can have a look to glytching.github.io/junit-extensions/randomBeans which is pretty good on that topic. Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-generating-data.html",
    "title":"Generating data?"
  },
  {
    "content":"Talend Component framework relies on several primitive components. They can all use @PostConstruct and @PreDestroy to initialize/release some underlying resource at the beginning/end of the processing. IMPORTANT: in distributed environments class' constructor will be called on cluster manager node, methods annotated with @PostConstruct and @PreDestroy annotations will be called on worker nodes. Thus, partition plan computation and pipeline task will be performed on different nodes. //// [ditaa, generated-deployment-diagram, png] …. /-------------------------\\ | Create and | |Submit task to cluster(1)| \\-------------------------/ | V --------------------------- | Cluster manager | |---------------------------| | Partition plan | | computation(2) | | | --------------------------- ^ | Serialized instances | V ----------------- | Worker node | |-----------------| |Flow Execution(3)| ----------------- …. //// 1. Created task consists of Jar file, containing class, which describes pipeline(flow) which should be processed in cluster. 2. During partition plan computation step pipeline is analyzed and split into stages. Cluster Manager node instantiates mappers/processors gets estimated data size using mappers, splits created mappers according to the estimated data size. All instances are serialized and sent to Worker nodes afterwards. 3. Serialized instances are received and deserialized, methods annotated with @PostConstruct annotation are called. After that, pipeline execution is started. Processor’s @BeforeGroup annotated method is called before processing first element in chunk. After processing number of records estimated as chunk size, Processor’s @AfterGroup annotated method called. Chunk size is calculated depending on environment the pipeline is processed by. After pipeline is processed, methods annotated with @PreDestroy annotation are called. //// [ditaa, generated-driver-processing-workflow, png] …. Partition plan computation(2) ---------------- | Create Mappers | ---------------- | V ------------------------- |Compute partition plan(2)| ------------------------- | V ---------------------- | Serialize splitted | |mappers and processors| ---------------------- …. //// //// [ditaa, generated-worker-processing-workflow, png] …. Flow Execution(3) ------------------ | @PostConstruct | | methods | ------------------ | V ------------------ | @BeforeGroup | | methods | ------------------ | V ------------------ | Perform task | | described in | | pipeline | ------------------ | V ------------------ | @AfterGroup | | methods | ------------------ | V ------------------ | @PreDestroy | | methods | ------------------ …. //// IMPORTANT: all framework managed methods MUST be public too. Private methods are ignored. NOTE: in term of design the framework tries to be as declarative as possible but also to stay extensible not using fixed interfaces or method signatures. This will allow to add incrementally new features of the underlying implementations. _ A PartitionMapper is a component able to split itself to make the execution more efficient. This concept is borrowed to big data world and useful only in this context (BEAM executions). Overall idea is to divide the work before executing it to try to reduce the overall execution time. The process is the following: 1. Estimate the size of the data you will work on. This part is often heuristic and not very precise. 2. From that size the execution engine (_runner for beam) will request the mapper to split itself in N mappers with a subset of the overall work. 3. The leaf (final) mappers will be used as a Producer (actual reader) factory. IMPORTANT: this kind of component MUST be Serializable to be distributable. A partition mapper requires 3 methods marked with specific annotations: 1. @Assessor for the evaluating method 2. @Split for the dividing method 3. @Emitter for the Producer factory The assessor method will return the estimated size of the data related to the component (depending its configuration). It MUST return a Number and MUST not take any parameter. Here is an example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\",role=\"initial-block-closed\"] ---- @Assessor public long estimateDataSetByteSize() { return ….; } ---- The split method will return a collection of partition mappers and can take optionally a @PartitionSize long value which is the requested size of the dataset per sub partition mapper. Here is an example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Split public List<MyMapper> split(@PartitionSize final long desiredSize) { return ….; } ---- The emitter method MUST not have any parameter and MUST return a producer. It generally uses the partition mapper configuration to instantiate/configure the producer. Here is an example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Emitter public MyProducer create() { return ….; } ---- A Producer is the component interacting with a physical source. It produces input data for the processing flow. A producer is a very simple component which MUST have a @Producer method without any parameter and returning any data: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Producer public MyData produces() { return …; } ---- A Processor is a component responsible to convert an incoming data to another model. A processor MUST have a method decorated with @ElementListener taking an incoming data and returning the processed data: [source,java] ---- @ElementListener public MyNewData map(final MyData data) { return …; } ---- IMPORTANT: this kind of component MUST be Serializable since it is distributed. IMPORTANT: if you don’t care much of the type of the parameter and need to access data on a \"map like\" based rule set, then you can use JsonObject as parameter type and Talend Component will just wrap the data to enable you to access it as a map. The parameter type is not enforced, i.e. if you know you will get a SuperCustomDto then you can use that as parameter type but for generic component reusable in any chain it is more than highly encouraged to use JsonObject until you have your an evaluation language based processor (which has its own way to access component). Here is an example: [source,java] ---- @ElementListener public MyNewData map(final JsonObject incomingData) { String name = incomingData.getString(\"name\"); int name = incomingData.getInt(\"age\"); return …; } // equivalent to (using POJO subclassing) public class Person { private String age; private int age; // getters/setters } @ElementListener public MyNewData map(final Person person) { String name = person.getName(); int name = person.getAge(); return …; } ---- A processor also supports @BeforeGroup and @AfterGroup which MUST be methods without parameters and returning void (result would be ignored). This is used by the runtime to mark a chunk of the data in a way which is estimated good for the execution flow size. IMPORTANT: this is estimated so you don’t have any guarantee on the size of a group. You can literally have groups of size 1. The common usage is to batch records for performance reasons: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @BeforeGroup public void initBatch() { // … } @AfterGroup public void endBatch() { // … } ---- IMPORTANT: it is a good practise to support a maxBatchSize here and potentially commit before the end of the group in case of a computed size which is way too big for your backend. In some case you may want to split the output of a processor in two. A common example is \"main\" and \"reject\" branches where part of the incoming data are put in a specific bucket to be processed later. This can be done using @Output. This can be used as a replacement of the returned value: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void map(final MyData data, @Output final OutputEmitter<MyNewData> output) { output.emit(createNewData(data)); } ---- Or you can pass it a string which will represent the new branch: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void map(final MyData data, @Output final OutputEmitter<MyNewData> main, @Output(\"rejected\") final OutputEmitter<MyNewDataWithError> rejected) { if (isRejected(data)) { rejected.emit(createNewData(data)); } else { main.emit(createNewData(data)); } } // or simply @ElementListener public MyNewData map(final MyData data, @Output(\"rejected\") final OutputEmitter<MyNewDataWithError> rejected) { if (isSuspicious(data)) { rejected.emit(createNewData(data)); return createNewData(data); // in this case we continue the processing anyway but notified another channel } return createNewData(data); } ---- Having multiple inputs is closeto the output case excep it doesn’t require a wrapper OutputEmitter: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public MyNewData map(@Input final MyData data, @Input(\"input2\") final MyData2 data2) { return createNewData(data1, data2); } ---- @Input takes the input name as parameter, if not set it uses the main (default) input branch. IMPORTANT: due to the work required to not use the default branch it is recommended to use it when possible and not name its branches depending on the component semantic. An Output is a Processor returning no data. __ Conceptually an output is a listener of data. It perfectly matches the concept of processor. Being the last of the execution chain or returning no data will make your processor an output: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void store(final MyData data) { // … } ---- For now Talend Component doesn’t enable you to define a Combiner. It would be the symmetric part of the partition mapper and allow to aggregate results in a single one. Every component family and component need to have a representative icon. You can use one of the icons provided by the component framework or you can use a custom icon. For the component family the icon is defined in package-info.java and for the component it need to be declared in the component class. To use a custom icon, you need to have the icon file placed in resources/icons folder of the project. The icon file need to have a name following the convention IconName_icon32.png [source,java] ---- @Icon(value = Icon.IconType.CUSTOM, custom = \"IconName\") ---- Last updated 2018-04-30 17:18:08 CEST",
    "link":"component-definition.html",
    "title":"Components Definition"
  },
  {
    "content":"Talend Component framework is under the responsability of Mike Hirt team. If you know nothing about Talend Components, the getting started is the place to start with. * From scratch: Overview | Requirements * Tutorial: Code | * Core features: Overview * Advanced: Testing Last updated 2018-04-30 17:18:08 CEST",
    "link":"documentation-overview.html",
    "title":"Talend Component Documentation Overview"
  },
  {
    "content":"API Documentation * JUnit API Documentation * HTTP JUnit API Documentation Last updated 2018-04-30 17:18:08 CEST",
    "link":"apidocs.html",
    "title":"Talend Component Javadocs"
  },
  {
    "content":"The Job builder let you create a job pipeline programmatically using Talend components (Producers and Processors). The job pipeline is an acyclic graph, so you can built complex pipelines. Let’s take a simple use case where we will have 2 data source (employee and salary) that we will format to csv and write the result to a file. A job is defined based on components (nodes) and links (edges) to connect their branches together. Every component is defined by an unique id and an URI that identify the component. The URI follow the form : [family]://[component][?version][&configuration] * family: the name of the component family * component: the name of the component * version : the version of the component, it’s represented in a key=value format. where the key is version and the value is a number. * configuration: here you can provide the component configuration as key=value tuple where the key is the path of the configuration and the value is the configuration value in string format. .URI Example [source] ---- job://csvFileGen?version=1&path=/temp/result.csv&encoding=utf-8\" ---- IMPORTANT: configuration parameters must be URI/URL encoded. Here is a more concrete job example: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- Job.components() <1> .component(\"employee\",\"db://input\") .component(\"salary\", \"db://input\") .component(\"concat\", \"transform://concat?separator=;\") .component(\"csv\", \"file://out?__version=2\") .connections() <2> .from(\"employee\").to(\"concat\", \"string1\") .from(\"salary\").to(\"concat\", \"string2\") .from(\"concat\").to(\"csv\") .build() <3> .run(); <4> ---- <1> We define all the components that will be used in the job pipeline. <2> Then, we define the connections between the components to construct the job pipeline. the links from → to use the component id and the default input/output branches. You can also connect a specific branch of a component if it has multiple or named inputs/outputs branches using the methods from(id, branchName) → to(id, branchName). In the example above, the concat component have to inputs (string1 and string2). <3> In this step, we validate the job pipeline by asserting that : * It has some starting components (component that don’t have a from connection and that need to be of type producer). * There is no cyclic connections. as the job pipeline need to be an acyclic graph. * All the components used in connections are already declared. * The connection is used only once. you can’t connect a component input/output branch twice. <4> We run the job pipeline. IMPORTANT: In this version, the execution of the job is linear. the component are not executed in parallel even if some steps may be independents. Depending the configuration you can select which environment you execute your job in. To select the environment the logic is the following one: 1. if an org.talend.sdk.component.runtime.manager.chain.Job.ExecutorBuilder is passed through the job properties then use it (supported type are a ExecutionBuilder instance, a Class or a String). 2. if an ExecutionBuilder SPI is present then use it (it is the case if component-runtime-beam is present in your classpath). 3. else just use a local/standalone execution. In the case of a Beam execution you can customize the pipeline options using system properties. They have to be prefixed by talend.beam.job.. For instance to set appName option you will set -Dtalend.beam.job.appName=mytest. The job builder let you set a key provider to join your data when a component has multiple inputs. The key provider can be set contextually to a component or globally to the job [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- Job.components() .component(\"employee\",\"db://input\") .property(GroupKeyProvider.class.getName(), (GroupKeyProvider) context → context.getData().getString(\"id\")) <1> .component(\"salary\", \"db://input\") .component(\"concat\", \"transform://concat?separator=;\") .connections() .from(\"employee\").to(\"concat\", \"string1\") .from(\"salary\").to(\"concat\", \"string2\") .build() .property(GroupKeyProvider.class.getName(), <2> (GroupKeyProvider) context → context.getData().getString(\"employee_id\")) .run(); ---- <1> Here we have defined a key provider for the data produced by the component employee <2> Here we have defined a key provider for all the data manipulated in this job. If the incoming data has different ids you can provide a complex global key provider relaying on the context that give you the component id and the branch Name. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- GroupKeyProvider keyProvider = context → { if (\"employee\".equals(context.getComponentId())) { return context.getData().getString(\"id\"); } return context.getData().getString(\"employee_id\"); }; ---- For beam case, you need to rely on beam pipeline definition and use component-runtime-beam dependency which provides Beam bridges. org.talend.sdk.component.runtime.beam.TalendIO provides a way to convert a partition mapper or a processor to an input or processor using the read or write methods. [source,java] ---- public class Main { public static void main(final String[] args) { final ComponentManager manager = ComponentManager.instance() Pipeline pipeline = Pipeline.create(); //Create beam input from mapper and apply input to pipeline pipeline.apply(TalendIO.read(manager.findMapper(manager.findMapper(\"sample\", \"reader\", 1, new HashMap<String, String>() {{ put(\"fileprefix\", \"input\"); }}).get())) .apply(new ViewsMappingTransform(emptyMap(), \"sample\")) // prepare it for the output record format (see next part) //Create beam processor from talend processor and apply to pipeline .apply(TalendIO.write(manager.findProcessor(\"test\", \"writer\", 1, new HashMap<String, String>() {{ put(\"fileprefix\", \"output\"); }}).get(), emptyMap())); //… run pipeline } } ---- org.talend.sdk.component.runtime.beam.TalendFn provides the way to wrap a processor in a Beam PTransform and integrate it in the pipeline. [source,java] ---- public class Main { public static void main(final String[] args) { //Component manager and pipeline initialization… //Create beam PTransform from processor and apply input to pipeline pipeline.apply(TalendFn.asFn(manager.findProcessor(\"sample\", \"mapper\", 1, emptyMap())).get())), emptyMap()); //… run pipeline } } ---- The multiple inputs/outputs are represented by a Map element in beam case to avoid to use multiple inputs/outputs. TIP: you can use ViewsMappingTransform or CoGroupByKeyResultMappingTransform to adapt the input/output format to the record format representing the multiple inputs/output, so a kind of Map<String, List<?>>, but materialized as a JsonObject. Input data must be of type JsonObject in this case. For simple I/O you can get automatic conversion of the Beam.io to a component I/O transparently if you decorated your PTransform with @PartitionMapper or @Processor. The limitation are: - Inputs must implement PTransform<PBegin, PCollection<?>> and must be a BoundedSource. - Outputs must implement PTransform<PCollection<?>, PDone> and just register on the input PCollection a DoFn. More information on that topic on How to wrap a Beam I/O page. Last updated 2018-04-30 17:18:08 CEST",
    "link":"services-pipeline.html",
    "title":"Creating a job pipeline"
  },
  {
    "content":"For common concerns like caching, auditing etc, it can be fancy to use interceptor like API. It is enabled by the framework on services. An interceptor defines an annotation marked with @Intercepts which defines the implementation of the interceptor (an InterceptorHandler). Here is an example: [source,java] ---- @Intercepts(LoggingHandler.class) @Target({ TYPE, METHOD }) @Retention(RUNTIME) public @interface Logged { String value(); } ---- Then handler is created from its constructor and can take service injections (by type). The first parameter, however, can be a BiFunction<Method, Object[], Object> which representes the invocation chain if your interceptor can be used with others. IMPORTANT: if you do a generic interceptor it is important to pass the invoker as first parameter. If you don’t do so you can’t combine interceptors at all. Here is an interceptor implementation for our @Logged API: [source,java] ---- public class LoggingHandler implements InterceptorHandler { // injected private final BiFunction<Method, Object[], Object> invoker; private final SomeService service; // internal private final ConcurrentMap<Method, String> loggerNames = new ConcurrentHashMap<>(); public CacheHandler(final BiFunction<Method, Object[], Object> invoker, final SomeService service) { this.invoker = invoker; this.service = service; } @Override public Object invoke(final Method method, final Object[] args) { final String name = loggerNames.computeIfAbsent(method, m → findAnnotation(m, Logged.class).get().value()); service.getLogger(name).info(\"Invoking {}\", method.getName()); return invoker.apply(method, args); } } ---- This implementation is compatible with interceptor chains since it takes the invoker as first constructor parameter and it also takes a service injection. Then the implementation just does what is needed - logging the invoked method here. NOTE: the findAnnotation annotation - inherited from InterceptorHandler is an utility method to find an annotation on a method or class (in this order). Last updated 2018-04-30 17:18:08 CEST",
    "link":"services-interceptors.html",
    "title":"Services and interceptors"
  },
  {
    "content":"It is possible to extend the Component API for custom front features. What is important here is to keep in mind you should do it only if it targets not portable components (only used by the Studio or Beam). In term of organization it is recommended to create a custom xxxx-component-api module with the new set of annotations. To extend the UI just add an annotation which can be put on @Option fields which is decorated with @Ui. All its members will be put in the metadata of the parameter. Example: [source,java] ---- @Ui @Target(TYPE) @Retention(RUNTIME) public @interface MyLayout { } ---- Last updated 2018-04-30 17:18:08 CEST",
    "link":"services-custom-api.html",
    "title":"Advanced: define a custom API"
  },
  {
    "content":"Note this part is mainly around tools usable with JUnit. You can use most of these techniques with TestNG as well, check out the documentation if you need to use TestNG. This is a great solution to repeat the same test multiple times. Overall idea is to define a test scenario (I test function F) and to make the input/output data dynamic. Here is an example. Let’s assume we have this test which validates the connection URI using ConnectionService: [source,java] ---- public class MyConnectionURITest { @Test public void checkMySQL() { assertTrue(new ConnectionService().isValid(\"jdbc:mysql://localhost:3306/mysql\")); } @Test public void checkOracle() { assertTrue(new ConnectionService().isValid(\"jdbc:oracle:thin:@//myhost:1521/oracle\")); } } ---- We clearly identify the test method is always the same except the value. It can therefore be rewritter using JUnit Parameterized runner like that: [source,java] ---- @RunWith(Parameterized.class) <1> public class MyConnectionURITest { @Parameterized.Parameters(name = \"{0}\") <2> public static Iterable<String> uris() { <3> return asList( \"jdbc:mysql://localhost:3306/mysql\", \"jdbc:oracle:thin:@//myhost:1521/oracle\"); } @Parameterized.Parameter <4> public String uri; @Test public void isValid() { <5> assertNotNull(uri); } } ---- <1> Parameterized is the runner understanding @Parameters and how to use it. Note that you can generate random data here if desired. <2> by default the name of the executed test is the index of the data, here we customize it using the first parameter toString() value to have something more readable <3> the @Parameters method MUST be static and return an array or iterable of the data used by the tests <4> you can then inject the current data using @Parameter annotation, it can take a parameter if you use an array of array instead of an iterable of object in @Parameterized and you can select which item you want injected this way <5> the @Test method will be executed using the contextual data, in this sample we’ll get executed twice with the 2 specified urls TIP: you don’t have to define a single @Test method, if you define multiple, each of them will be executed with all the data (ie if we add a test in previous example you will get 4 tests execution - 2 per data, ie 2x2) JUnit 5 reworked this feature to make it way easier to use. The full documentation is available at junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests. The main difference is you can also define inline on the test method that it is a parameterized test and which are the values: [source,java] ---- @ParameterizedTest @ValueSource(strings = { \"racecar\", \"radar\", \"able was I ere I saw elba\" }) void mytest(String currentValue) { // do test } ---- However you can still use the previous behavior using a method binding configuration: [source,java] ---- @ParameterizedTest @MethodSource(\"stringProvider\") void mytest(String currentValue) { // do test } static Stream<String> stringProvider() { return Stream.of(\"foo\", \"bar\"); } ---- This last option allows you to inject any type of value - not only primitives - which is very common to define scenarii. IMPORTANT: don’t forget to add junit-jupiter-params dependency to benefit from this feature. Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-best-practices.html",
    "title":"Best practises"
  },
  {
    "content":"[cols=\"1,3a,4a,4a\",role=\"table gallery\",options=\"header,autowidth\"] |=== | Name | Code | Studio Rendering | Web Rendering | Input/Text |[source,java] ---- @Option String config; ---- |image::gallery/widgets/studio/input.png[Studio Input,window=\"_blank\",link=\"_images/gallery/widgets/studio/input.png\"] |image::gallery/widgets/web/input.png[Web Input,window=\"_blank\",link=\"_images/gallery/widgets/web/input.png\"] | Password |[source,java] ---- @Option @Credential String config; ---- |image::gallery/widgets/studio/password.png[Studio Password,window=\"_blank\",link=\"_images/gallery/widgets/studio/password.png\"] |image::gallery/widgets/web/password.png[Web Password,window=\"_blank\",link=\"_images/gallery/widgets/web/password.png\"] | Textarea |[source,java] ---- @Option @Textarea String config; ---- |image::gallery/widgets/studio/textarea.png[Studio Textarea,window=\"_blank\",link=\"_images/gallery/widgets/studio/textarea.png\"] |image::gallery/widgets/web/textarea.png[Web Textarea,window=\"_blank\",link=\"_images/gallery/widgets/web/textarea.png\"] | Checkbox |[source,java] ---- @Option Boolean config; ---- |image::gallery/widgets/studio/checkbox.png[Studio Checkbox,window=\"_blank\",link=\"_images/gallery/widgets/studio/checkbox.png\"] |image::gallery/widgets/web/checkbox.png[Web Checkbox,window=\"_blank\",link=\"_images/gallery/widgets/web/checkbox.png\"] | List |[source,java] ---- @Option List<String> config; ---- |image::gallery/widgets/studio/list.png[Studio List,window=\"_blank\",link=\"_images/gallery/widgets/studio/list.png\"] |image::gallery/widgets/web/list.png[Web List,window=\"_blank\",link=\"_images/gallery/widgets/web/list.png\"] | Table |[source,java] ---- @Option Object config; ---- |image::gallery/widgets/studio/table.png[Studio Table,window=\"_blank\",link=\"_images/gallery/widgets/studio/table.png\"] |image::gallery/widgets/web/table.png[Web Table,window=\"_blank\",link=\"_images/gallery/widgets/web/table.png\"] | Code |[source,java] ---- @Code(\"java\") @Option String config; ---- |image::gallery/widgets/studio/javaCode.png[Studio Code,window=\"_blank\",link=\"_images/gallery/widgets/studio/javaCode.png\"] |image::gallery/widgets/web/javaCode.png[Web Code,window=\"_blank\",link=\"_images/gallery/widgets/web/javaCode.png\"] | Schema |[source,java] ---- @Option @Structure List<String> config; ---- |image::gallery/widgets/studio/schema.png[Studio Schema,window=\"_blank\",link=\"_images/gallery/widgets/studio/schema.png\"] |image::gallery/widgets/web/schema.png[Web Schema,window=\"_blank\",link=\"_images/gallery/widgets/web/schema.png\"] |=== [cols=\"1,3a,4a,4a\",role=\"table gallery\",options=\"header,autowidth\"] |=== | Name | Code | Studio Rendering | Web Rendering | Property validation |[source,java] ---- / configuration class / @Option @Validable(\"url\") String config; / service class */ @AsyncValidation(\"url\") ValidationResult doValidate(String url) { //validate the property } ---- |image::gallery/widgets/studio/validation_property.png[Studio Code,window=\"_blank\",link=\"_images/gallery/widgets/studio/validation_property.png\"] |image::gallery/widgets/web/validation_property.png[Web Code,window=\"_blank\",link=\"_images/gallery/widgets/web/validation_property.png\"] | Property validation with Pattern |[source,java] ---- / configuration class */ @Option @Pattern(\"/^[a-zA-Z\\\\-]+$/\") String username; ---- |image::gallery/widgets/studio/validation_pattern.png[Studio Code,window=\"_blank\",link=\"_images/gallery/widgets/studio/validation_pattern.png\"] |image::gallery/widgets/web/validation_pattern.png[Web Code,window=\"_blank\",link=\"_images/gallery/widgets/web/validation_pattern.png\"] | Data store validation |[source,java] ---- @Datastore @Checkable public class config { / config …/ } /** service class */ @HealthCheck public HealthCheckStatus testConnection(){ //validate the connection } ---- |image::gallery/widgets/studio/validation_datastore.png[Studio Code,window=\"_blank\",link=\"_images/gallery/widgets/studio/prop_validation.png\"] |image::gallery/widgets/web/validation_datastore.png[Web Code,window=\"_blank\",link=\"_images/gallery/widgets/web/prop_validation.png\"] |=== There are also other types of validation similar to @Pattern that you can use : * @Min, @Max for numbers. * @Unique for collection values * @Required for required configuration Last updated 2018-04-30 17:18:08 CEST",
    "link":"gallery.html",
    "title":"Gallery"
  },
  {
    "content":"If you desire you can reuse your Maven settings.xml servers - including the encrypted ones. org.talend.sdk.component.maven.MavenDecrypter will give you the ability to find a server username/password from a server identifier: [source,java] ---- final MavenDecrypter decrypter = new MavenDecrypter(); final Server decrypted = decrypter.find(\"my-test-server\"); // decrypted.getUsername(); // decrypted.getPassword(); ---- It is very useful to not store secrets and test on real systems on a continuous integration platform. TIP: even if you don’t use maven on the platform you can generate the settings.xml and settings-security.xml files to use that feature. See maven.apache.org/guides/mini/guide-encryption.html for more details. Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-maven-passwords.html",
    "title":"Secrets/Passwords and Maven"
  },
  {
    "content":"component-runtime-junit is a small test library allowing you to validate simple logic based on Talend Component tooling. To import it add to your project the following dependency: [source,xml] ---- <dependency> <groupId>org.talend.sdk.component</groupId> <artifactId>component-runtime-junit</artifactId> <version>${talend-component.version}</version> <scope>test</scope> </dependency> ---- This dependency also provide some mocked components that you can use with your own component to create tests. The mocked components are provided under the family test : * emitter : a mock of an input component * collector : a mock of an output component Then you can define a standard JUnit test and use the SimpleComponentRule rule: [source,java] ---- public class MyComponentTest { @Rule <1> public final SimpleComponentRule components = new SimpleComponentRule(\"org.talend.sdk.component.mycomponent.\"); @Test public void produce() { Job.components() <2> .component(\"mycomponent\",\"yourcomponentfamily://yourcomponent?\"+createComponentConfig()) .component(\"collector\", \"test://collector\") .connections() .from(\"mycomponent\").to(\"collector\") .build() .run(); final List<MyRecord> records = components.getCollectedData(MyRecord.class); <3> doAssertRecords(records); // depending your test } } ---- <1> the rule will create a component manager and provide two mock components: an emitter and a collector. Don’t forget to set the root package of your component to enable it. <2> you define any chain you want to test, it generally uses the mock as source or collector <3> you validate your component behavior, for a source you can assert the right records were emitted in the mock collect The JUnit 5 integration is mainly the same as for JUnit 4 except it uses the new JUnit 5 extension mecanism. The entry point is the @WithComponents annotation you put on your test class which takes the component package you want to test and you can use @Injected to inject in a test class field an instance of ComponentsHandler which exposes the same utilities than the JUnit 4 rule: [source,java] ---- @WithComponents(\"org.talend.sdk.component.junit.component\") <1> public class ComponentExtensionTest { @Injected <2> private ComponentsHandler handler; @Test public void manualMapper() { final Mapper mapper = handler.createMapper(Source.class, new Source.Config() { { values = asList(\"a\", \"b\"); } }); assertFalse(mapper.isStream()); final Input input = mapper.create(); assertEquals(\"a\", input.next()); assertEquals(\"b\", input.next()); assertNull(input.next()); } } ---- <1> The annotation defines which components to register in the test context. <2> The field allows to get the handler to be able to orchestrate the tests. NOTE: if it is the first time you use JUnit 5, don’t forget the imports changed and you must use org.junit.jupiter.api.Test instead of org.junit.Test. Some IDE versions and surefire versions can also need you to install either a plugin or a specific configuration. Using the component \"test\"/\"collector\" as in previous sample stores all records emitted by the chain (typically your source) in memory, you can then access them using theSimpleComponentRule.getCollectoedRecord(type). Note that this method filters by type, if you don’t care of the type just use Object.class. The input mocking is symmetric to the output but here you provide the data you want to inject: [source,java] ---- public class MyComponentTest { @Rule public final SimpleComponentRule components = new SimpleComponentRule(\"org.talend.sdk.component.mycomponent.\"); @Test public void produce() { components.setInputData(asList(createData(), createData(), createData())); <1> Job.components() <2> .component(\"emitter\",\"test://emitter\") .component(\"out\", \"yourcomponentfamily://myoutput?\"+createComponentConfig()) .connections() .from(\"emitter\").to(\"out\") .build .run(); assertMyOutputProcessedTheInputData(); } } ---- <1> using setInputData you prepare the execution(s) to have a fake input when using \"test\"/\"emitter\" component. The component configuration is a POJO (using @Option on fields) and the runtime configuration (ExecutionChainBuilder) uses a Map<String, String>. To make the conversion easier, the JUnit integration provides a SimpleFactory.configurationByExample utility to get this map instance from a configuration instance. Example: [source,java] ---- final MyComponentConfig componentConfig = new MyComponentConfig(); componentConfig.setUser(\"….\"); // .. other inits final Map<String, String> configuration = configurationByExample(componentConfig); ---- The same factory provides a fluent DSL to create configuration calling configurationByExample without any parameter. The advantage is to be able to convert an object as a Map<String, String> as seen previously or as a query string to use it with the Job DSL: [source,java] ---- final String uri = \"family://component?\" + configurationByExample().forInstance(componentConfig).configured().toQueryString(); ---- It handles the encoding of the URI to ensure it is correctly done. The SimpleComponentRule also allows to test a mapper unitarly, you can get an instance from a configuration and you can execute this instance to collect the output. Here is a snippet doing that: [source,java] ---- public class MapperTest { @ClassRule public static final SimpleComponentRule COMPONENT_FACTORY = new SimpleComponentRule( \"org.company.talend.component\"); @Test public void mapper() { final Mapper mapper = COMPONENT_FACTORY.createMapper(MyMapper.class, new Source.Config() {{ values = asList(\"a\", \"b\"); }}); assertEquals(asList(\"a\", \"b\"), COMPONENT_FACTORY.collectAsList(String.class, mapper)); } } ---- As for the mapper a processor is testable unitary. The case is a bit more complex since you can have multiple inputs and outputs: [source,java] ---- public class ProcessorTest { @ClassRule public static final SimpleComponentRule COMPONENT_FACTORY = new SimpleComponentRule( \"org.company.talend.component\"); @Test public void processor() { final Processor processor = COMPONENT_FACTORY.createProcessor(Transform.class, null); final SimpleComponentRule.Outputs outputs = COMPONENT_FACTORY.collect(processor, new JoinInputFactory().withInput(\"default\", asList(new Transform.Record(\"a\"), new Transform.Record(\"bb\"))) .withInput(\"second\", asList(new Transform.Record(\"1\"), new Transform.Record(\"2\"))) ); assertEquals(2, outputs.size()); assertEquals(asList(2, 3), outputs.get(Integer.class, \"size\")); assertEquals(asList(\"a1\", \"bb2\"), outputs.get(String.class, \"value\")); } } ---- Here again the rule allows you to instantiate a Processor from your code and then to collect the output from the inputs you pass in. There are two convenient implementation of the input factory: 1. MainInputFactory for processors using only the default input. 2. JoinInputfactory for processors using multiple inputs have a method withInput(branch, data) The first arg is the branch name and the second arg is the data used by the branch. TIP: you can also implement your own input representation if needed implementing org.talend.sdk.component.junit.ControllableInputFactory. Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-junit.html",
    "title":"component-runtime-junit"
  },
  {
    "content":"ifeval::[\"html5\" == \"html5\"] ifeval::[\"{docbranch}\" == \"master\"] IMPORTANT: this is a version under development which has not yet been deployed. You can however use it using the -SNAPSHOT version and Sonatype snapshot repository. TIP: if you want a PDF version of that page you can find it in our snapshots: PDF. endif::[] ifeval::[\"{docbranch}\" != \"master\"] NOTE: if you want a PDF version of that page just click on this link. endif::[] endif::[] NOTE: if you prefer you can use the single page documentation. * Overview * Getting Started * Reference Guide * Testing * Best Practices * Design choices * How to wrap a Beam I/O * Web * Talend Studio Integration * Changelog * Wall Of Fame * API Documentation * Appendix Last updated 2018-04-30 17:18:08 CEST",
    "link":"index.html",
    "title":"Talend Component Kit Developer Reference Guide"
  },
  {
    "content":"The framework provides some built-in services you can inject by type in components and actions out of the box. Here is the list: [options=\"header,autowidth\"] |=== | Type | Description a| org.talend.sdk.component.api.service.cache.LocalCache | Provides a small abstraction to cache data which don’t need to be recomputed very often. Commonly used by actions for the UI interactions. a| org.talend.sdk.component.api.service.dependency.Resolver a| Allows to resolve a dependency from its Maven coordinates. a| javax.json.bind.Jsonb a| A JSON-B instance. If your model is static and you don’t want to handle the serialization manually using JSON-P you can inject that instance. a| javax.json.spi.JsonProvider a| A JSON-P instance. Prefer other JSON-P instances if you don’t exactly know why you use this one. a| javax.json.JsonBuilderFactory a| A JSON-P instance. It is recommended to use this one instead of a custom one for memory/speed optimizations. a| javax.json.JsonWriterFactory a| A JSON-P instance. It is recommended to use this one instead of a custom one for memory/speed optimizations. a| javax.json.JsonReaderFactory a| A JSON-P instance. It is recommended to use this one instead of a custom one for memory/speed optimizations. a| javax.json.stream.JsonParserFactory a| A JSON-P instance. It is recommended to use this one instead of a custom one for memory/speed optimizations. a| javax.json.stream.JsonGeneratorFactory a| A JSON-P instance. It is recommended to use this one instead of a custom one for memory/speed optimizations. a| org.talend.sdk.component.api.service.configuration.LocalConfiguration a| Represents the local configuration which can be used during the design. WARNING: it is not recommended to use it for the runtime since the local configuration is generally different and the instances are distincts. TIP: you can also use the local cache as an interceptor with @Cached a| Every interface that extends HttpClient and that contains methods annotated with @Request a| This let you define an http client in a declarative manner using an annotated interface. TIP: See the [_httpclient_usage] for details. |=== IMPORTANT: all these injected instances are serializable which is important for the big data environment, if you create the instances yourself you will not benefit from that features and the memory optimization done by the runtime so try to prefer to reuse the framework instances over custom ones. Let assume that we have a REST API defined like below, and that it requires a basic authentication header. |=== | GET /api/records/{id} | - | POST /api/records | with a json playload to be created {\"id\":\"some id\", \"data\":\"some data\"} |=== To create an http client able to consume this REST API, we will define an interface that extends HttpClient, The HttpClient interface lets you set the base for the http address that our client will hit. The base is the part of the address that we will need to add to the request path to hit the api. Every method annotated with @Request of our interface will define an http request. Also every request can have @Codec that let us encode/decode the request/response playloads. TIP: if your payload(s) is(are) String or Void you can ignore the coder/decoder. [source,java] ---- public interface APIClient extends HttpClient { @Request(path = \"api/records/{id}\", method = \"GET\") @Codec(decoder = RecordDecoder.class) //decoder = decode returned data to Record class Record getRecord(@Header(\"Authorization\") String basicAuth, @Path(\"id\") int id); @Request(path = \"api/records\", method = \"POST\") @Codec(encoder = RecordEncoder.class, decoder = RecordDecoder.class) //encoder = encode record to fit request format (json in this example) Record createRecord(@Header(\"Authorization\") String basicAuth, Record record); } ---- IMPORTANT: The interface should extends HttpClient. In the codec classes (class that implement Encoder/Decoder) you can inject any of your services annotated with @Service or @Internationalized into the constructor. The i18n services can be useful to have i18n messages for errors handling for example. This interface can be injected into our Components classes or Services to consume the defined api. [source,java] ---- @Service public class MyService { private APIClient client; public MyService(…,APIClient client){ //… this.client = client; client.base(\"http://localhost:8080\");// init the base of the api, ofen in a PostConstruct or init method } //… // Our get request Record rec = client.getRecord(\"Basic MLFKG?VKFJ\", 100); //… // Our post request Record newRecord = client.createRecord(\"Basic MLFKG?VKFJ\", new Record()); } ---- Note: by default /+json are mapped to JSON-P and /+xml to JAX-B if the model has a @XmlRootElement annotation. For advanced cases you can customize the Connection directly using @UseConfigurer on the method. It will call your custom instance of Configurer. Note that you can use some @ConfigurerOption in the method signature to pass some configurer configuration. For instance if you have this configurer: [source,java] ---- public class BasicConfigurer implements Configurer { @Override public void configure(final Connection connection, final ConfigurerConfiguration configuration) { final String user = configuration.get(\"username\", String.class); final String pwd = configuration.get(\"password\", String.class); connection.withHeader( \"Authorization\", Base64.getEncoder().encodeToStringuser + ':' + pwd).getBytes(StandardCharsets.UTF_8); } } ---- You can then set it on a method to automatically add the basic header with this kind of API usage: [source,java] ---- public interface APIClient extends HttpClient { @Request(path = \"…\") @UseConfigurer(BasicConfigurer.class) Record findRecord(@ConfigurerOption(\"username\") String user, @ConfigurerOption(\"password\") String pwd); } ---- Last updated 2018-04-30 17:18:08 CEST",
    "link":"services-built-in.html",
    "title":"Built in services"
  },
  {
    "content":"In the simplest case you should store messages using ResourceBundle properties file in your component module to use internationalization. The location of the properties file should be in the same package as the related component(s) and is named Messages (ex: org.talend.demo.MyComponent will use org.talend.demo.Messages[locale].properties). Out of the box components are internationalized using the same location logic for the resource bundle and here is the list of supported keys: [options=\"header,autowidth\"] |==== |Name Pattern|Description |${family}._displayName|the display name of the family |${family}.${configurationType}.${name}._displayName|the display name of a configuration type (dataStore or dataSet) |${family}.${component_name}._displayName|the display name of the component (used by the GUIs) |${property_path}._displayName|the display name of the option. |${simple_class_name}.${property_name}._displayName|the display name of the option using it class name. |${enum_simple_class_name}.${enum_name}._displayName|the display name of the enum_name enum value of the enum enum_simple_class_name. |${property_path}._placeholder|the placeholder of the option. |==== Example of configuration for a component named list belonging to the family memory (@Emitter(family = \"memory\", name = \"list\")): [source] ---- memory.list._displayName = Memory List ---- Configuration class are also translatable using the simple class name in the messages properties file. This useful when you have some common configuration shared within multiple components. If you have a configuration class like : [source,java] ---- public class MyConfig { @Option private String host; @Option private int port; } ---- You can give it a translatable display name by adding ${simple_class_name}.${property_name}._displayName to Messages.properties under the same package as the config class. [source] ---- MyConfig.host._displayName = Server Host Name MyConfig.host._placeholder = Enter Server Host Name… MyConfig.port._displayName = Server Port MyConfig.port._placeholder = Enter Server Port… ---- IMPORTANT: If you have a display name using the property path, it will override the display name defined using the simple class name. this rule apply also to placeholders Last updated 2018-04-30 17:18:08 CEST",
    "link":"component-internationalization.html",
    "title":"Internationalization"
  },
  {
    "content":"Version: 0.0.12 This page gives some hints about how to release the repository. Before configuring Maven you need to have a GPG key. Once you installed GPG, you can either import an existing key or generate one using gpg --gen-key. Then a few entries into your maven settings.xml are needed to provide the needed credentials for the release. Here is the overall template: [source,xml] ---- <settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\" maven.apache.org/SETTINGS/1.0.0 maven.apache.org/xsd/settings-1.0.0.xsd\">; <servers> <server> <id>ossrh</id> <username>${env.OSSRH_USER}</username> <password>${env.OSSRH_PASS}</password> </server> <server> <id>github</id> <username>${env.TLND_GITHUB_USER}</username> <password>${env.TLND_GITHUB_PASS}</password> </server> <server> <id>jira</id> <username>${env.TLND_JIRA_USER}</username> <password>${env.TLND_JIRA_PASS}</password> </server> <server> <id>blackduck</id> <username>${env.TLND_BLACKDUCK_USER}</username> <password>${env.TLND_BLACKDUCK_PASS}</password> </server> </servers> <profiles> <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <additionalparam>-Xdoclint:none</additionalparam> <gpg.executable>gpg</gpg.executable> <gpg.defaultKeyring>false</gpg.defaultKeyring> <gpg.keyname>${env.GPG_KEYNAME}</gpg.keyname> <gpg.passphrase>${env.GPG_PASSPHRASE}</gpg.passphrase> <gpg.publicKeyring>${env.HOME}/.gpg/talend.pub.bin</gpg.publicKeyring> <gpg.secretKeyring>${env.HOME}/.gpg/talend.priv.bin</gpg.secretKeyring> </properties> </profile> </profiles> </settings> ---- NOTE: all the environment variables should be either set in your environment or hardcoded inline. Note that it is more than highly recommended to use maven encryption: maven.apache.org/guides/mini/guide-encryption.html. 1. The OSSH variables are your Sonatype OSS repository credential with the permissions to deploy on org.talend. If you don’t have it already you can ask for one at issues.sonatype.org/projects/OSSRH using your talend address. 2. The GITHUB variables are your Github account credentials. It is mainly used to update the documentation. 3. The JIRA variables are your Talend account credentials with read permissions on jira.talendforge.org/projects/TCOMP/. 4. The Blackduck configuration is used for security scans. 5. The GPG variables reference the key you created in previous part. The release contacts JIRA to create the release notes. It uses all TCOMP issues which have the label changelog. Before any release don’t forget to go through all issues of the version you will release and add/remove the label depending the issue you want to appear into the release note. IMPORTANT: at that stage we assume previous steps have been done. Then, the release uses a standard Maven process, you should be able to do it in two steps: [source,sh] ---- mvn release:prepare mvn release:perform ---- Once these commands passed, you need to do/ensure: 1. Rebuild the project which is now on the new SNAPSHOT version (to update site metadata and versions for next iteration). Commit the diff. 2. The git tag corresponding to the release was pushed upstream (on github.com/Talend/component-runtime), if not, execute git push --follow-tags? After a moment - it can take a few hours - the binaries will be available on central (repo.apache.maven.org/maven2/). Don’t forget to check it to ensure there was no issue during the release process. You can also validate that the release deployed the new version (into the version menu) for the website. Last updated 2018-04-30 17:18:08 CEST",
    "link":"release-process.html",
    "title":"Talend Component Kit Release Process"
  },
  {
    "content":"The HTTP JUnit module allows you to mock REST API very easily. Here are its coordinates: [source,xml] ---- <dependency> <groupId>org.talend.sdk.component</groupId> <artifactId>component-runtime-junit</artifactId> <version>${talend-component.version}</version> <scope>test</scope> </dependency> ---- TIP: this module uses Apache Johnzon and Netty, if you have any conflict (in particular with netty) you can add the classifier shaded to the dependency and the two dependencies are shaded avoiding the conflicts with your component. It supports JUnit 4 and JUnit 5 as well but the overall concept is the exact same one: the extension/rule is able to serve precomputed responses saved in the classpath. You can plug your own ResponseLocator to map a request to a response but the default implementation - which should be sufficient in most cases - will look in talend/testing/http/<class name>_<method name>.json. Note that you can also put it in talend/testing/http/<request path>.json. JUnit 4 setup is done through two rules: JUnit4HttpApi which is responsible to start the server and JUnit4HttpApiPerMethodConfigurator which is responsible to configure the server per test and also handle the capture mode (see later). IMPORTANT: if you don’t use the JUnit4HttpApiPerMethodConfigurator, the capture feature will be deactivated and the per test mocking will not be available. Most of the test will look like: [source,java] ---- public class MyRESTApiTest { @ClassRule public static final JUnit4HttpApi API = new JUnit4HttpApi(); @Rule public final JUnit4HttpApiPerMethodConfigurator configurator = new JUnit4HttpApiPerMethodConfigurator(API); @Test public void direct() throws Exception { // … do your requests } } ---- For tests using SSL based services, you will need to use activeSsl() on the JUnit4HttpApi rule. If you need to access the server ssl socket factory you can do it from the HttpApiHandler (the rule): [source,java] [subs=+quotes] ---- @ClassRule public static final JUnit4HttpApi API = new JUnit4HttpApi().activeSsl(); @Test public void test() throws Exception { final HttpsURLConnection connection = getHttpsConnection(); connection.setSSLSocketFactory(API.getSslContext().getSocketFactory()); // …. } ---- JUnit 5 uses a JUnit 5 extension based on the HttpApi annotation you can put on your test class. You can inject the test handler (which has some utilities for advanced cases) through @HttpApiInject: [source,java] ---- @HttpApi class JUnit5HttpApiTest { @HttpApiInject private HttpApiHandler<?> handler; @Test void getProxy() throws Exception { // …. do your requests } } ---- NOTE: the injection is optional and the @HttpApi allows you to configure several behaviors of the test. For tests using SSL based services, you will need to use @HttpApi(useSsl = true). You can access the client SSL socket factory through the api handler: [source,java] [subs=+quotes] ---- @HttpApi*(useSsl = true)* class MyHttpsApiTest { @HttpApiInject private HttpApiHandler<?> handler; @Test void test() throws Exception { final HttpsURLConnection connection = getHttpsConnection(); connection.setSSLSocketFactory(handler.getSslContext().getSocketFactory()); // …. } } ---- The strength of this implementation is to run a small proxy server and auto configure the JVM: http[s].proxyHost, http[s].proxyPort, HttpsURLConnection#defaultSSLSocketFactory and SSLContext#default are auto configured to work out of the box with the proxy. It allows you to keep in your tests the native and real URLs. For instance this test is perfectlt valid: [source,java] ---- public class GoogleTest { @ClassRule public static final JUnit4HttpApi API = new JUnit4HttpApi(); @Rule public final JUnit4HttpApiPerMethodConfigurator configurator = new JUnit4HttpApiPerMethodConfigurator(API); @Test public void google() throws Exception { assertEquals(HttpURLConnection.HTTP_OK, get(\"https://google.fr?q=Talend\")); } private int get(final String uri) throws Exception { // do the GET request, skipped for brievity } } ---- If you execute this test, it will fail with a HTTP 400 because the proxy doesn’t find the mocked response. You can create it manually as seen in the introduction of the module but you can also set the property talend.junit.http.capture to the folder where to store the captures. It must be the root folder and not the folder where the json are (ie not prefixed by talend/testing/http by default). Generally you will want to use src/test/resources. If new File(\"src/test/resources\") resolves to the valid folder when executing your test (Maven default), then you can just set the system property to true, otherwise you need to adjust accordingly the system property value. Once you ran the tests with this system property, the testing framework will have created the correct mock response files and you can remove the system property. The test will still pass, using google.com…even if you disconnect your machine from the internet. The rule (extension) is doing all the work for you :). Setting talend.junit.http.passthrough system property to true, the server will just be a proxy and will execute each request to the actual server - like in capturing mode. Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-http.html",
    "title":"component-runtime-http-junit"
  },
  {
    "content":"Talend Intellij plugin is a plugin for the IDE Intellij that add some support to Talend components project. Main features: * New project generation support. * i18n completion for component configuration. In the Intellij IDEA : * Go to File → Settings… * On the left panel click on Plugins. * Click on Browse repositories… image::intellij/plugin/1-plugins.png[Plugins] * Type Talend in the search field then choose Talend Component Kit - Intellij Plugin. * Click on the Install button on the right. image::intellij/plugin/2-browse-talend.png[Browse talend plugin] Click on the restart Intellij IDEA button. image::intellij/plugin/3-restart.png[IDEA restart] Confirm the IDEA restart to complete the installation. image::intellij/plugin/4-restart.png[IDEA restart] The plugin is now installed into your Intellij IDEA, you can start using it. Once the plugin installed, you can generate Talend component project by going to File → New → Project. In the New Project wizard choose Talend Component then click Next. image::intellij/plugin/new-project_1.png[New Talend Project] The plugin will load the component starter and let you design your components. For more information about the component kit starter, you can check this tutorial image::intellij/plugin/new-project_2.png[New Talend Project] When you finish designing your project Next then click Finish. image::intellij/plugin/new-project_3.png[New Talend Project] The project will be automatically imported into the IDEA using the build tool that you have chosen. This feature offer auto completion for configuration i18n. Talend component configuration let you setup translatable, user friendly labels for you configuration using properties file. This plugin will provide some completion in those properties file for the configuration keys and default values. Let’s say that you have simple configuration class for a basic authentication that you will use in your component. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- @Checkable(\"basicAuth\") @DataStore(\"basicAuth\") @GridLayout({ @GridLayout.Row({ \"url\" }), @GridLayout.Row({ \"username\", \"password\" }), }) public class BasicAuthConfig implements Serializable { @Option private String url; @Option private String username; @Option @Credential private String password; } ---- This configuration classs contains 3 properties that you may want to attach some user friendly labels to them. For example: You may want the url option to have a label like My awesome server URL. For this you will need to create a Messages.properties file in the project resources to be able to add your labels. The plugin will automatically detect you configuration and provide you with keys completion in the properties file. Click Ctrl+Space to see the keys suggestions. image::intellij/plugin/suggestion_1.png[Keys suggestion] Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-talend-intellij-plugin-usage.html",
    "title":"Installation and usage of Talend Intellij plugin"
  },
  {
    "content":"The studio integration relies on Component Server (see Web for more details). When the plugin is deployed it starts a local server the Studio uses to gather data about the components. Copy org.talend.sdk.component.studio-integration.jar in the $STUDIO_HOME/plugins directory and restart the studio. Also ensure the component-server dependencies and commons-cli 1.4 are into the Studio maven repository. If you install any component, ensure its dependencies are in the repository as well. TIP: you can set in $STUDIO_HOME/configuration/config.ini the value maven.repository to global to reuse your local maven repository. The configuration goes into $STUDIO_HOME/configuration/config.ini. |=== | Name | Description | Default | component.environment | Active the developer mode when set to dev | - | component.debounce.timeout | Specifies timeout in milliseconds before calling listeners in components Text fields | 750 | component.kit.skip | If true the plugin is not active. Useful if you don’t have any component develop with the framework. | false | component.java.arguments | Component server additional options | - | component.java.m2 | The maven repository the server uses to resolve components | default to global Studio configuration | component.java.coordinates | A list of comma separated GAV (groupId:artifactId:version) of components to register | - | component.java.registry | A properties file with values matching component GAV (groupId:artifactId:version) which are registered at startup | - | component.java.port | Set a port to use for the server | random | components.server.beam.active | Active, if set to true, beam support (Experimental). It requires beam sdk java core dependencies to be available. | false | component.server.jul.forceConsole a| Adds a console handler to JUL to see logs in the console. This can be helpful in dev since the formatting will be better than the OSGi one in workspace/.metadata/.log. NOTE: it uses java.util.logging.SimpleFormatter.format property to define its format. Default is %1$tb %1$td, %1$tY %1$tl:%1$tM:%1$tS %1$Tp %2$s%n%4$s: %5$s%6$s%n but for development purposes [%4$s] %5$s%6$s%n is simpler and more readable. | false |=== IMPORTANT: to activate the plugin and be able to deploy your component, don’t forget to set component.kit.skip to true. A common developer configuration/config.ini will use the following specific entries: .configuration/config.ini [source,properties] ---- # use local .m2 instead of embedded studio one maven.repository = global # load these component modules component.java.coordinates = \\ group1:artifact1:0.0.1,\\ group2:artifact2:0.2.1 # during development, see developer model part component.environment = dev # log into the console the component interactions - optional component.server.jul.forceConsole = true java.util.logging.SimpleFormatter.format = [%4$s] %5$s%6$s%n ---- If you run multiple Studio instance automatically in parallel you can have some issues with the random port computation (can happen on a CI platform). For that purpose you can create the file $HOME/.talend/locks/org.talend.sdk.component.studio-integration.lock. When a server will start it will acquire a lock on that file and prevent another one to get a port until it is started. It ensures you can’t get two concurrent processes getting the same allocated port. IMPORTANT: it is highly unlikely it happens on a desktop and forcing a different value through component.java.port in your config.ini is likely a better solution for local installations. The developer mode will add a Talend Component Kit button in the main toolbar: image::studio-reload-button.png[Studio Reload Button] When clicking on this button, all component developped with the framework will be reloaded. The cache will be invalidated and the component refreshed. IMPORTANT: you still need to add/remove the components to see the changes. This feature is very useful to see some updates in components without having to restart the studio completly. Last updated 2018-04-30 17:18:08 CEST",
    "link":"studio.html",
    "title":"Talend Component Studio Integration"
  },
  {
    "content":"Few recommendations apply to the way a component packages are organized: . ensure to create a package-info.java with the component family/categories at the root of your component package: [source,java] ---- @Components(family = \"jdbc\", categories = \"Database\") package org.talend.sdk.component.jdbc; import org.talend.sdk.component.api.component.Components; ---- [start=2] . create a package for the configuration . create a package for the actions . create a package for the component and one subpackage by type of component (input, output, processors, …) It is recommended to ensure your configuration is serializable since it is likely you will pass it through your components which can be serialized. The first step to build a component is to identify the way it must be configured. It is generally split into two main big concepts: 1. the DataStore which is the way you can access the backend 2. the DataSet which is the way you interact with the backend Here are some examples to let you get an idea of what you put in each categories: [options=\"header,autowidth\"] |==== | Example description | DataStore | DataSet | Accessing a relational database like MySQL | the JDBC driver, url, username and password | the query to execute, row mapper, … | Access a file system | the file pattern (or directory + file extension/prefix/…) | the file format, potentially the buffer size, … |==== It is common to make the dataset including the datastore since both are required to work. However it is recommended to replace this pattern by composing both in a higher level configuration model: [source,java] ---- @DataSet public class MyDataSet { // … } @DataStore public class MyDataStore { // … } public class MyComponentConfiguration { @Option private MyDataSet dataset; @Option private MyDataStore datastore; } ---- Processor configuration is simpler than I/O configuration since it is specific each time. For instance a mapper will take the mapping between the input and output model: [source,java] ---- public class MappingConfiguration { @Option private Map<String, String> fieldsMapping; @Option private boolean ignoreCase; //… } ---- I/O are particular because they can be linked to a set of actions. It is recommended to wire all the ones you can apply to ensure the consumers of your component can provide a rich experience to their users. Here are the most common ones: [cols=\"1,1,2,6,6\"] |==== | Type | Action | Description | Configuration example | Action example | DataStore | @Checkable | Expose a way to ensure the datastore/connection works a| [source,java] ---- @DataStore @Checkable public class JdbcDataStore implements Serializable { @Option private String driver; @Option private String url; @Option private String username; @Option private String password; } ---- a| [source,java] ---- @HealthCheck public HealthCheckStatus healthCheck(@Option(\"datastore\") JdbcDataStore datastore) { if (!doTest(dataStore)) { // often add an exception message mapping or equivalent return new HealthCheckStatus(Status.KO, \"Test failed\"); } return new HealthCheckStatus(Status.KO, e.getMessage()); } ---- |==== Until the studio integration is complete, it is recommended to limit processors to 1 input. It is also recommended to provide as much information as possible to let the UI work with the data during its edition. The light validations are all the validations you can execute on the client side. They are listed in the UI hint part. This is the ones to use first before going with custom validations since they will be more efficient. These ones will enforce custom code to be executed, they are more heavy so try to avoid to use them for simple validations you can do with the previous part. Here you define an action taking some parameters needed for the validation and you link the option you want to validate to this action. Here is an example to validate a dataset. For example for our JDBC driver we could have: [source,java] ---- // … public class JdbcDataStore implements Serializable { @Option @Validable(\"driver\") private String driver; // … } @AsyncValidation(\"driver\") public ValidationResult validateDriver(@Option(\"value\") String driver) { if (findDriver(driver) != null) { return new ValidationResult(Status.OK, \"Driver found\"); } return new ValidationResult(Status.KO, \"Driver not found\"); } ---- Note that you can also make a class validable and you can use it to validate a form if you put it on your whole configuration: [source,java] ---- // note: some part of the API were removed for brievity public class MyConfiguration { // a lot of @Options } public MyComponent { public MyComponent(@Validable(\"configuration\") MyConfiguration config) { // … } //… } @AsyncValidation(\"configuration\") public ValidationResult validateDriver(@Option(\"value\") MyConfiguration configuration) { if (isValid(configuration)) { return new ValidationResult(Status.OK, \"Configuration valid\"); } return new ValidationResult(Status.KO, \"Driver not valid ${because …}\"); } ---- IMPORTANT: the parameter binding of the validation method uses the same logic than the component configuration injection. Therefore the @Option specifies the prefix to use to reference a parameter. It is recommended to use @Option(\"value\") until you know exactly why you don’t use it. This way the consumer can match the configuration model and just prefix it with value. to send the instance to validate. It can be neat and user friendly to provide completion on some fields. Here an example for the available drivers: [source,java] ---- // … public class JdbcDataStore implements Serializable { @Option @Completable(\"driver\") private String driver; // … } @Completion(\"driver\") public CompletionList findDrivers() { return new CompletionList(findDriverList()); } ---- Each component must have its own icon: [source,java] ---- @Icon(Icon.IconType.DB_INPUT) @PartitionMapper(family = \"jdbc\", name = \"input\") public class JdbcPartitionMapper implements Serializable { } ---- TIP: you can use talend.surge.sh/icons/ to identify the one you want to use. Not mandatory for the first version but recommended: enforce the version of your component. [source,java] ---- @Version(1) @PartitionMapper(family = \"jdbc\", name = \"input\") public class JdbcPartitionMapper implements Serializable { } ---- If you break a configuration entry in a later version ensure to: 1. upgrade the version 2. support a migration of the configuration [source,java] ---- @Version(value = 2, migrationHandler = JdbcPartitionMapper.Migrations.class) @PartitionMapper(family = \"jdbc\", name = \"input\") public class JdbcPartitionMapper implements Serializable { public static class Migrations implements MigrationHandler { // implement your migration } } ---- Testing the components is crucial, you can use unit tests and simple standalone JUnit but it is highly recommended to have a few Beam tests to ensure your component works in Big Data world. Don’t hesitate to send your feedback on writing component and best practices you can encounter. Last updated 2018-04-30 17:18:08 CEST",
    "link":"best-practices.html",
    "title":"Talend Component Best Practices"
  },
  {
    "content":"In this tutorial we will create a complete working output component for hazelcast. This will include : 1. The component configuration and the UI layout 2. The output that is responsible for connecting and writing data to the data source. How to create component configuration has already been described in \"Create an input component\" tutorial. For now we will use the same component configuration. We will only add a couple of fields required for our output component to already described configuration. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Option private String keyAttribute; @Option private String valueAttribute; ---- We will need those fields to determine key and value attributes for our Hazelcast map. As our output component needs to work in distributed environments it should implement Serializable interface. Let’s take a look at the skeleton of our output component. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Version <1> @Icon(custom = \"hazelcastOutput\", value = CUSTOM) <2> @Processor(name = \"Output\") <3> public class HazelcastOutput implements Serializable { private final HazelcastConfiguration configuration; private final JsonBuilderFactory jsonFactory; private final Jsonb jsonb; private final HazelcastService service; public HazelcastOutput(@Option(\"configuration\") final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service) {} <4> @PostConstruct public void init() {} <5> @PreDestroy public void release() {} <6> @ElementListener public void onElement(final JsonObject defaultInput) {} <7> } ---- <1> @Version annotation indicate the version of the component. it will be used to migrate the component configuration if needed. <2> @Icon annotation indicate the icon of the component. here we have defined a custom icon that need to be bundled in the component jar under resources/icons. <3> @Processor annotation indicate that this class is the processor(output) and give it’s name. <4> This constructor of the processor is responsible of injecting the component configuration and services. Configuration parameter are annotated by @Option. and other parameters are considered as services and will be injected by the component framework. The service may be local services (class annotated with @Service) or some services provided by the component framework. <5> The method annotated with @PostConstruct is executed once by instance and can be used to do some initialization. Here we will get the hazelcast instance according to the provided configuration. <6> The method annotated with @PreDestroy is used to clean resource at the end of the execution of the output. here we will shutdown the hazelcast instance loaded in the post Construct method. <7> Data is passed to the method annotated with @ElementListener. That method is responsible for data output. You can put all the related logic in this method. NOTE: in real implementation you can desire to bulk write the updates accordingly to groups, see Processor description Let’s implement all methods required for our output. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- public HazelcastOutput(@Option(\"configuration\") final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service) { this.configuration = configuration; this.jsonFactory = jsonFactory; this.jsonb = jsonb; this.service = service; } ---- [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- private transient HazelcastInstance instance; private transient IMap<Object, Object> map; @PostConstruct public void init() { instance = service.findInstance(configuration.newConfig()); <1> map = instance.getMap(configuration.getMapName()); <2> } ---- We will need Hazelcast instance and Hazelcast map. We add those as attributes to the output. <1> Here we create an instance of hazelcast according to the provided configuration. Here you can notice that we use the injected HazelcastService instance to perform that. This service is implemented in the project. See the implementation in \"Create an input component\" tutorial. <2> We get the Hazelcast map according to the map name from configuration. We use Hazelcast instance for that purpose. NOTE: in production you will not want to create one instance per thread/worker but we will cover that in another coming tutorial [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @PreDestroy public void close() { instance.getLifecycleService().shutdown(); map = null; } ---- We shutdown the instance that we have created in the PostConstruct and we free the Hazelcast map reference. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @ElementListener public void onElement(final JsonObject defaultInput) { <1> final Object key = toValue(defaultInput.get(configuration.getKeyAttribute())); final Object value = toValue(defaultInput.get(configuration.getValueAttribute())); map.put(key, value); } private Object toValue(final JsonValue jsonValue) { <2> if (jsonValue == null) { return null; } if (jsonValue.getValueType() == STRING) { return JsonString.class.cast(jsonValue).getString(); } if (jsonValue.getValueType() == NUMBER) { return JsonNumber.class.cast(jsonValue).doubleValue(); } return jsonValue.asJsonObject(); } ---- <1> This method will be used to pass the incoming data to our output. Every object passed should be a JsonObject instance. This method can include any logic required to write data to the data source. In our implementation we will put data to Hazelcast map. <2> This is our inner method which is used to transform incoming values in format required to put data to Hazelcast map. Here is the full code source for the output to have a global view of it. Read more about output… [source,java,indent=0,subs=\"verbatim,quotes,attributes\",role=\"initial-block-closed\"] ---- @Version <1> @Icon(custom = \"hazelcastOutput\", value = CUSTOM) <2> @Processor(name = \"Output\") <3> public class HazelcastOutput implements Serializable { private final HazelcastConfiguration configuration; private final JsonBuilderFactory jsonFactory; private final Jsonb jsonb; private final HazelcastService service; private transient HazelcastInstance instance; private transient IMap<Object, Object> map; public HazelcastOutput(@Option(\"configuration\") final HazelcastConfiguration configuration, final JsonBuilderFactory jsonFactory, final Jsonb jsonb, final HazelcastService service) { this.configuration = configuration; this.jsonFactory = jsonFactory; this.jsonb = jsonb; this.service = service; } @PostConstruct public void init() { instance = service.findInstance(configuration.newConfig()); map = instance.getMap(configuration.getMapName()); } @ElementListener public void onElement(final JsonObject defaultInput) { final Object key = toValue(defaultInput.get(configuration.getKeyAttribute())); final Object value = toValue(defaultInput.get(configuration.getValueAttribute())); map.put(key, value); } @PreDestroy public void release() { instance.getLifecycleService().shutdown(); map = null; } private Object toValue(final JsonValue jsonValue) { if (jsonValue == null) { return null; } if (jsonValue.getValueType() == STRING) { return JsonString.class.cast(jsonValue).getString(); } if (jsonValue.getValueType() == NUMBER) { return JsonNumber.class.cast(jsonValue).doubleValue(); } return jsonValue.asJsonObject(); } } ---- We have seen how to create a complete working output in this tutorial. Later we will explain how to create some unit tests for it. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-create-an-output-component.html",
    "title":"Write an Output component"
  },
  {
    "content":"Testing code that consume REST API(s) can sometimes presents some difficulties, as you can face allot of constraints when dealing with them, like API rates limit, authentication tokens and passwords sharing, API availability, sandbox that expire or API’s cost that may be high… As a developer you don’t want to care about all that, as all what you want to achieve is writing some good tests for your code logic. This is why, having the possibility to easily mock the API response is trivial. The component framework provides an API simulation tool that make it easy to write unit tests. In this tutorial we will show how to use it in unit tests. In this previous tutorial, we have created a component that consume Zendesk Search API. We will add some unit tests for it. NOTE: We have added 4 tickets that have the status open to our Zendesk test instance. that we will use in our tests In our tutorial we will use some concept from component junit testing. You can refer to this page to read about the SimpleComponentRule … L’est create a first unit test that will perform a real http request to Zendesk Search API instance. You can read how to create a simple unit test in this tutorial [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public class SearchTest { @ClassRule public static final SimpleComponentRule component = new SimpleComponentRule(\"component.package\"); @Test public void searchQuery() { // Initiating the component test configuration <1> BasicAuth basicAuth = new BasicAuth(\"https://instance.zendesk.com\", \"username\", \"password\"); final SearchQuery searchQuery = new SearchQuery(basicAuth, \"type:ticket status:open\", \"created_at\", \"desc\"); // We convert our configuration instance to URI configuration <2> final String uriConfig = SimpleFactory.configurationByExample() .forInstance(searchQuery) .configured().toQueryString(); // We create our job test pipeline <3> Job.components() .component(\"search\", \"zendesk://search?\" + uriConfig) .component(\"collector\", \"test://collector\") .connections() .from(\"search\").to(\"collector\") .build() .run(); final List<JsonObject> res = component.getCollectedData(JsonObject.class); assertEquals(4, res.size()); } } ---- <1> We initiate our authentication configuration using zendesk instance’s url and credentials. We also initiate our search query configuration. We want to get all the open ticket order by the creation date in a descendant order. <2> Here we perform a simple conversion of our configuration to URI format, that we will use in the job test pipeline, using SimpleFactory class provided by the component framework. Read more about job pipeline. <3> We create our job test pipeline. this is a simple pipeline that will execute our search component and redirect the result to the test collector component that will collect the search result. We execute the pipeline. Then, We get the job result and we ensure that we have received the 4 tickets. You can also check that the retrieved tickets have the open status. So here we have created a complete working test. the test is performing real http request to our zendesk instance. but we may don’t want do that every time on the development environment. We may want to execute real http request only on an integration environment and on development environment use some mocked result to develop faster or for any other reasons. Now we will transform this unit test to a mocked test that will use only mocked response of zendesk Search API. To do that you will need to add 2 junit rules provided by the component framework. 1. JUnit4HttpApi - this rule will start a simulation server that will act as a proxy and catch all the http requests performed inside the tests. This simulation server (proxy) have 2 modes : * capture : this mode will forward the captured http request to the real server and capture there response. * simulation : this mode will return a mocked response from the already captured responses. This rule need to be added as a class rule 2. JUnit4HttpApi - this rule have a reference to the first one and it role is to configure the simulation server for every unit test. it provide the simulation server by the running test context. This rule need to be added as a simple (method) rule. Let’s add those 2 rules to our test to make it run in a simulation mode. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public class SearchTest { @ClassRule public static final SimpleComponentRule component = new SimpleComponentRule(\"component.package\"); private final MavenDecrypter mavenDecrypter = new MavenDecrypter(); @ClassRule public static final JUnit4HttpApi API = new JUnit4HttpApi() <1> .activeSsl(); <2> @Rule public final JUnit4HttpApiPerMethodConfigurator configurator = new JUnit4HttpApiPerMethodConfigurator(API); <3> @Test public void searchQuery() { // the exact same code as above } ---- <1> As described above, we create and start a simulation server for this test class. <2> As the API that we consume use ssl, we need to activate ssl on or simulation server by simply calling the activeSsl() method. <3> We add our simulation server configuration provider. that will provide the test context to the simulation server. We almost done, Now we need to run our test in capture mode to catch the real API responses to be able to use them later in the simulated mode. To do that, we will have to set an environment variable talend.junit.http.capture to true. This will tel the simulation server to run in a capture mode. The captured response will be saved into resources/talend.testing.http package in a json format, then reused to perform API simulation. Now you know how to easily mock your component that consume REST API. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-test-rest-api.html",
    "title":"How to test a REST API"
  },
  {
    "content":"The entry point of the API is the ContainerManager, it will enable you to define what is the Shared classloader and to create children: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- try (final ContainerManager manager = new ContainerManager( <1> ContainerManager.DependenciesResolutionConfiguration.builder() <2> .resolver(new MvnDependencyListLocalRepositoryResolver(\"META-INF/talend/dependencies.list\")) .rootRepositoryLocation(new File(System.getProperty(\"user.home\", \".m2/repository\")) .create(), ContainerManager.ClassLoaderConfiguration.builder() <3> .parent(getClass().getClassLoader()) .classesFilter(name → true) .parentClassesFilter(name → true) .create())) { // create plugins } ---- <1> the ContainerManager is an AutoCloseable so you can use it in a try/finally block if desired. NOTE: it is recommended to keep it running if you can reuse plugins to avoid to recreate classloaders and to mutualize them. This manager has two main configuration entries: how to resolve dependencies for plugins from the plugin file/location and how to configure the classloaders (what is the parent classloader, how to handle the parent first/last delegation etc…). <2> the DependenciesResolutionConfiguration enables to pass a custom Resolver which will be used to build the plugin classloaders. For now the library only provides MvnDependencyListLocalRepositoryResolver which will read the output of mvn dependencies:list put in the plugin jar and will resolve from a local maven repository the dependencies. Note that SNAPSHOT are only resolved based on their name and not from metadata (only useful in development). To continue the comparison to a Servlet server, you can easily implement an unpacked war resolver if you want. <3> the ClassLoaderConfiguration is configuring how the whole container/plugin pair will behave: what is the shared classloader?, which classes are loaded from the shared loader first (intended to be used for API which shouldn’t be loaded from the plugin loader), which classes are loaded from the parent classloader (useful to exclude to load a \"common\" library from the parent classloader for instance, can be neat for guava, commons-lang3 etc…). Once you have a manager you can create plugins: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- final Container plugin1 = manager.create( <1> \"plugin-id\", <2> new File(\"/plugin/myplugin1.jar\")); <3> ---- <1> to create a plugin Container just use the create method of the manager <2> you can give an explicit id to the plugin (or if you bypass it, the manager will use the jar name) <3> you specify the plugin root jar To create the plugin container, the Resolver will resolve the dependencies needed for the plugin, then the manager will create the plugin classloader and register the plugin Container. It is common to need to do some actions when a plugin is registered/unregistered. For that purpose ContainerListener can be used: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- public class MyListener implements ContainerListener { @Override public void onCreate(final Container container) { System.out.println(\"Container #\" + container.getId() + \" started.\"); } @Override public void onClose(final Container container) { System.out.println(\"Container #\" + container.getId() + \" stopped.\"); } } ---- They are registered on the manager directly: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- final ContainerManager manager = getContainerManager(); final ContainerListener myListener = new MyListener(); manager.registerListener(myListener); <1> // do something manager.unregisterListener(myListener); <2> ---- <1> registerListener is used to add the listener from now on, it will not get any event for already created containers. <2> you can remove a listener with unregisterListener at any time. Last updated 2018-04-30 17:18:08 CEST",
    "link":"appendix.html",
    "title":"Talend Component Appendix"
  },
  {
    "content":"In a previous tutorial we have created an input component for hazelcast. In this one we will show how to write some unit tests for it. In this tutorial we will cover : 1. How to load components in a unit test. 2. How to create a job pipeline. 3. How to run the test in standalone mode. Here is our test class. let’s examine it in details. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public class HazelcastMapperTest { @ClassRule public static final SimpleComponentRule COMPONENTS = new SimpleComponentRule(HazelcastMapperTest.class .getPackage().getName()); <1> private static HazelcastInstance instance; <2> @BeforeClass public static void startInstanceWithData() { <3> instance = Hazelcast.newHazelcastInstance(); final IMap<Object, Object> map = instance.getMap(HazelcastMapperTest.class.getSimpleName()); IntStream.range(0, 100).forEach(i → map.put(\"test_\" + i, \"value #\" + i)); } @AfterClass public static void stopInstance() { <4> instance.getLifecycleService().shutdown(); } @Test public void run() { <5> Job.components() <6> .component(\"source\", \"Hazelcast://Input?configuration.mapName=\" + HazelcastMapperTest.class.getSimpleName()) .component(\"output\", \"test://collector\") .connections() .from(\"source\").to(\"output\") .build() .run(); final List<JsonObject> outputs = COMPONENTS.getCollectedData(JsonObject.class); <7> assertEquals(100, outputs.size()); } } ---- <1> SimpleComponentRule is a junit rule that let you load your component from a package. This rule also provide some tests components like emitter and collector Read more…. <2> An embedded hazelcast instance that we will use to test our input component. <3> Here we create an embedded hazelcast instance and we fill it with some test data. We create a map with the name of our test class and add some data to it. <4> We clean up the instance after the end o the tests. <5> This is our unit test. Here we will create a job pipeline that use our input component. <6> We use the Job pipeline builder to create a job. It contains two components the input component and the test collector component. We connect the input component to the collector component, build the job and run it locally. <7> After the job has finished running. We simply use the COMPONENTS rule instance to get the collected data from the collector component. Then we can do some assertion on the collected data. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-test-your-components.html",
    "title":"Test your components"
  },
  {
    "content":"If you want to ensure your component works in Beam the minimum to do is to try with the direct runner (if you don’t want to use spark). Check beam.apache.org/contribute/testing/ out for more details. Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-beam.html",
    "title":"Beam testing"
  },
  {
    "content":"The folowing artifact will allow you to test against a spark cluster: [source,xml] ---- <dependency> <groupId>org.talend.sdk.component</groupId> <artifactId>component-runtime-testing-spark</artifactId> <version>${talend-component.version}</version> <scope>test</scope> </dependency> ---- The usage relies on a JUnit TestRule. It is recommended to use it as a @ClassRule to ensure a single instance of a spark cluster is built but you can also use it as a simple @Rule which means it will be created per method instead of per test class. It takes as parameter the spark and scala version to use. It will then fork a master and N slaves. Finally it will give you submit* method allowing you to send jobs either from the test classpath or from a shade if you run it as an integration test. Here is a sample: [source,java] ---- public class SparkClusterRuleTest { @ClassRule public static final SparkClusterRule SPARK = new SparkClusterRule(\"2.10\", \"1.6.3\", 1); @Test public void classpathSubmit() throws IOException { SPARK.submitClasspath(SubmittableMain.class, getMainArgs()); // do wait the test passed } } ---- TIP: this is working with @Parameterized so you can submit a bunch of jobs with different args and even combine it with beam TestPipeline if you make it transient! The integration with JUnit 5 of that spark cluster logic uses @WithSpark marker for the extension and let you, optionally, inject through @SparkInject, the BaseSpark<?> handler to access te spark cluster meta information - like its host/port. Here is a basic test using it: [source,java] ---- @WithSpark class SparkExtensionTest { @SparkInject private BaseSpark<?> spark; @Test void classpathSubmit() throws IOException { final File out = new File(jarLocation(SparkClusterRuleTest.class).getParentFile(), \"classpathSubmitJunit5.out\"); if (out.exists()) { out.delete(); } spark.submitClasspath(SparkClusterRuleTest.SubmittableMain.class, spark.getSparkMaster(), out.getAbsolutePath()); await().atMost(5, MINUTES).until( () → out.exists() ? Files.readAllLines(out.toPath()).stream().collect(joining(\"\\n\")).trim() : null, equalTo(\"b → 1\\na → 1\")); } } ---- In current state, SparkClusterRule doesn’t allow to know a job execution is done - even if it exposes the webui url so you can poll it to check. The best at the moment is to ensure the output of your job exists and contains the right value. awaitability or equivalent library can help you to write such logic. Here are the coordinates of the artifact: [source,xml] ---- <dependency> <groupId>org.awaitility</groupId> <artifactId>awaitility</artifactId> <version>3.0.0</version> <scope>test</scope> </dependency> ---- And here is how to wait a file exists and its content (for instance) is the expected one: [source,java] ---- await() .atMost(5, MINUTES) .until( () → out.exists() ? Files.readAllLines(out.toPath()).stream().collect(joining(\"\\n\")).trim() : null, equalTo(\"the expected content of the file\")); ---- Last updated 2018-04-30 17:18:08 CEST",
    "link":"testing-spark.html",
    "title":"component-runtime-testing-spark"
  },
  {
    "content":"This part is limited to particular kinds of Beam PTransform: - the PTransform<PBegin, PCollection<?>> for the inputs - the PTransform<PCollection<?>, PDone> for the outputs. The outputs also must use a single (composite or not) DoFn in their apply method. Assume you want to wrap an input like this one (based on existing Beam ones): [source,java] ---- @AutoValue public abstract [static] class Read extends PTransform<PBegin, PCollection<String>> { // config @Override public PCollection<String> expand(final PBegin input) { return input.apply( org.apache.beam.sdk.io.Read.from(new BoundedElasticsearchSource(this, null))); } // … other transform methods } ---- To wrap the Read in a framework component you create a transform delegating to this one with a @PartitionMapper annotation at least (you likely want to follow the best practices as well adding @Icon and @Version) and using @Option constructor injections to configure the component: [source,java] ---- @PartitionMapper(family = \"myfamily\", name = \"myname\") public class WrapRead extends PTransform<PBegin, PCollection<String>> { private PTransform<PBegin, PCollection<String>> delegate; public WrapRead(@Option(\"dataset\") final WrapReadDataSet dataset) { delegate = TheIO.read().withConfiguration(this.createConfigurationFrom(dataset)); } @Override public PCollection<String> expand(final PBegin input) { return delegate.expand(input); } // … other methods like the mapping with the native configuration (createConfigurationFrom) } ---- Assume you want to wrap an output like this one (based on existing Beam ones): [source,java] ---- @AutoValue public abstract [static] class Write extends PTransform<PCollection<String>, PDone> { // configuration withXXX(…) @Override public PDone expand(final PCollection<String> input) { input.apply(ParDo.of(new WriteFn(this))); return PDone.in(input.getPipeline()); } // other methods of the transform } ---- You can wrap this output exactly the same way than for the inputs but using @Processor this time: [source,java] ---- @PartitionMapper(family = \"myfamily\", name = \"myname\") public class WrapRead extends PTransform<PCollection<String>, PDone> { private PTransform<PCollection<String>, PDone> delegate; public WrapRead(@Option(\"dataset\") final WrapReadDataSet dataset) { delegate = TheIO.write().withConfiguration(this.createConfigurationFrom(dataset)); } @Override public PDone expand(final PCollection<String> input) { return delegate.expand(input); } // … other methods like the mapping with the native configuration (createConfigurationFrom) } ---- Note that the class org.talend.sdk.component.runtime.beam.transform.DelegatingTransform fully delegates to another transform the \"expansion\". Therefore you can extend it and just implement the configuration mapping: [source,java] ---- @Processor(family = \"beam\", name = \"file\") public class BeamFileOutput extends DelegatingTransform<PCollection<String>, PDone> { public BeamFileOutput(@Option(\"output\") final String output) { super(TextIO.write() .withSuffix(\"test\") .to(FileBasedSink.convertToFileResourceIfPossible(output))); } } ---- In terms of classloading, when you write an IO all the Beam SDK Java core stack is assumed in Talend Component Kit runtime as provided so never include it in compile scope - it would be ignored anyway. If you need a JSonCoder you can use org.talend.sdk.component.runtime.beam.factory.service.PluginCoderFactory service which gives you access the JSON-P and JSON-B coders. Here is a sample input based on beam Kafka: [source,java] ---- @Version @Icon(Icon.IconType.KAFKA) @Emitter(name = \"Input\") @AllArgsConstructor @Documentation(\"Kafka Input\") public class KafkaInput extends PTransform<PBegin, PCollection<JsonObject>> { <1> private final InputConfiguration configuration; private final JsonBuilderFactory builder; private final PluginCoderFactory coderFactory; private KafkaIO.Read<byte[], byte[]> delegate() { final KafkaIO.Read<byte[], byte[]> read = KafkaIO.<byte[], byte[]> read() .withBootstrapServers(configuration.getBootstrapServers()) .withTopics(configuration.getTopics().stream().map(InputConfiguration.Topic::getName).collect(toList())) .withKeyDeserializer(ByteArrayDeserializer.class).withValueDeserializer(ByteArrayDeserializer.class); if (configuration.getMaxResults() > 0) { return read.withMaxNumRecords(configuration.getMaxResults()); } return read; } @Override <2> public PCollection<JsonObject> expand(final PBegin pBegin) { final PCollection<KafkaRecord<byte[], byte[]>> kafkaEntries = pBegin.getPipeline().apply(delegate()); return kafkaEntries.apply(ParDo.of(new RecordToJson(builder))).setCoder(coderFactory.jsonp()); <3> } @AllArgsConstructor private static class RecordToJson extends DoFn<KafkaRecord<byte[], byte[]>, JsonObject> { private final JsonBuilderFactory builder; @ProcessElement public void onElement(final ProcessContext context) { context.output(toJson(context.element())); } // todo: we shouldnt be typed string/string so make it evolving private JsonObject toJson(final KafkaRecord<byte[], byte[]> element) { return builder.createObjectBuilder().add(\"key\", new String(element.getKV().getKey())) .add(\"value\", new String(element.getKV().getValue())).build(); } } } ---- <1> the PTransform generics define it is an input (PBegin marker) <2> the expand method chains the native IO with a custom mapper (RecordToJson) <3> the mapper uses the JSON-P coder automatically created from the contextual component Since the Beam wrapper doesn’t respect the standard Kit programming Model (no @Emitter for instance) you need to set <talend.validation.component>false</talend.validation.component> property in your pom.xml (or equivalent for Gradle) to skip the Kit component programming model validations. Last updated 2018-04-30 17:18:08 CEST",
    "link":"wrapping-a-beam-io.html",
    "title":"Wrapping a Beam I/O"
  },
  {
    "content":"Development vs Continuous integration Setup In a previous tutorial, we have shown how to create a mocked test for our Zendesk search component. In the test you can notice that we have used our Zendesk credentials directly into the code to do a first capture of the API response, then we have switched to a fake credentials in simulation mode as we do not call the real API anymore. But what if you want to continue to call the real API on the CI server or on a a specific environment ? Let’s make our test able to get the credentials depending on the execution mode (simulation/passthrough). IMPORTANT: This instructions, need to be done, on the CI server or on any environment that require the real credentials. We will use Maven servers, that support password encryption as a credentials provider, and the test rule MavenDecrypterRule provided by the framework. This rule let you get credentials from maven settings using a server id. So let’s create an encrypted server credential for our zendesk instance. 1. Create a master password using the command : mvn --encrypt-master-password <password> 2. Store this master password in settings-security.xml file in ~/.m2 folder. 3. Encrypt zendesk instance password using the command: mvn --encrypt-password <zendesk-password> 4. Create a server entry under servers in maven settings.xml file in ~/.m2. [source,xml] ---- <server> <id>zendesk</id> <username>username@email.com</username> <password>The enccrypted password {oL37x/xiSvwtlhrMQ=}</password> </server> ---- NOTE: Encryption is optional but recommended. TIP: If you want to store the settings-security.xml and settings.xml files elsewhere that the default location ~/.m2. You can do it by setting the path of the directory containing the files into the environment variable talend.maven.decrypter.m2.location We start by adding MavenDecrypterRule rule to our test class. This rule will let us inject server information stored in maven settings.xml to our test. The rule will also decrypt the password if they are encrypted. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public class SearchTest { @Rule public final MavenDecrypterRule mavenDecrypterRule = new MavenDecrypterRule(this); } ---- Now we can inject our Zendesk server to our test. For that we add a new field to our class annotated by @DecryptedServer annotation that will holde the server id to be injected. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- public class SearchTest { @Rule public final MavenDecrypterRule mavenDecrypterRule = new MavenDecrypterRule(this); @DecryptedServer(\"zendesk\") private Server server; } ---- The MavenDecrypterRule will be able at runtime to inject the server instance into this class. the server instance contains the username and de decrypted password. Now we can use the server instance in our test to get the real credential in a secured manner. [source,java,indent=0,subs=\"verbatim,quotes,attributes\",] ---- BasicAuth basicAuth = new BasicAuth(\"https://instance.zendesk.com\", server.getUsername(), server.getPassword()); ---- Here is the complete test class after modification : [source,java,indent=0,subs=\"verbatim,quotes,attributes\",role=\"initial-block-closed\"] ---- public class SearchTest { @ClassRule public static final SimpleComponentRule component = new SimpleComponentRule(\"component.package\"); private final MavenDecrypter mavenDecrypter = new MavenDecrypter(); @ClassRule public static final JUnit4HttpApi API = new JUnit4HttpApi() .activeSsl(); @Rule public final JUnit4HttpApiPerMethodConfigurator configurator = new JUnit4HttpApiPerMethodConfigurator(API); @Rule public final MavenDecrypterRule mavenDecrypterRule = new MavenDecrypterRule(this); @DecryptedServer(\"zendesk\") private Server server; @Test public void searchQuery() { // Initiating the component test configuration BasicAuth basicAuth = new BasicAuth(\"https://instance.zendesk.com\", server.getUsername(), server.getPassword()); final SearchQuery searchQuery = new SearchQuery(basicAuth, \"type:ticket status:open\", \"created_at\", \"desc\"); // We convert our configuration instance to URI configuration final String uriConfig = SimpleFactory.configurationByExample() .forInstance(searchQuery) .configured().toQueryString(); // We create our job test pipeline Job.components() .component(\"search\", \"zendesk://search?\" + uriConfig) .component(\"collector\", \"test://collector\") .connections() .from(\"search\").to(\"collector\") .build() .run(); final List<JsonObject> res = component.getCollectedData(JsonObject.class); assertEquals(4, res.size()); } } ---- This test will continue to work in simulation mode. as we have our API simulation proxy activated. Let’s make it work in real mode on a CI server. we will use jenkins in this tutorial Log in to your jenkins then : Click on New Item to create a new build job image::jenkins/1_jenkins_new_item.png[Create a new job] Enter an Item name (Job name) and choose the freestyle job. Then click OK. image::jenkins/2_jenkins_new_item.png[Create a new job] In Source Code Management section enter your project repository URL. We are using our github repository in this tutorial. We will build the master branch image::jenkins/4_jenkins_source_code.png[Source Code Management] In the Build Section click on add build step, then choose Invoke top-level Maven targets image::jenkins/6_jenkins_build_cmd.png[Build Section] Choose you Maven version, and enter your maven build command. we are using a simple clean install and click save. image::jenkins/6_jenkins_build_cmd_2.png[Build Section] You can notice that we have added the option -Dtalend.junit.http.passthrough=true to our build command. This Option will tell the API simulation proxy to run in passthrough mode. So it’s will forward all the http request that we have maded in our test to the real API server. We also get the real credentials, thanks to our MavenDecrypterRule rule. TIP: You can configure the passthrough mode globally on your CI server by setting the environment variable talend.junit.http.passthrough to true. 6.Test the job. click Build now you can notice that your job have built correctly. image::jenkins/7_jenkins_build_result.png[Test the job] That’s all you need to do, now your tests run in a simulation mode on dev and in a (passthrough) mode on your CI server. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-dev-vs-ci-setup.html",
    "title":"tutorial dev vs ci setup"
  },
  {
    "content":"The component starter let you design your components configuration and generate a ready to implement project structure. NOTE: In this tutorial, we will use the component starter to generate some components for MySQL database. The starter is available at starter-toolkit.talend.io. image::starter/starter_project-config.png[Project configuration] 1. Choose your favorite build tool. we will use maven in this tutorial but you can select gradle if you prefer. 2. We add the test facet to get some unit tests generated automatically for the component that we will create after. 3. Complete the component description (family, and category). 4. Complete the project metadata. the groupId, ArtifactId and the package. 5. Click ADD A COMPONENT button to start designing your components. Let’s create an input component that will connect to a MySQL database, execute a SQL query and get the result. image::starter/starter_input-config.png[Input configuration] 1. We choose the component type. INPUT in this case. 2. Give the component a cool name. MySQLInput is good enough. 3. This button will let us create the required configuration for our component. To be able to execute a SQL query, we will need a connection url, the username and the password for the database and the SQL query to be executed. Click the (+) button on the right to add a field and chose it type. 4. Our component will be an ordinary (non streaming) component. so just let this toggle button off. 5. Here we define the record type that this component will produce. we let it generic for now. as our component will generate some json records. You can also choose a custom type to define a POJO that will represent your records. In this step we will create a very simple processor component that will receive a record, log it and return it at is. image::starter/starter_processor-config.png[Processor configuration] 1. Choose the component type. PROCESSOR/OUTPUT in this case. 2. Give an explicit name to the component. RecordLogger, as our processor will log the records. 3. This is the configuration button. this component don’t need any configuration. so we just skip this step. 4. Here we define the inputs of the component. so click ADD INPUT button to create an input. In this component we will have only one input that will receive the record to be log. 5. click the input name to access it configuration. you can change the name and define the record model. Here we will use a generic record. otherwise you can use a POJO to represent the record. 6. Click ADD OUTPUT to create an output for this processor. we will need only one output as our logger only emit the received record. NOTE: the outputs are also configurable in the same way as the inputs (as explained in 5). Now we will create an output component that will receive a record and insert it into a database table. NOTE: Output components are Processors without outputs. In other words, the output is a processor that don’t produce any records. image::starter/starter_output-config.png[Output configuration] 1. Choose the component type. PROCESSOR/OUTPUT in this case. 2. Give the component a name. MySQLOutput 3. Create the component configuration. in this case we will need the connection url, the credentials of the database and the table name to insert the record in. 4. Like what we have done for the processor component above. we add an input 5. We make our input generic as we want to handle generic records in this component. 6. We don’t create any outputs for this component as it will not produce any records. this is the only difference betweene an output an a processor. In the previous steps we have configured a project and created 3 components of different types (input, processor and output). Now click the GO TO FINISH button beside the ADD A COMPONENT button. You will be redirected to a summary page that shows : image::starter/starter_project-download.png[Output configuration] 1. Project configuration summary. 2. The list of the created components at the left panel. 3. You have two options to get the generated project. Download it locally as a zip file using the DOWNLOAD AS ZIP 4. Create a github repository and push the project to it using the button CREATE ON GITHUB. Download the project as a zip to your local machine or clone it if you have created a github project. As we have created a maven project here, we will use maven command to compile the project. In the project directory we execute the command mvn package If you don’t have Maven installed on your machine, you can use the maven wrapper provided in the generated project. All you need is to execute the command: ./mvnw package NOTE: If you have created a gradle project you can compile with gradle build or use the gradle wrapper ./gradlew build The generated project code contains documentation that will guide you to implement the component logic. so import the project to your favorite IDE and start implementing coding. In the next tutorial, we will explain how to implement an Input component in details. Last updated 2018-04-30 17:18:08 CEST",
    "link":"tutorial-generate-project-using-starter.html",
    "title":"Generate a project using the component starter"
  },
  {
    "content":"gradle-talend-component intends to help you to write components validating components match best practices. It is inspired from the Maven plugin and adds the ability to generate automatically the dependencies.txt file the SDK uses to build the component classpath. For more information on the configuration you can check out the maven properties matching the attributes. Here is how to use it: [source,groovy] ---- buildscript { repositories { mavenLocal() mavenCentral() } dependencies { classpath \"org.talend.sdk.component:gradle-talend-component:${talendComponentVersion}\" } } apply plugin: 'org.talend.sdk.component' apply plugin: 'java' // optional customization talendComponentKit { // dependencies.txt generation, replaces maven-dependency-plugin dependenciesLocation = \"TALEND-INF/dependencies.txt\" boolean skipDependenciesFile = false; // classpath for validation utilities sdkVersion = \"${talendComponentVersion}\" apiVersion = \"${talendComponentApiVersion}\" // documentation skipDocumentation = false documentationOutput = new File(….) documentationLevel = 2 // first level will be == in the generated adoc documentationTitle = 'My Component Family' // default to project name documentationFormats = [:] // adoc attributes documentationFormats = [:] // renderings to do // validation skipValidation = false validateFamily = true validateSerializable = true validateInternationalization = true validateModel = true validateMetadata = true validateComponent = true validateDataStore = true validateDataSet = true validateActions = true // web serverArguments = [] serverPort = 8080 // car carOutput = new File(….) carMetadata = [:] // custom meta (string key-value pairs) } ---- Last updated 2018-04-30 17:18:08 CEST",
    "link":"build-tools-gradle.html",
    "title":"Gradle Plugin"
  },
  {
    "content":"Recommanded practise for internationalization are: * store messages using ResourceBundle properties file in your component module * the location of the properties are in the same package than the related component(s) and is named Messages (ex: org.talend.demo.MyComponent will use org.talend.demo.Messages[locale].properties) * for your own messages use the internationalization API Overal idea is to design its messages as methods returning String values and back the template by a ResourceBundle located in the same package than the interface defining these methods and named Messages. IMPORTANT: this is the mecanism to use to internationalize your own messages in your own components. To ensure you internationalization API is identified you need to mark it with @Internationalized: [source,java] ---- @Internationalized <1> public interface Translator { String message(); String templatizedMessage(String arg0, int arg1); <2> String localized(String arg0, @Language Locale locale); <3> } ---- <1> @Internationalized allows to mark a class as a i18n service <2> you can pass parameters and the message will use MessageFormat syntax to be resolved based on the ResourceBundle template <3> you can use @Language on a Locale parameter to specify manually the locale to use, note that a single value will be used (the first parameter tagged as such). Last updated 2018-04-30 17:18:08 CEST",
    "link":"services-internationalization.html",
    "title":"Internationalization"
  },
  {
    "content":"As seen in the Getting Started, you need an annotation to register your component through family method. Multiple components can use the same family value but the pair family+name MUST be unique for the system. If you desire (recommended) to share the same component family name instead of repeating yourself in all family methods, you can use @Components annotation on the root package of you component, it will enable you to define the component family and the categories the component belongs to (default is Misc if not set). Here is a sample package-info.java: [source,java] ---- @Components(name = \"my_component_family\", categories = \"My Category\") package org.talend.sdk.component.sample; import org.talend.sdk.component.api.component.Components; ---- For an existing component it can look like: [source,java] ---- @Components(name = \"Salesforce\", categories = {\"Business\", \"Cloud\"}) package org.talend.sdk.component.sample; import org.talend.sdk.component.api.component.Components; ---- Components can require a few metadata to be integrated in Talend Studio or Cloud platform. Here is how to provide these information. These metadata are set on the component class and belongs to org.talend.sdk.component.api.component package. [options=\"header,autowidth\"] |==== | API | Description | @Icon | Set an icon key used to represent the component. Note you can use a custom key with custom() method but it is not guaranteed the icon will be rendered properly. | @Version | Set the component version, default to 1. |==== Example: [source,java] ---- @Icon(FILE_XML_O) @PartitionMapper(name = \"jaxbInput\") public class JaxbPartitionMapper implements Serializable { // … } ---- If some impacting changes happen on the configuration they can be manage through a migration handler at component level (to enable to support trans-model migration). The @Version annotation supports a migrationHandler method which will take the implementation migrating the incoming configuration to the current model. For instance if filepath configuration entry from v1 changed to location in v2 you can remap the value to the right key in your MigrationHandler implementation. TIP: it is recommended to not manage all migrations in the handler but rather split it in services you inject in the migration handler (through constructor): [source,java] ---- // full component code structure skipped for brievity, kept only migration part @Version(value = 3, migrationHandler = MyComponent.Migrations.class) public class MyComponent { // the component code… private interface VersionConfigurationHandler { Map<String, String> migrate(Map<String, String> incomingData); } public static class Migrations { private final List<VersionConfigurationHandler> handlers; // VersionConfigurationHandler implementations are decorated with @Service public Migrations(final List<VersionConfigurationHandler> migrations) { this.handlers = migrations; this.handlers.sort(/some custom logic/); } @Override public Map<String, String> migrate(int incomingVersion, Map<String, String> incomingData) { Map<String, String> out = incomingData; for (MigrationHandler handler : handlers) { out = handler.migrate(out); } } } } ---- What is important in this snippet is not much the way the code is organized but rather the fact you organize your migrations the way which fits the best your component. If migrations are not conflicting no need of something fancy, just apply them all but if you need to apply them in order you need to ensure they are sorted. Said otherwise: don’t see this API as a migration API but as a migration callback and adjust the migration code structure you need behind the MigrationHandler based on your component requirements. The service injection enables you to do so. @PartitionMapper will obviously mark a partition mapper: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @PartitionMapper(family = \"demo\", name = \"my_mapper\") public class MyMapper { } ---- @Emitter is a shortcut for @PartitionMapper when you don’t support distribution. Said otherwise it will enforce an implicit partition mapper execution with an assessor size of 1 and a split returning itself. [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Emitter(family = \"demo\", name = \"my_input\") public class MyInput { } ---- A method decorated with @Processor will be considered as a producer factory: [source,java,indent=0,subs=\"verbatim,quotes,attributes\"] ---- @Processor(family = \"demo\", name = \"my_processor\") public class MyProcessor { } ---- Last updated 2018-04-30 17:18:08 CEST",
    "link":"component-registering.html",
    "title":"Registering components"
  }
]
++++
